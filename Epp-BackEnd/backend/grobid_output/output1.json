{
    "metadata": {
        "title": "VLMAE: Vision-Language Masked Autoencoder",
        "authors": [
            {
                "name": "Sunan He",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Taian Guo",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Tao Dai",
                "affiliations": [
                    {
                        "name": "College of Computer Science and Software Engineering",
                        "type": "department"
                    },
                    {
                        "name": "Shenzhen University",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Ruizhi Qiao",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Chen Wu",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Xiujun Shu",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Bo Ren",
                "affiliations": [
                    {
                        "name": "YouTu Lab",
                        "type": "laboratory"
                    }
                ]
            }
        ],
        "abstract": "Image and language modeling is of crucial importance for vision-language pre-training (VLP), which aims to learn multi-modal representations from large-scale paired imagetext data. However, we observe that most existing VLP methods focus on modeling the interactions between image and text features while neglecting the information disparity between image and text, thus suffering from focal bias. To address this problem, we propose a vision-language masked autoencoder framework (VLMAE). VLMAE employs visual generative learning, facilitating the model to acquire finegrained and unbiased features. Unlike the previous works, VLMAE pays attention to almost all critical patches in an image, providing more comprehensive understanding. Extensive experiments demonstrate that VLMAE achieves better performance in various vision-language downstream tasks, including visual question answering, image-text retrieval and visual grounding, even with up to 20% pre-training speedup."
    },
    "sections": [
        {
            "n": "1",
            "title": "Introduction",
            "content": [
                "In recent years, with the prevalence of self-supervised learning and transformer-based models in both natural language processing and computer vision (Devlin et al. 2018;Dosovitskiy et al. 2020), we have witnessed rapid development in vision-language representation learning. In multi-modal tasks, such as visual question answering and image-text retrieval, where the model needs to comprehend and aggregate visual-textual information, vision-language pre-training (VLP) plays a fundamental role.",
                "Early VLP methods (Lu et al. 2019;Li et al. 2020;Chen et al. 2020) rely on a well-trained but heavy object detector on the input image for visual proposal feature extraction, followed by a cross-model encoder for visual-textual interaction. To eliminate the heavy object detector, some recent methods (Huang et al. 2021;Kim, Son, and Kim 2021;Wang et al. 2021) resort to vision dictionary with a lightweight visual embedder or modality experts networks for better trade-off between efficiency and performance of downstream tasks. Very recently, ALBEF (Li et al. 2021  By contrast, our VLMAE produces better results (in green) with a universal focus over the whole image.",
                "tures, facilitating the aggregation of multi-modal information. TCL (Yang et al. 2022) further leverages cross-modal and intra-modal self-supervision simultaneously to obtain multi-modal interactions more easily, leading to the most recent state-of-the-art performance. Despite impressive performance of ALBEF and TCL, such methods may focus on specific objects of interest from training data, while neglecting other critical objects (see Figure 1), thus leading to focal bias problem. This is mainly due to the information disparity between image and text. For example, as shown in Figure 1, the corresponding training text of the top row figure is \"The chair is purple\", which only describes parts of the image. The image encoder needs to pay more attention to the text-relevant area to facilitate the alignment between image and text while neglecting some irrelevant regions. However, for VLP models, without any textual prior, the image feature should be task-agnostic, and all critical objects in an image should be attended to. To alleviate such bias, in this work, we introduce a generative task of pixel-level reconstruction, relying on a comprehensive understanding of the image.",
                "Pixel-level image reconstruction with the encoderdecoder paradigm is prevalent in representation learning (Vincent et al. 2008;Kingma and Welling 2013). Recently, with asymmetric encoder-decoder architecture, masked autoencoders (MAE) (He et al. 2022) exhibits high generalizability and remarkable performance in vision tasks. The simple but insightful pre-training strategy, i.e., masking a very high portion of random patches, exploits image redundancy and forces the model to acquire comprehensive understanding of the image. Inspired by MAE, we adopt the masking-then-predicting paradigm and introduce a pixel-level reconstruction task for alleviating the focal bias problem.",
                "In this paper, we propose a novel VLP framework called Vision-Language Masked AutoEncoder (VLMAE), which is composed of two uni-modal encoders to extract visual and textual features, an image decoder for pixel-level reconstruction, and a fusion learner to aggregate the multi-modal features. VLMAE employs image-text contrastive learning and visual generative learning simultaneously. Contrastive learning promotes the alignment of image-text pairs and enables the model to obtain global semantic representations, and generative learning facilitates the model to acquire more comprehensive and fine-grained understanding. We propose Regional Masked Image Modeling (RMIM) loss to refine the image-text alignment and facilitate fusion of multi-modal features. To enhance the model's representation ability, we further propose the Image Feature Reconstruction (IFR) task, which requires the model to predict masked patch features based on visible patch inputs. Meanwhile, the masking design for the image patches in VLMAE reduces pretraining cost significantly.",
                "Our main contributions can be summarized as: age and text embeddings separately with dual uni-modal encoders (Radford et al. 2021;Andonian, Chen, and Hamid 2022). Recent methods model the vision-language interaction with contrastive loss on massive noisy paired imagetext data crawled from Internet. Despite thire simplicity, they achieve remarkable performance in cross-modal retrieval tasks with high efficiency. However, these methods do not perform well in more complex multi-modal downstream tasks, such as visual question answering and visual reasoning, which require the aggregation of visual-textual information and reasoning capabilities. They lack the capability to model more complicated interactions between the two modalities, which is indispensable for multi-modal reasoning. The second category adopts a transformer-based multimodal fusion encoder to model the interactions between images and texts (Kim, Son, and Kim 2021;Li et al. 2021;Zeng, Zhang, and Li 2021). These models achieve superior performance for downstream vision-language reasoning or classification tasks, thanks to the interaction modeling capability of the deep fusion encoder. Earlier methods rely on a pre-trained object detector for visual region feature extraction, which observably slows down the inference procedure.",
                "Latter methods endeavor to remove the heavy object detector for high inference efficiency. Among them, SOHO (Huang et al. 2021) adopts a vision dictionary to extract compact visual features from the whole image, while ViLT (Kim, Son, and Kim 2021) uses lightweight image and text tokenizers instead of separate uni-modal encoders to achieve faster inference speed. While these methods of the second category aggregate the multi-modal feature effectively, they ignore the importance of image-text alignment, hindering the interaction between the visual and textual modalities. To address this disadvantage, ALBEF (Li et al. 2021) proposes to align the visual and textual representations with contrastive learning before fusing them in the fusion encoder with cross-modal attention. Besides, ALBEF adopts pseudo-targets produced by a momentum model to conduct momentum distillation for better noise resistance capability when learning from the noisy web data. VLMO (Wang et al. 2021) further introduces Mixture-of-Modality-Experts (MOME) Transformer and stage-wise pre-training strategy, leading to better performance in downstream tasks with prominent efficiency. TCL (Yang et al. 2022) also shares the similar aligningbefore-fusing spirit of ALBEF but leverages cross-modal and intra-modal self-supervision simultaneously to enhance multi-modal interactions, achieving the most recent stateof-the-art performance. Though effective, previous visionlanguage pre-training methods with contrastive learning rely on modeling global semantic interactions and aim to maximize image-text mutual information, resulting in the focal bias problem. Motivated by recent generative methods in visual self-supervised learning (He et al. 2022;Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP. To restore the image, the model needs to pay attention to almost all patches instead of only salient regions. With such masking-then-predicting paradigm, our model provides more comprehensive understanding of the image and achieves superior performance in downstream multi-modal tasks with significant pre-training cost reduction."
            ],
            "paragraphs": [
                {
                    "text": "In recent years, with the prevalence of self-supervised learning and transformer-based models in both natural language processing and computer vision (Devlin et al. 2018;Dosovitskiy et al. 2020), we have witnessed rapid development in vision-language representation learning. In multi-modal tasks, such as visual question answering and image-text retrieval, where the model needs to comprehend and aggregate visual-textual information, vision-language pre-training (VLP) plays a fundamental role.",
                    "citations": [
                        {
                            "target": "b5",
                            "text": "(Devlin et al. 2018;"
                        },
                        {
                            "target": "b7",
                            "text": "Dosovitskiy et al. 2020)"
                        }
                    ]
                },
                {
                    "text": "Early VLP methods (Lu et al. 2019;Li et al. 2020;Chen et al. 2020) rely on a well-trained but heavy object detector on the input image for visual proposal feature extraction, followed by a cross-model encoder for visual-textual interaction. To eliminate the heavy object detector, some recent methods (Huang et al. 2021;Kim, Son, and Kim 2021;Wang et al. 2021) resort to vision dictionary with a lightweight visual embedder or modality experts networks for better trade-off between efficiency and performance of downstream tasks. Very recently, ALBEF (Li et al. 2021  By contrast, our VLMAE produces better results (in green) with a universal focus over the whole image.",
                    "citations": [
                        {
                            "target": "b21",
                            "text": "(Lu et al. 2019;"
                        },
                        {
                            "target": "b16",
                            "text": "Li et al. 2020;"
                        },
                        {
                            "target": "b3",
                            "text": "Chen et al. 2020"
                        },
                        {
                            "target": "b12",
                            "text": "(Huang et al. 2021;"
                        },
                        {
                            "target": "b13",
                            "text": "Kim, Son, and Kim 2021;"
                        },
                        {
                            "target": "b31",
                            "text": "Wang et al. 2021"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021"
                        }
                    ]
                },
                {
                    "text": "tures, facilitating the aggregation of multi-modal information. TCL (Yang et al. 2022) further leverages cross-modal and intra-modal self-supervision simultaneously to obtain multi-modal interactions more easily, leading to the most recent state-of-the-art performance. Despite impressive performance of ALBEF and TCL, such methods may focus on specific objects of interest from training data, while neglecting other critical objects (see Figure 1), thus leading to focal bias problem. This is mainly due to the information disparity between image and text. For example, as shown in Figure 1, the corresponding training text of the top row figure is \"The chair is purple\", which only describes parts of the image. The image encoder needs to pay more attention to the text-relevant area to facilitate the alignment between image and text while neglecting some irrelevant regions. However, for VLP models, without any textual prior, the image feature should be task-agnostic, and all critical objects in an image should be attended to. To alleviate such bias, in this work, we introduce a generative task of pixel-level reconstruction, relying on a comprehensive understanding of the image.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "(Yang et al. 2022"
                        },
                        {
                            "target": "",
                            "text": "Figure 1)"
                        }
                    ]
                },
                {
                    "text": "Pixel-level image reconstruction with the encoderdecoder paradigm is prevalent in representation learning (Vincent et al. 2008;Kingma and Welling 2013). Recently, with asymmetric encoder-decoder architecture, masked autoencoders (MAE) (He et al. 2022) exhibits high generalizability and remarkable performance in vision tasks. The simple but insightful pre-training strategy, i.e., masking a very high portion of random patches, exploits image redundancy and forces the model to acquire comprehensive understanding of the image. Inspired by MAE, we adopt the masking-then-predicting paradigm and introduce a pixel-level reconstruction task for alleviating the focal bias problem.",
                    "citations": [
                        {
                            "target": "b30",
                            "text": "(Vincent et al. 2008;"
                        },
                        {
                            "target": "b14",
                            "text": "Kingma and Welling 2013)"
                        },
                        {
                            "target": "b11",
                            "text": "(He et al. 2022"
                        }
                    ]
                },
                {
                    "text": "In this paper, we propose a novel VLP framework called Vision-Language Masked AutoEncoder (VLMAE), which is composed of two uni-modal encoders to extract visual and textual features, an image decoder for pixel-level reconstruction, and a fusion learner to aggregate the multi-modal features. VLMAE employs image-text contrastive learning and visual generative learning simultaneously. Contrastive learning promotes the alignment of image-text pairs and enables the model to obtain global semantic representations, and generative learning facilitates the model to acquire more comprehensive and fine-grained understanding. We propose Regional Masked Image Modeling (RMIM) loss to refine the image-text alignment and facilitate fusion of multi-modal features. To enhance the model's representation ability, we further propose the Image Feature Reconstruction (IFR) task, which requires the model to predict masked patch features based on visible patch inputs. Meanwhile, the masking design for the image patches in VLMAE reduces pretraining cost significantly.",
                    "citations": []
                },
                {
                    "text": "Our main contributions can be summarized as: age and text embeddings separately with dual uni-modal encoders (Radford et al. 2021;Andonian, Chen, and Hamid 2022). Recent methods model the vision-language interaction with contrastive loss on massive noisy paired imagetext data crawled from Internet. Despite thire simplicity, they achieve remarkable performance in cross-modal retrieval tasks with high efficiency. However, these methods do not perform well in more complex multi-modal downstream tasks, such as visual question answering and visual reasoning, which require the aggregation of visual-textual information and reasoning capabilities. They lack the capability to model more complicated interactions between the two modalities, which is indispensable for multi-modal reasoning. The second category adopts a transformer-based multimodal fusion encoder to model the interactions between images and texts (Kim, Son, and Kim 2021;Li et al. 2021;Zeng, Zhang, and Li 2021). These models achieve superior performance for downstream vision-language reasoning or classification tasks, thanks to the interaction modeling capability of the deep fusion encoder. Earlier methods rely on a pre-trained object detector for visual region feature extraction, which observably slows down the inference procedure.",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "(Radford et al. 2021;"
                        },
                        {
                            "target": "b0",
                            "text": "Andonian, Chen, and Hamid 2022)"
                        },
                        {
                            "target": "b13",
                            "text": "(Kim, Son, and Kim 2021;"
                        },
                        {
                            "target": "b17",
                            "text": "Li et al. 2021;"
                        },
                        {
                            "target": "b38",
                            "text": "Zeng, Zhang, and Li 2021)"
                        }
                    ]
                },
                {
                    "text": "Latter methods endeavor to remove the heavy object detector for high inference efficiency. Among them, SOHO (Huang et al. 2021) adopts a vision dictionary to extract compact visual features from the whole image, while ViLT (Kim, Son, and Kim 2021) uses lightweight image and text tokenizers instead of separate uni-modal encoders to achieve faster inference speed. While these methods of the second category aggregate the multi-modal feature effectively, they ignore the importance of image-text alignment, hindering the interaction between the visual and textual modalities. To address this disadvantage, ALBEF (Li et al. 2021) proposes to align the visual and textual representations with contrastive learning before fusing them in the fusion encoder with cross-modal attention. Besides, ALBEF adopts pseudo-targets produced by a momentum model to conduct momentum distillation for better noise resistance capability when learning from the noisy web data. VLMO (Wang et al. 2021) further introduces Mixture-of-Modality-Experts (MOME) Transformer and stage-wise pre-training strategy, leading to better performance in downstream tasks with prominent efficiency. TCL (Yang et al. 2022) also shares the similar aligningbefore-fusing spirit of ALBEF but leverages cross-modal and intra-modal self-supervision simultaneously to enhance multi-modal interactions, achieving the most recent stateof-the-art performance. Though effective, previous visionlanguage pre-training methods with contrastive learning rely on modeling global semantic interactions and aim to maximize image-text mutual information, resulting in the focal bias problem. Motivated by recent generative methods in visual self-supervised learning (He et al. 2022;Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP. To restore the image, the model needs to pay attention to almost all patches instead of only salient regions. With such masking-then-predicting paradigm, our model provides more comprehensive understanding of the image and achieves superior performance in downstream multi-modal tasks with significant pre-training cost reduction.",
                    "citations": [
                        {
                            "target": "b12",
                            "text": "(Huang et al. 2021"
                        },
                        {
                            "target": "b13",
                            "text": "(Kim, Son, and Kim 2021)"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        },
                        {
                            "target": "b31",
                            "text": "(Wang et al. 2021"
                        },
                        {
                            "target": "b35",
                            "text": "(Yang et al. 2022"
                        },
                        {
                            "target": "b11",
                            "text": "(He et al. 2022;"
                        },
                        {
                            "target": "b2",
                            "text": "Chen et al. 2022)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "2.2",
            "title": "Masked Image Modeling",
            "content": [
                "Inspired by Masked Language Modeling (MLM) (Devlin et al. 2018) in NLP, Masked Image Modeling (MIM) is adopted in visual pre-training and has shown impressive results in downstream visual tasks. Existing MIM works can be classified into two categories. Methods in the first category predict discrete tokens generated by VQ-VAE (Van Den Oord, Vinyals et al. 2017) or its variants, such as BEiT (Bao, Dong, and Wei 2021), mc-BEiT (Li et al. 2022) and PeCo (Dong et al. 2021). Methods of the second category adopt masking strategies to exploit the redundancy nature of images for visual pre-training. MaskFeat (Wei et al. 2022) randomly masks a portion of video sequence and regresses Histograms of Oriented Gradients (HOG) features of masked regions. MAE (He et al. 2022) and SimMIM (Xie et al. 2022) predict pixel RGB values directly to promote image pre-training, achieving even better performance than complicatedly designed token classification methods.",
                "Most recently, several concurrent works explore the masked token/patch prediction task for vision-language pretraining. M3AE (Geng et al. 2022) randomly masks the unified sequence of image patches and text tokens, and encodes the visible image patches into embeddings with the same dimension as the language embeddings to perform joint training of the two modalities. Due to the lack of alignment and interaction between the two modalities during pretraining, it is hard to apply directly to various multi-modal downstream tasks. VLC (Gui et al. 2022) initializes the vision backbone from the pre-trained MAE model, thus avoiding supervised training, and performs intra-modal reconstruction via masked image/language modeling. However, the performance in downstream tasks can be sub-optimal due to the lack of elaborate arrangements between MIM and other proxy tasks. Meanwhile, VLC suffers from severe pre-training burden due to the large image input size (e.g., 384) and full patches input. In comparison, our VL-MAE adopts a simple yet effective design to simultaneously employ image-text contrastive learning and visual generative learning, achieving SOTA performance with less pretraining cost."
            ],
            "paragraphs": [
                {
                    "text": "Inspired by Masked Language Modeling (MLM) (Devlin et al. 2018) in NLP, Masked Image Modeling (MIM) is adopted in visual pre-training and has shown impressive results in downstream visual tasks. Existing MIM works can be classified into two categories. Methods in the first category predict discrete tokens generated by VQ-VAE (Van Den Oord, Vinyals et al. 2017) or its variants, such as BEiT (Bao, Dong, and Wei 2021), mc-BEiT (Li et al. 2022) and PeCo (Dong et al. 2021). Methods of the second category adopt masking strategies to exploit the redundancy nature of images for visual pre-training. MaskFeat (Wei et al. 2022) randomly masks a portion of video sequence and regresses Histograms of Oriented Gradients (HOG) features of masked regions. MAE (He et al. 2022) and SimMIM (Xie et al. 2022) predict pixel RGB values directly to promote image pre-training, achieving even better performance than complicatedly designed token classification methods.",
                    "citations": [
                        {
                            "target": "b5",
                            "text": "(Devlin et al. 2018)"
                        },
                        {
                            "target": "b29",
                            "text": "(Van Den Oord, Vinyals et al. 2017)"
                        },
                        {
                            "target": "b1",
                            "text": "(Bao, Dong, and Wei 2021)"
                        },
                        {
                            "target": "b18",
                            "text": "(Li et al. 2022)"
                        },
                        {
                            "target": "b6",
                            "text": "(Dong et al. 2021)"
                        },
                        {
                            "target": "b32",
                            "text": "(Wei et al. 2022)"
                        },
                        {
                            "target": "b11",
                            "text": "(He et al. 2022)"
                        },
                        {
                            "target": "b34",
                            "text": "(Xie et al. 2022)"
                        }
                    ]
                },
                {
                    "text": "Most recently, several concurrent works explore the masked token/patch prediction task for vision-language pretraining. M3AE (Geng et al. 2022) randomly masks the unified sequence of image patches and text tokens, and encodes the visible image patches into embeddings with the same dimension as the language embeddings to perform joint training of the two modalities. Due to the lack of alignment and interaction between the two modalities during pretraining, it is hard to apply directly to various multi-modal downstream tasks. VLC (Gui et al. 2022) initializes the vision backbone from the pre-trained MAE model, thus avoiding supervised training, and performs intra-modal reconstruction via masked image/language modeling. However, the performance in downstream tasks can be sub-optimal due to the lack of elaborate arrangements between MIM and other proxy tasks. Meanwhile, VLC suffers from severe pre-training burden due to the large image input size (e.g., 384) and full patches input. In comparison, our VL-MAE adopts a simple yet effective design to simultaneously employ image-text contrastive learning and visual generative learning, achieving SOTA performance with less pretraining cost.",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "(Geng et al. 2022"
                        },
                        {
                            "target": "b10",
                            "text": "(Gui et al. 2022"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3",
            "title": "Methodology",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "3.1",
            "title": "Observation",
            "content": [
                "As image-text dataset plays a fundamental role in visionlanguage pre-training tasks, we first explore the prevalent pre-training datasets. Concretely, we calculate the similarity between image and text embedding extracted by a pretrained CLIP model (Radford et al. 2021). Figure 2 shows the distribution of similarity and samples from different intervals. In low relevance interval, texts only depict the background or \"unsalient\" regions, and in medium relevance interval, texts describe the \"salient\" objects but ignore some details. For contrastive learning based methods (Li et al. 2021;Yang et al. 2022), this information disparity between Shadow of tree and leaves on pavement.",
                "Closed brown wooden door."
            ],
            "paragraphs": [
                {
                    "text": "As image-text dataset plays a fundamental role in visionlanguage pre-training tasks, we first explore the prevalent pre-training datasets. Concretely, we calculate the similarity between image and text embedding extracted by a pretrained CLIP model (Radford et al. 2021). Figure 2 shows the distribution of similarity and samples from different intervals. In low relevance interval, texts only depict the background or \"unsalient\" regions, and in medium relevance interval, texts describe the \"salient\" objects but ignore some details. For contrastive learning based methods (Li et al. 2021;Yang et al. 2022), this information disparity between Shadow of tree and leaves on pavement.",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "(Radford et al. 2021)"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021;"
                        },
                        {
                            "target": "b35",
                            "text": "Yang et al. 2022)"
                        }
                    ]
                },
                {
                    "text": "Closed brown wooden door.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "Low Relevance",
            "content": [
                "The cow is black and white.",
                "Street signs on a metal pole."
            ],
            "paragraphs": [
                {
                    "text": "The cow is black and white.",
                    "citations": []
                },
                {
                    "text": "Street signs on a metal pole.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "Medium Relevance",
            "content": [
                "Red sling backs with apples.",
                "Cat sitting inside open umbrella.",
                "High Relevance image and text may cause the focal bias problem because image encoder tends to pay more attention to the text-relevant area for better alignment between visual and textual embedding. Therefore, we introduce generative learning to alleviate such bias during the pre-training phase."
            ],
            "paragraphs": [
                {
                    "text": "Red sling backs with apples.",
                    "citations": []
                },
                {
                    "text": "Cat sitting inside open umbrella.",
                    "citations": []
                },
                {
                    "text": "High Relevance image and text may cause the focal bias problem because image encoder tends to pay more attention to the text-relevant area for better alignment between visual and textual embedding. Therefore, we introduce generative learning to alleviate such bias during the pre-training phase.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.2",
            "title": "Model Architecture",
            "content": [
                "The overall architecture of our VLMAE model is illustrated in Figure 3. VLMAE contains an image encoder g(•) and a text encoder h(•) to extract uni-modal features. An image decoder is introduced to reconstruct masked patches, and the fusion learner aggregates the vision-language features for multi-modal tasks. Similar to previous works (Li et al. 2021;Yang et al. 2022), for each encoder, we maintain a momentum counterpart ĝ(•), ĥ(•) and update their parameters following θ ĝ = mθ ĝ + (1 -m)θ g ."
            ],
            "paragraphs": [
                {
                    "text": "The overall architecture of our VLMAE model is illustrated in Figure 3. VLMAE contains an image encoder g(•) and a text encoder h(•) to extract uni-modal features. An image decoder is introduced to reconstruct masked patches, and the fusion learner aggregates the vision-language features for multi-modal tasks. Similar to previous works (Li et al. 2021;Yang et al. 2022), for each encoder, we maintain a momentum counterpart ĝ(•), ĥ(•) and update their parameters following θ ĝ = mθ ĝ + (1 -m)θ g .",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021;"
                        },
                        {
                            "target": "b35",
                            "text": "Yang et al. 2022)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.3",
            "title": "Feature Extraction",
            "content": [
                "Given an image I, we split it into non-overlapping patches and project them into a sequence of patch embeddings,",
                ", where N is the number of patches. Following MAE (He et al. 2022), we randomly divide the patches into visible patches x vis"
            ],
            "paragraphs": [
                {
                    "text": "Given an image I, we split it into non-overlapping patches and project them into a sequence of patch embeddings,",
                    "citations": []
                },
                {
                    "text": ", where N is the number of patches. Following MAE (He et al. 2022), we randomly divide the patches into visible patches x vis",
                    "citations": [
                        {
                            "target": "b11",
                            "text": "(He et al. 2022)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.4",
            "title": "Regional Mask Image Modeling",
            "content": [
                "Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion. Concretely,we concatenate the masked visual representations v cls , v vis 1 , . . . , v vis N -M and text representations {w cls , w 1 , . . . , w L } and feed them into the image decoder. Specifically, for the text with bounding box annotation, we propose to apply a regional reconstruction loss to facilitate the aggregation of multi-modal features. Formally, the reconstruction loss is defined as:",
                "where D is the pre-training dataset and T is the input text. I vis denotes the visible patches and I msk denotes the masked patches in text-relevant region. For text without bounding box annotation, we treat the whole image as text-relevant region. The objective f mim calculates the mean squared error (MSE) between the reconstructed and original images at the pixel level."
            ],
            "paragraphs": [
                {
                    "text": "Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion. Concretely,we concatenate the masked visual representations v cls , v vis 1 , . . . , v vis N -M and text representations {w cls , w 1 , . . . , w L } and feed them into the image decoder. Specifically, for the text with bounding box annotation, we propose to apply a regional reconstruction loss to facilitate the aggregation of multi-modal features. Formally, the reconstruction loss is defined as:",
                    "citations": [
                        {
                            "target": "b11",
                            "text": "(He et al. 2022)"
                        }
                    ]
                },
                {
                    "text": "where D is the pre-training dataset and T is the input text. I vis denotes the visible patches and I msk denotes the masked patches in text-relevant region. For text without bounding box annotation, we treat the whole image as text-relevant region. The objective f mim calculates the mean squared error (MSE) between the reconstructed and original images at the pixel level.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.5",
            "title": "Image Feature Reconstruction",
            "content": [
                "Because only parts of image patches are fed into the image encoder while the remaining patches are invisible, it causes information insufficiency. In order to enhance the representation ability of the model, we propose the image feature reconstruction task, which requires the image encoder to reconstruct the global feature generated by momentum image encoder. Concretely, we take the output [CLS] embedding vcls of momentum image encoder ĝ(•), which is given by the complete image, as the target feature. The reconstruction loss is denoted as",
                "The reconstruction objective f r is L1 loss in practice."
            ],
            "paragraphs": [
                {
                    "text": "Because only parts of image patches are fed into the image encoder while the remaining patches are invisible, it causes information insufficiency. In order to enhance the representation ability of the model, we propose the image feature reconstruction task, which requires the image encoder to reconstruct the global feature generated by momentum image encoder. Concretely, we take the output [CLS] embedding vcls of momentum image encoder ĝ(•), which is given by the complete image, as the target feature. The reconstruction loss is denoted as",
                    "citations": []
                },
                {
                    "text": "The reconstruction objective f r is L1 loss in practice.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.6",
            "title": "Image-Text Contrastive Learning",
            "content": [
                "To further fuse the vision and language representations, we introduce common vision-language pre-training tasks, namely, image-text contrastive learning (ITC), image-text matching (ITM), and masked language modeling (MLM).",
                "To facilitate the alignment among the features of image and its corresponding texts, we introduce contrastive learning between image feature v cls and text feature w cls . The similarity function is formatted as s(I, T ) = φ v (v cls ) φw ( ŵcls ) and s(T, I) = φ w (w cls ) φv (v cls ), where φ(•) is a linear projection. Given a batch of imagetext pairs, the image-text and text-image similarities are:",
                "where B is the batchsize and τ is a learnable temperature parameter. The image-text contrastive loss is defined as:",
                ") where H(; ) is the cross-entropy. y i2t and y t2i are ground truth logits which are guided by momentum distillation following previous work (Li et al. 2021)."
            ],
            "paragraphs": [
                {
                    "text": "To further fuse the vision and language representations, we introduce common vision-language pre-training tasks, namely, image-text contrastive learning (ITC), image-text matching (ITM), and masked language modeling (MLM).",
                    "citations": []
                },
                {
                    "text": "To facilitate the alignment among the features of image and its corresponding texts, we introduce contrastive learning between image feature v cls and text feature w cls . The similarity function is formatted as s(I, T ) = φ v (v cls ) φw ( ŵcls ) and s(T, I) = φ w (w cls ) φv (v cls ), where φ(•) is a linear projection. Given a batch of imagetext pairs, the image-text and text-image similarities are:",
                    "citations": []
                },
                {
                    "text": "where B is the batchsize and τ is a learnable temperature parameter. The image-text contrastive loss is defined as:",
                    "citations": []
                },
                {
                    "text": ") where H(; ) is the cross-entropy. y i2t and y t2i are ground truth logits which are guided by momentum distillation following previous work (Li et al. 2021).",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.7",
            "title": "Image-Text Matching",
            "content": [
                "As a widely used training objective in VLP, ITM aims to determine whether a pair of image and text is matched. Concretely, we take the output [CLS] embedding of fusion learner as the multi-modal representation to predict the correspondence between image and text. The objective L itm is defined as: where p itm is the predicted probability. y itm is the grounding truth which evaluates to 1 iff that the input image and text are matched."
            ],
            "paragraphs": [
                {
                    "text": "As a widely used training objective in VLP, ITM aims to determine whether a pair of image and text is matched. Concretely, we take the output [CLS] embedding of fusion learner as the multi-modal representation to predict the correspondence between image and text. The objective L itm is defined as: where p itm is the predicted probability. y itm is the grounding truth which evaluates to 1 iff that the input image and text are matched.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.8",
            "title": "Masked Language Modeling",
            "content": [
                "Following BERT (Devlin et al. 2018), MLM aims to predict the masked words based on visual and textual features. We randomly mask out 15% tokens in an input sentence. Each token is replaced with the [MASK] token, a random word, or left unchanged, with the probability of 80%, 10% and 10%, respectively. Denote masked text input as T msk , the L mlm is formatted as:",
                "where p msk is the predicted probability of the masked token and y msk is the ground truth distribution.",
                "The training objective of VLMAE is summarized as follows:"
            ],
            "paragraphs": [
                {
                    "text": "Following BERT (Devlin et al. 2018), MLM aims to predict the masked words based on visual and textual features. We randomly mask out 15% tokens in an input sentence. Each token is replaced with the [MASK] token, a random word, or left unchanged, with the probability of 80%, 10% and 10%, respectively. Denote masked text input as T msk , the L mlm is formatted as:",
                    "citations": [
                        {
                            "target": "b5",
                            "text": "(Devlin et al. 2018)"
                        }
                    ]
                },
                {
                    "text": "where p msk is the predicted probability of the masked token and y msk is the ground truth distribution.",
                    "citations": []
                },
                {
                    "text": "The training objective of VLMAE is summarized as follows:",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.2",
            "title": "Implementation Details",
            "content": [
                "In VLMAE, we apply a 12-layer ViT-B/16 (Dosovitskiy et al. 2020) as our image encoder, which is initialized by Im-ageNet 1K pre-trained weights. Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder. The text encoder is implemented by the first 6 layers of BERT base and the last 6 layers are taken as the fusion learner. During the pre-training stage, we train the model for 30 epochs with a total batch size of 512 on 8 GPUs. We utilize AdamW (Loshchilov and Hutter 2017) optimizer with weight decay of 0.02. The learning rate is warmed-up to 1e-4 in the first 1000 iterations and decayed to 1e-5 following a cosine schedule. Following (Li et al. 2021;Yang et al. 2022), we crop the image into 256 × 256 and apply random color jittering, random grayscale conversion, random Gaussian Blur, random horizontal flip, and RandAugment (Cubuk et al. 2020) on it. During the fine-tuning stage, we increase the image resolution to 384 × 384."
            ],
            "paragraphs": [
                {
                    "text": "In VLMAE, we apply a 12-layer ViT-B/16 (Dosovitskiy et al. 2020) as our image encoder, which is initialized by Im-ageNet 1K pre-trained weights. Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder. The text encoder is implemented by the first 6 layers of BERT base and the last 6 layers are taken as the fusion learner. During the pre-training stage, we train the model for 30 epochs with a total batch size of 512 on 8 GPUs. We utilize AdamW (Loshchilov and Hutter 2017) optimizer with weight decay of 0.02. The learning rate is warmed-up to 1e-4 in the first 1000 iterations and decayed to 1e-5 following a cosine schedule. Following (Li et al. 2021;Yang et al. 2022), we crop the image into 256 × 256 and apply random color jittering, random grayscale conversion, random Gaussian Blur, random horizontal flip, and RandAugment (Cubuk et al. 2020) on it. During the fine-tuning stage, we increase the image resolution to 384 × 384.",
                    "citations": [
                        {
                            "target": "b7",
                            "text": "(Dosovitskiy et al. 2020"
                        },
                        {
                            "target": "b11",
                            "text": "(He et al. 2022)"
                        },
                        {
                            "target": "b20",
                            "text": "(Loshchilov and Hutter 2017)"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021;"
                        },
                        {
                            "target": "b35",
                            "text": "Yang et al. 2022)"
                        },
                        {
                            "target": "b4",
                            "text": "(Cubuk et al. 2020"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.3",
            "title": "Downstream Tasks",
            "content": [
                "Image-Text Retrieval Image-Text Retrieval consists of two subtasks, namely, image-to-text retrieval (TR) and text-to-image retrieval (IR). We evaluate our model on Flickr30K (Plummer et al. 2015) and COCO (Lin et al. 2014) dataset. In testing data, Flickr30K contains 1K images and 5K texts, and COCO contains 5K images and 25K texts. Following the same protocol as ALBEF (Li et al. 2021), we conduct experiments on the fine-tuning and zero-shot settings. In the fine-tuning setting, we fine-tune our pre-trained model on training data and test it on validation/test data.",
                "In the zero-shot setting, following (Li et al. 2021;Yang et al. 2022), we fine-tune the model on COCO and test it on Flickr30k.",
                "Visual Question Answering (VQA) In the visual question answering (Goyal et al. 2017) task, given an image and a text question, the model is expected to understand features of both modals and provide a textual answer. Following the same setting in (Li et al. 2021), we treat this task as a generation problem and introduce an answer decoder on top of the fusion learner. Given the multi-model features, the answer decoder is required to generate the answer from 3192 candidates.",
                "Visual Entailment (SNLI-VE) visual entailment (Xie et al. 2019) task requires the model to judge the relation between an image and a text is entailment, neutral, or contradictory. Following the same setting in (Li et al. 2021), we consider this task as a three-way classification problem and generate the prediction using an MLP on the representation of [CLS] token of the text decoder.",
                "Visual Reasoning (NLVR2) In visual reasoning (Suhr et al. 2018) task, given an image-text pair, the model aims to predict whether a text describes a pair of images. Following (Li et al. 2021), we evaluate our model on NLVR2 dataset, which consists of 107,292 image-text examples, and extend the model to suit paired images input.",
                "Visual Grounding (VG) In visual grounding task, the model aims to localize the region in an image corresponding to a specific textual description. Following (Li et al. 2021), we study this task on weakly-supervised setting and evaluate our model on RefCOCO+ (Yu et al. 2016) dataset. Concretely, we fine-tune the pre-trained model with image-text pairs without any bounding box annotations. During inference, we generate heatmaps by Grad-CAM (Selvaraju et al. 2017) and rank the proposals generated by MAttNet (Yu et al. 2018) based on these heatmaps."
            ],
            "paragraphs": [
                {
                    "text": "Image-Text Retrieval Image-Text Retrieval consists of two subtasks, namely, image-to-text retrieval (TR) and text-to-image retrieval (IR). We evaluate our model on Flickr30K (Plummer et al. 2015) and COCO (Lin et al. 2014) dataset. In testing data, Flickr30K contains 1K images and 5K texts, and COCO contains 5K images and 25K texts. Following the same protocol as ALBEF (Li et al. 2021), we conduct experiments on the fine-tuning and zero-shot settings. In the fine-tuning setting, we fine-tune our pre-trained model on training data and test it on validation/test data.",
                    "citations": [
                        {
                            "target": "b23",
                            "text": "(Plummer et al. 2015)"
                        },
                        {
                            "target": "b19",
                            "text": "(Lin et al. 2014)"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                },
                {
                    "text": "In the zero-shot setting, following (Li et al. 2021;Yang et al. 2022), we fine-tune the model on COCO and test it on Flickr30k.",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021;"
                        },
                        {
                            "target": "b35",
                            "text": "Yang et al. 2022)"
                        }
                    ]
                },
                {
                    "text": "Visual Question Answering (VQA) In the visual question answering (Goyal et al. 2017) task, given an image and a text question, the model is expected to understand features of both modals and provide a textual answer. Following the same setting in (Li et al. 2021), we treat this task as a generation problem and introduce an answer decoder on top of the fusion learner. Given the multi-model features, the answer decoder is required to generate the answer from 3192 candidates.",
                    "citations": [
                        {
                            "target": "b9",
                            "text": "(Goyal et al. 2017"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                },
                {
                    "text": "Visual Entailment (SNLI-VE) visual entailment (Xie et al. 2019) task requires the model to judge the relation between an image and a text is entailment, neutral, or contradictory. Following the same setting in (Li et al. 2021), we consider this task as a three-way classification problem and generate the prediction using an MLP on the representation of [CLS] token of the text decoder.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "(Xie et al. 2019"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                },
                {
                    "text": "Visual Reasoning (NLVR2) In visual reasoning (Suhr et al. 2018) task, given an image-text pair, the model aims to predict whether a text describes a pair of images. Following (Li et al. 2021), we evaluate our model on NLVR2 dataset, which consists of 107,292 image-text examples, and extend the model to suit paired images input.",
                    "citations": [
                        {
                            "target": "b28",
                            "text": "(Suhr et al. 2018"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                },
                {
                    "text": "Visual Grounding (VG) In visual grounding task, the model aims to localize the region in an image corresponding to a specific textual description. Following (Li et al. 2021), we study this task on weakly-supervised setting and evaluate our model on RefCOCO+ (Yu et al. 2016) dataset. Concretely, we fine-tune the pre-trained model with image-text pairs without any bounding box annotations. During inference, we generate heatmaps by Grad-CAM (Selvaraju et al. 2017) and rank the proposals generated by MAttNet (Yu et al. 2018) based on these heatmaps.",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        },
                        {
                            "target": "b37",
                            "text": "(Yu et al. 2016"
                        },
                        {
                            "target": "b26",
                            "text": "(Selvaraju et al. 2017)"
                        },
                        {
                            "target": "b36",
                            "text": "(Yu et al. 2018)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.4",
            "title": "Evaluation on Image-Text Retrieval",
            "content": [
                "We first evaluate our model on image-text retrieval task, which is the most common downstream task in visionlanguage pre-training. Image Retrieval Text Retrieval Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 OSCAR 4M 70.0 91. Table 2: Performance comparison of fine-tuned image-text retrieval on Flickr30K and COCO datasets.",
                "Method TR IR R@1 R@5 R@10 R@1 R@5 R@10 ALBEF 90. on fine-tuned and zero-shot settings, respectively. In finetuned image-text retrieval, our model surpasses the previous state-of-the-art TCL (Yang et al. 2022) on MSCOCO dataset, reaching 77.3% and 59.6% in terms of TR@1 and IR@1, respectively. On Flickr30k dataset, VLMAE has a comparable performance with TCL and outperforms previous work ALBEF (Li et al. 2021). In the zero-shot setting, VLMAE surpasses TCL on text retrieval subtask but has an inferior performance on image retrieval. We speculate that this is because, compared with other methods (Li et al. 2021)"
            ],
            "paragraphs": [
                {
                    "text": "We first evaluate our model on image-text retrieval task, which is the most common downstream task in visionlanguage pre-training. Image Retrieval Text Retrieval Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 OSCAR 4M 70.0 91. Table 2: Performance comparison of fine-tuned image-text retrieval on Flickr30K and COCO datasets.",
                    "citations": []
                },
                {
                    "text": "Method TR IR R@1 R@5 R@10 R@1 R@5 R@10 ALBEF 90. on fine-tuned and zero-shot settings, respectively. In finetuned image-text retrieval, our model surpasses the previous state-of-the-art TCL (Yang et al. 2022) on MSCOCO dataset, reaching 77.3% and 59.6% in terms of TR@1 and IR@1, respectively. On Flickr30k dataset, VLMAE has a comparable performance with TCL and outperforms previous work ALBEF (Li et al. 2021). In the zero-shot setting, VLMAE surpasses TCL on text retrieval subtask but has an inferior performance on image retrieval. We speculate that this is because, compared with other methods (Li et al. 2021)",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "(Yang et al. 2022"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021"
                        },
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.6",
            "title": "Evaluation on Visual Grounding",
            "content": [
                "As shown in Table 5, VLMAE surpasses the previous stateof-the-art ALBEF (Li et al. 2021) by a large margin, reaching 71.6% and 50.7% accuracy on Test data. Similar to the finding in CAE (Chen et al. 2022), we argue that the reconstruction of pixels in an image makes the model focus on almost every patch instead of only the salient region, resulting in better performance on location tasks (see Section 4.8 and Figure 4)."
            ],
            "paragraphs": [
                {
                    "text": "As shown in Table 5, VLMAE surpasses the previous stateof-the-art ALBEF (Li et al. 2021) by a large margin, reaching 71.6% and 50.7% accuracy on Test data. Similar to the finding in CAE (Chen et al. 2022), we argue that the reconstruction of pixels in an image makes the model focus on almost every patch instead of only the salient region, resulting in better performance on location tasks (see Section 4.8 and Figure 4).",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021"
                        },
                        {
                            "target": "b2",
                            "text": "(Chen et al. 2022)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.7",
            "title": "Ablation Study",
            "content": [
                "In this section, we conduct ablation studies on image-text retrieval task to validate the effectiveness of the newly proposed RMIM and IFR. Table 6 shows the results on both MSCOCO and Flickr30k datasets. We consider the VLMAE trained with only ITC, ITM, and MLM loss as our baseline model. With the addition of MIM loss, model achieves better performance on both IR and TR tasks. We argue that the reconstruction of pixels can facilitate the model to capture more comprehensive feature of the image. Moreover, introducing regional MIM loss further improves the model performance, which can be attributed to better image-text align-  We also explore the effect of the mask ratio, which is a trade-off between training efficiency and model performance. Table 7 shows the results with different mask ratios. Compared with ImageNet (Russakovsky et al. 2015) dataset, the content of images in vision-language pretraining datasets is more complicated. We argue that a too large mask ratio (e.g., 75%) may cause severe information loss, leading to a performance drop. In practice, we choose a 50% mask ratio to balance training efficiency and performance."
            ],
            "paragraphs": [
                {
                    "text": "In this section, we conduct ablation studies on image-text retrieval task to validate the effectiveness of the newly proposed RMIM and IFR. Table 6 shows the results on both MSCOCO and Flickr30k datasets. We consider the VLMAE trained with only ITC, ITM, and MLM loss as our baseline model. With the addition of MIM loss, model achieves better performance on both IR and TR tasks. We argue that the reconstruction of pixels can facilitate the model to capture more comprehensive feature of the image. Moreover, introducing regional MIM loss further improves the model performance, which can be attributed to better image-text align-  We also explore the effect of the mask ratio, which is a trade-off between training efficiency and model performance. Table 7 shows the results with different mask ratios. Compared with ImageNet (Russakovsky et al. 2015) dataset, the content of images in vision-language pretraining datasets is more complicated. We argue that a too large mask ratio (e.g., 75%) may cause severe information loss, leading to a performance drop. In practice, we choose a 50% mask ratio to balance training efficiency and performance.",
                    "citations": [
                        {
                            "target": "b25",
                            "text": "(Russakovsky et al. 2015)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.8",
            "title": "Qualitative Assessment",
            "content": [
                "To demonstrate the effectiveness of our model, following ALBEF (Li et al. 2021), we visualize the cross-attention maps in visual grounding task. As shown in Figure 4, compared with ALBEF, VLMAE can identify the relevant region more precisely and pay attention to the whole relevant area."
            ],
            "paragraphs": [
                {
                    "text": "To demonstrate the effectiveness of our model, following ALBEF (Li et al. 2021), we visualize the cross-attention maps in visual grounding task. As shown in Figure 4, compared with ALBEF, VLMAE can identify the relevant region more precisely and pay attention to the whole relevant area.",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.9",
            "title": "Investigation About Focal Bias",
            "content": [
                "To demonstrate that VLMAE can alleviate the focal bias problem, we explore the attention weights between the [CLS] token and the patch tokens of the image encoder. Results are shown in Figure 5. Since the text in pre-training dataset only depicts parts of the image (e.g., \"A bridge over the small river\"), ALBEF neglects other irrelevant regions (e.g., \"River bank\"). However, thanks to generative learning, VLMAE pays attention to most patches and provides"
            ],
            "paragraphs": [
                {
                    "text": "To demonstrate that VLMAE can alleviate the focal bias problem, we explore the attention weights between the [CLS] token and the patch tokens of the image encoder. Results are shown in Figure 5. Since the text in pre-training dataset only depicts parts of the image (e.g., \"A bridge over the small river\"), ALBEF neglects other irrelevant regions (e.g., \"River bank\"). However, thanks to generative learning, VLMAE pays attention to most patches and provides",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.10",
            "title": "Pre-training Efficiency",
            "content": [
                "To demonstrate the efficiency of our proposed method, in Table 8, we compare VLMAE with previous works (Li et al. 2021;Yang et al. 2022)"
            ],
            "paragraphs": [
                {
                    "text": "To demonstrate the efficiency of our proposed method, in Table 8, we compare VLMAE with previous works (Li et al. 2021;Yang et al. 2022)",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "(Li et al. 2021;"
                        },
                        {
                            "target": "b35",
                            "text": "Yang et al. 2022)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "5",
            "title": "Conclusion",
            "content": [
                "In this paper, we propose a vision-language masked autoencoder framework (VLMAE). VLMAE utilizes an asymmetric encoder-decoder architecture, consisting of a fusion learner for aggregating multi-modal information and a lightweight image decoder for reconstructing pixels. Due to its simple yet effective design, VLMAE can successfully handle the focal bias problem in pre-training, providing a more comprehensive understanding of the image. Moreover, VLMAE utilizes Regional Masked Image Modeling to stabilize pixel-level reconstruction and Image Feature Reconstruction to improve the model's representation ability. Extensive experiments on various downstream vision-language benchmarks demonstrate that VLMAE surpasses existing state-of-the-art methods while prominently alleviating the training cost in the pre-training stage.",
                "Another view of the new flower bed near the garage area remember it was all gravel.",
                "Nothing says fun like a girl in pink leotards on a pink tall bike wielding a sword that's on fire.",
                "An olive tree in your bathroom turns it into a Mediterranean spa.",
                "Pickup truck outside the bar in red hook.",
                "Oldest wooden bridge in the world Luzern.",
                "A new graduate by the clock tower at Otago university.",
                "It must be great being a gardener in Hastings the flower beds are beautifully kept.",
                "The 42-foot long slate wall is a monument to the first amendment built by the Thomas Jefferson Center for free expression.",
                "This was the kitchen in the house we rented at nags head north Carolina back in the summer of 04 great vacation.",
                "Coffee table and small peace lily indoor plant with vase as shown below.",
                "Neutral bedroom + black floor + lush tree California home by Nickey Kehoe.",
                "Icelandic girl jumping into Ohrid lake in Macedonia."
            ],
            "paragraphs": [
                {
                    "text": "In this paper, we propose a vision-language masked autoencoder framework (VLMAE). VLMAE utilizes an asymmetric encoder-decoder architecture, consisting of a fusion learner for aggregating multi-modal information and a lightweight image decoder for reconstructing pixels. Due to its simple yet effective design, VLMAE can successfully handle the focal bias problem in pre-training, providing a more comprehensive understanding of the image. Moreover, VLMAE utilizes Regional Masked Image Modeling to stabilize pixel-level reconstruction and Image Feature Reconstruction to improve the model's representation ability. Extensive experiments on various downstream vision-language benchmarks demonstrate that VLMAE surpasses existing state-of-the-art methods while prominently alleviating the training cost in the pre-training stage.",
                    "citations": []
                },
                {
                    "text": "Another view of the new flower bed near the garage area remember it was all gravel.",
                    "citations": []
                },
                {
                    "text": "Nothing says fun like a girl in pink leotards on a pink tall bike wielding a sword that's on fire.",
                    "citations": []
                },
                {
                    "text": "An olive tree in your bathroom turns it into a Mediterranean spa.",
                    "citations": []
                },
                {
                    "text": "Pickup truck outside the bar in red hook.",
                    "citations": []
                },
                {
                    "text": "Oldest wooden bridge in the world Luzern.",
                    "citations": []
                },
                {
                    "text": "A new graduate by the clock tower at Otago university.",
                    "citations": []
                },
                {
                    "text": "It must be great being a gardener in Hastings the flower beds are beautifully kept.",
                    "citations": []
                },
                {
                    "text": "The 42-foot long slate wall is a monument to the first amendment built by the Thomas Jefferson Center for free expression.",
                    "citations": []
                },
                {
                    "text": "This was the kitchen in the house we rented at nags head north Carolina back in the summer of 04 great vacation.",
                    "citations": []
                },
                {
                    "text": "Coffee table and small peace lily indoor plant with vase as shown below.",
                    "citations": []
                },
                {
                    "text": "Neutral bedroom + black floor + lush tree California home by Nickey Kehoe.",
                    "citations": []
                },
                {
                    "text": "Icelandic girl jumping into Ohrid lake in Macedonia.",
                    "citations": []
                }
            ]
        }
    ],
    "references": [
        {
            "id": "b0",
            "title": "Robust Cross-Modal Representation Learning with Progressive Self-Distillation",
            "authors": [
                "A Andonian",
                "S Chen",
                "R Hamid"
            ],
            "date": "2022",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b1",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "Beit: Bert pre-training of image transformers"
        },
        {
            "id": "b2",
            "title": "",
            "authors": [],
            "date": "2022",
            "journal": "Context autoencoder for self-supervised representation learning"
        },
        {
            "id": "b3",
            "title": "Uniter: Universal image-text representation learning",
            "authors": [
                "Y.-C Chen",
                "L Li",
                "L Yu",
                "A El Kholy",
                "F Ahmed",
                "Z Gan",
                "Y Cheng",
                "J Liu"
            ],
            "date": "2020",
            "journal": "European conference on computer vision"
        },
        {
            "id": "b4",
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "authors": [
                "E Cubuk",
                "B Zoph",
                "J Shlens",
                "Q Le"
            ],
            "date": "2020",
            "journal": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops"
        },
        {
            "id": "b5",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Bert: Pre-training of deep bidirectional transformers for language understanding"
        },
        {
            "id": "b6",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "Peco: Perceptual codebook for bert pre-training of vision transformers"
        },
        {
            "id": "b7",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "An image is worth 16x16 words: Transformers for image recognition at scale"
        },
        {
            "id": "b8",
            "title": "",
            "authors": [],
            "date": "2022",
            "journal": "Multimodal Masked Autoencoders Learn Transferable Representations"
        },
        {
            "id": "b9",
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "authors": [
                "Y Goyal",
                "T Khot",
                "D Summers-Stay",
                "D Batra",
                "D Parikh"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b10",
            "title": "",
            "authors": [],
            "date": "2022",
            "journal": "Training Vision-Language Transformers from Captions Alone"
        },
        {
            "id": "b11",
            "title": "Masked autoencoders are scalable vision learners",
            "authors": [
                "K He",
                "X Chen",
                "S Xie",
                "Y Li",
                "P Dollár",
                "R Girshick"
            ],
            "date": "2022. 16000-16009. 2, 3, 4, 5, 6",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b12",
            "title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
            "authors": [
                "Z Huang",
                "Z Zeng",
                "Y Huang",
                "B Liu",
                "D Fu",
                "J Fu"
            ],
            "date": "2021",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b13",
            "title": "Vilt: Vision-andlanguage transformer without convolution or region supervision",
            "authors": [
                "W Kim",
                "B Son",
                "I Kim"
            ],
            "date": "2021",
            "journal": "International Conference on Machine Learning"
        },
        {
            "id": "b14",
            "title": "",
            "authors": [],
            "date": "2013",
            "journal": "Auto-encoding variational bayes"
        },
        {
            "id": "b15",
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "authors": [
                "R Krishna",
                "Y Zhu",
                "O Groth",
                "J Johnson",
                "K Hata",
                "J Kravitz",
                "S Chen",
                "Y Kalantidis",
                "L.-J Li",
                "D Shamma"
            ],
            "date": "2017",
            "journal": "International journal of computer vision"
        },
        {
            "id": "b16",
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
            "authors": [
                "G Li",
                "N Duan",
                "Y Fang",
                "M Gong",
                "D Jiang"
            ],
            "date": "2020",
            "journal": "Proceedings of the AAAI Conference on Artificial Intelligence"
        },
        {
            "id": "b17",
            "title": "",
            "authors": [],
            "date": "2021. 1, 2, 3, 4, 5, 6, 7, 10",
            "journal": "Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems"
        },
        {
            "id": "b18",
            "title": "",
            "authors": [],
            "date": "2022",
            "journal": "mc-BEiT: Multi-choice Discretization for Image BERT Pre-training"
        },
        {
            "id": "b19",
            "title": "",
            "authors": [],
            "date": "2014",
            "journal": "Microsoft coco: Common objects in context"
        },
        {
            "id": "b20",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Decoupled weight decay regularization"
        },
        {
            "id": "b21",
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "authors": [
                "J Lu",
                "D Batra",
                "D Parikh",
                "S Lee"
            ],
            "date": "2019",
            "journal": "Advances in neural information processing systems"
        },
        {
            "id": "b22",
            "title": "",
            "authors": [],
            "date": "2011",
            "journal": "Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems"
        },
        {
            "id": "b23",
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
            "authors": [
                "B Plummer",
                "L Wang",
                "C Cervantes",
                "J Caicedo",
                "J Hockenmaier",
                "S Lazebnik"
            ],
            "date": "2015",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b24",
            "title": "Learning transferable visual models from natural language supervision",
            "authors": [
                "A Radford",
                "J Kim",
                "C Hallacy",
                "A Ramesh",
                "G Goh",
                "S Agarwal",
                "G Sastry",
                "A Askell",
                "P Mishkin",
                "J Clark"
            ],
            "date": "2021",
            "journal": "International Conference on Machine Learning"
        },
        {
            "id": "b25",
            "title": "Imagenet large scale visual recognition challenge",
            "authors": [
                "O Russakovsky",
                "J Deng",
                "H Su",
                "J Krause",
                "S Satheesh",
                "S Ma",
                "Z Huang",
                "A Karpathy",
                "A Khosla",
                "M Bernstein"
            ],
            "date": "2015",
            "journal": "International journal of computer vision"
        },
        {
            "id": "b26",
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "authors": [
                "R Selvaraju",
                "M Cogswell",
                "A Das",
                "R Vedantam",
                "D Parikh",
                "D Batra"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b27",
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "authors": [
                "P Sharma",
                "N Ding",
                "S Goodman",
                "R Soricut"
            ],
            "date": "2018",
            "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
        },
        {
            "id": "b28",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "A corpus for reasoning about natural language grounded in photographs"
        },
        {
            "id": "b29",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Neural discrete representation learning. Advances in neural information processing systems"
        },
        {
            "id": "b30",
            "title": "Extracting and composing robust features with denoising autoencoders",
            "authors": [
                "P Vincent",
                "H Larochelle",
                "Y Bengio",
                "P.-A Manzagol"
            ],
            "date": "2008",
            "journal": "Proceedings of the 25th international conference on Machine learning"
        },
        {
            "id": "b31",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "Vlmo: Unified vision-language pre-training with mixture-of-modalityexperts"
        },
        {
            "id": "b32",
            "title": "Masked feature prediction for self-supervised visual pre-training",
            "authors": [
                "C Wei",
                "H Fan",
                "S Xie",
                "C.-Y Wu",
                "A Yuille",
                "C Feichtenhofer"
            ],
            "date": "2022",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b33",
            "title": "",
            "authors": [],
            "date": "2019",
            "journal": "Visual entailment: A novel task for fine-grained image understanding"
        },
        {
            "id": "b34",
            "title": "Simmim: A simple framework for masked image modeling",
            "authors": [
                "Z Xie",
                "Z Zhang",
                "Y Cao",
                "Y Lin",
                "J Bao",
                "Z Yao",
                "Q Dai",
                "H Hu"
            ],
            "date": "2022",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b35",
            "title": "Vision-Language Pre-Training with Triple Contrastive Learning",
            "authors": [
                "J Yang",
                "J Duan",
                "S Tran",
                "Y Xu",
                "S Chanda",
                "L Chen",
                "B Zeng",
                "T Chilimbi",
                "J Huang"
            ],
            "date": "2022. 15671-15680. 1, 2, 3, 5, 6, 7",
            "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b36",
            "title": "Mattnet: Modular attention network for referring expression comprehension",
            "authors": [
                "L Yu",
                "Z Lin",
                "X Shen",
                "J Yang",
                "X Lu",
                "M Bansal",
                "T Berg"
            ],
            "date": "2018",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b37",
            "title": "Modeling context in referring expressions",
            "authors": [
                "L Yu",
                "P Poirson",
                "S Yang",
                "A Berg",
                "T Berg"
            ],
            "date": "2016",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b38",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"
        }
    ]
}