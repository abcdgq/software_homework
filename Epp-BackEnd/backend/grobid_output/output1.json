{
    "metadata": {
        "title": "FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion",
        "authors": [
            {
                "name": "Fabian Duffhauss",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Ngo Vien",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Hanna Ziesche",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Gerhard Neumann",
                "affiliations": [
                    {
                        "name": "Karlsruhe Institute of Technology",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "{fabian Duffhauss"
            },
            {
                "name": "Anhvien Ngo"
            },
            {
                "name": "Hanna Ziesche}"
            },
            {
                "name": "@ Bosch"
            },
            {
                "name": "Com"
            }
        ],
        "abstract": "Sensor fusion can significantly improve the performance of many computer vision tasks. However, traditional fusion approaches are either not data-driven and cannot exploit prior knowledge nor find regularities in a given dataset or they are restricted to a single application. We overcome this shortcoming by presenting a novel deep hierarchical variational autoencoder called FusionVAE that can serve as a basis for many fusion tasks. Our approach is able to generate diverse image samples that are conditioned on multiple noisy, occluded, or only partially visible input images. We derive and optimize a variational lower bound for the conditional log-likelihood of FusionVAE. In order to assess the fusion capabilities of our model thoroughly, we created three novel datasets for image fusion based on popular computer vision datasets. In our experiments, we show that FusionVAE learns a representation of aggregated information that is relevant to fusion tasks. The results demonstrate that our approach outperforms traditional methods significantly. Furthermore, we present the advantages and disadvantages of different design choices."
    },
    "sections": [
        {
            "n": "1",
            "title": "Introduction",
            "content": [
                "Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving [7,30,67], for 6D pose estimation [16,17,61], and for robotic grasping [62,72]. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.",
                "Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. 1, FusionVAE merges a varying number of input images for reconstructing the original target image using"
            ],
            "paragraphs": [
                {
                    "text": "Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving [7,30,67], for 6D pose estimation [16,17,61], and for robotic grasping [62,72]. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.",
                    "citations": [
                        {
                            "target": "b6",
                            "text": "[7,"
                        },
                        {
                            "target": "b29",
                            "text": "30,"
                        },
                        {
                            "target": "b65",
                            "text": "67]"
                        },
                        {
                            "target": "b15",
                            "text": "[16,"
                        },
                        {
                            "target": "b16",
                            "text": "17,"
                        },
                        {
                            "target": "b59",
                            "text": "61]"
                        },
                        {
                            "target": "b60",
                            "text": "[62,"
                        },
                        {
                            "target": "",
                            "text": "72]"
                        }
                    ]
                },
                {
                    "text": "Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. 1, FusionVAE merges a varying number of input images for reconstructing the original target image using",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "FusionVAE Inputs Target Predictions",
            "content": [
                "Fig. 1: Overview of our FusionVAE approach. The network receives up to three partly occluded input images, fuses them together with prior knowledge, and predicts different hypothesis of how the target images could look like.",
                "prior knowledge about the dataset. To the best of our knowledge, FusionVAE is the first approach that combines these two tasks. Therefore, we developed three challenging benchmarks based on well-known computer vision datasets to evaluate the performance of our approach. In addition, we perform comparisons to baselines by extending traditional approaches to perform the same tasks. FusionVAE outperforms all these traditional methods on all proposed benchmark tasks significantly. We show that FusionVAE can generate high-quality images given few input images with partial observability. We provide ablation studies to illustrate the impact of commonly used information aggregation operations and to prove the benefits of the employed posterior distribution. We can summarize the four main contributions of our paper as follows: i) We create three challenging image fusion tasks for generative models. ii) We develop a deep hierarchical VAE called FusionVAE that is able to perform image-based data fusion while employing prior knowledge of the used dataset. iii) We show that FusionVAE produces high-quality fused output images and outperforms traditional methods by a large margin. iv) We perform ablation studies showing the benefits of our design choices regarding both the posterior distribution and commonly used aggregation methods. and expressiveness and thus, when applied to image generation leads to oversmoothed results lacking fine-grained details. Over the last years much work has been invested into the effort of improving the generative performance of VAEs. One stream of work is based on introducing a hierarchy into the latent space of the VAE and scaling this hierarchy to greater and greater depth. First introduced in [54] many hierarchical VAEs are based on coupling the inference and generative processes by introducing a deterministic bottom-up path combined with a stochastic top-down process in the inference network and sharing the latter with the generative model. This setting has been extended by an additional deterministic top-down path and bidirectional inference in [44]. Recently, very deep hierarchical VAEs were realized in [8] by introducing residual bottlenecks with dedicated scaling, update skipping, and nearest neighbour up-sampling. Closest to our work is the recently proposed NVAE architecture [57], which relies on depth-wise convolution, residual posterior parametrization, and spectral regularization to enhance stability.",
                "Other approaches propose increasing the expressiveness of VAEs by combining them with auto-regressive models like RNNs or PixelCNNs [6,11,13,50], conditioning contexts (CVAE) [53,60], normalizing flows [28], generative adversarial networks (GANs) [31,46], or variational generative adversarial networks (CVAE-GAN) [1]."
            ],
            "paragraphs": [
                {
                    "text": "Fig. 1: Overview of our FusionVAE approach. The network receives up to three partly occluded input images, fuses them together with prior knowledge, and predicts different hypothesis of how the target images could look like.",
                    "citations": []
                },
                {
                    "text": "prior knowledge about the dataset. To the best of our knowledge, FusionVAE is the first approach that combines these two tasks. Therefore, we developed three challenging benchmarks based on well-known computer vision datasets to evaluate the performance of our approach. In addition, we perform comparisons to baselines by extending traditional approaches to perform the same tasks. FusionVAE outperforms all these traditional methods on all proposed benchmark tasks significantly. We show that FusionVAE can generate high-quality images given few input images with partial observability. We provide ablation studies to illustrate the impact of commonly used information aggregation operations and to prove the benefits of the employed posterior distribution. We can summarize the four main contributions of our paper as follows: i) We create three challenging image fusion tasks for generative models. ii) We develop a deep hierarchical VAE called FusionVAE that is able to perform image-based data fusion while employing prior knowledge of the used dataset. iii) We show that FusionVAE produces high-quality fused output images and outperforms traditional methods by a large margin. iv) We perform ablation studies showing the benefits of our design choices regarding both the posterior distribution and commonly used aggregation methods. and expressiveness and thus, when applied to image generation leads to oversmoothed results lacking fine-grained details. Over the last years much work has been invested into the effort of improving the generative performance of VAEs. One stream of work is based on introducing a hierarchy into the latent space of the VAE and scaling this hierarchy to greater and greater depth. First introduced in [54] many hierarchical VAEs are based on coupling the inference and generative processes by introducing a deterministic bottom-up path combined with a stochastic top-down process in the inference network and sharing the latter with the generative model. This setting has been extended by an additional deterministic top-down path and bidirectional inference in [44]. Recently, very deep hierarchical VAEs were realized in [8] by introducing residual bottlenecks with dedicated scaling, update skipping, and nearest neighbour up-sampling. Closest to our work is the recently proposed NVAE architecture [57], which relies on depth-wise convolution, residual posterior parametrization, and spectral regularization to enhance stability.",
                    "citations": [
                        {
                            "target": "b52",
                            "text": "[54]"
                        },
                        {
                            "target": "b42",
                            "text": "[44]"
                        },
                        {
                            "target": "b7",
                            "text": "[8]"
                        },
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                },
                {
                    "text": "Other approaches propose increasing the expressiveness of VAEs by combining them with auto-regressive models like RNNs or PixelCNNs [6,11,13,50], conditioning contexts (CVAE) [53,60], normalizing flows [28], generative adversarial networks (GANs) [31,46], or variational generative adversarial networks (CVAE-GAN) [1].",
                    "citations": [
                        {
                            "target": "b5",
                            "text": "[6,"
                        },
                        {
                            "target": "b10",
                            "text": "11,"
                        },
                        {
                            "target": "b12",
                            "text": "13,"
                        },
                        {
                            "target": "b48",
                            "text": "50]"
                        },
                        {
                            "target": "b51",
                            "text": "[53,"
                        },
                        {
                            "target": "b58",
                            "text": "60]"
                        },
                        {
                            "target": "b27",
                            "text": "[28]"
                        },
                        {
                            "target": "b30",
                            "text": "[31,"
                        },
                        {
                            "target": "b44",
                            "text": "46]"
                        },
                        {
                            "target": "b0",
                            "text": "[1]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "2.2",
            "title": "Fusion of Multiple Images",
            "content": [
                "Image fusion has long been dominated by classical computer vision. Only lately deep learning methods entered the domain with the CNN-based approach proposed by Liu et al. [38]. In a subsequent publication the authors extended their work to a multi-scale setting [37]. Shortly afterwards, Prabhakar et al. developed a fusion method based on a siamese network architecture, called DeepFuse [48] which was improved in subsequent work [33] by employing the DenseNet architecture [20]. Concurrently, Li et al. [35] proposed a fusion architecture based on VGG [52] and in order to scale to even greater depth another one [34] based on ResNet-50 [15]. The aforementioned methods use CNNs as feature extractors and as decoders, while the fusion operations themselves are restricted to classical methods like averaging or addition of feature maps or weighted source images. A fully CNN-based feature-map fusion mechanism was proposed in [24].",
                "While all previous publications target only a specific fusion task (e.g. multifocus fusion, multi-resolution fusion, etc.) or were limited to specific domains (e.g. medical images), two very recent works propose novel multi-purpose fusion networks, which are applicable to many fusion tasks and image types [64,71]. Very recently also GAN-based methods entered the domain of image fusion, starting with the work by Ma et al. on infrared-visible fusion [41,43] and with [42,63] on multi-resolution image fusion. Most recent are two publications on GAN-based multi-focus image fusion [14,21]. While GAN-based approaches can generate high-fidelity images, it is known that they suffer from the mode collapse problem. VAE-based methods in contrast are known to generate more faithful data distribution [57]. Different from previous work, this paper proposes a VAE-based multi-purpose fusion framework."
            ],
            "paragraphs": [
                {
                    "text": "Image fusion has long been dominated by classical computer vision. Only lately deep learning methods entered the domain with the CNN-based approach proposed by Liu et al. [38]. In a subsequent publication the authors extended their work to a multi-scale setting [37]. Shortly afterwards, Prabhakar et al. developed a fusion method based on a siamese network architecture, called DeepFuse [48] which was improved in subsequent work [33] by employing the DenseNet architecture [20]. Concurrently, Li et al. [35] proposed a fusion architecture based on VGG [52] and in order to scale to even greater depth another one [34] based on ResNet-50 [15]. The aforementioned methods use CNNs as feature extractors and as decoders, while the fusion operations themselves are restricted to classical methods like averaging or addition of feature maps or weighted source images. A fully CNN-based feature-map fusion mechanism was proposed in [24].",
                    "citations": [
                        {
                            "target": "b36",
                            "text": "[38]"
                        },
                        {
                            "target": "b35",
                            "text": "[37]"
                        },
                        {
                            "target": "b46",
                            "text": "[48]"
                        },
                        {
                            "target": "b31",
                            "text": "[33]"
                        },
                        {
                            "target": "b19",
                            "text": "[20]"
                        },
                        {
                            "target": "b33",
                            "text": "[35]"
                        },
                        {
                            "target": "b50",
                            "text": "[52]"
                        },
                        {
                            "target": "b32",
                            "text": "[34]"
                        },
                        {
                            "target": "b14",
                            "text": "[15]"
                        },
                        {
                            "target": "b23",
                            "text": "[24]"
                        }
                    ]
                },
                {
                    "text": "While all previous publications target only a specific fusion task (e.g. multifocus fusion, multi-resolution fusion, etc.) or were limited to specific domains (e.g. medical images), two very recent works propose novel multi-purpose fusion networks, which are applicable to many fusion tasks and image types [64,71]. Very recently also GAN-based methods entered the domain of image fusion, starting with the work by Ma et al. on infrared-visible fusion [41,43] and with [42,63] on multi-resolution image fusion. Most recent are two publications on GAN-based multi-focus image fusion [14,21]. While GAN-based approaches can generate high-fidelity images, it is known that they suffer from the mode collapse problem. VAE-based methods in contrast are known to generate more faithful data distribution [57]. Different from previous work, this paper proposes a VAE-based multi-purpose fusion framework.",
                    "citations": [
                        {
                            "target": "b62",
                            "text": "[64,"
                        },
                        {
                            "target": "",
                            "text": "71]"
                        },
                        {
                            "target": "b39",
                            "text": "[41,"
                        },
                        {
                            "target": "b41",
                            "text": "43]"
                        },
                        {
                            "target": "b40",
                            "text": "[42,"
                        },
                        {
                            "target": "b61",
                            "text": "63]"
                        },
                        {
                            "target": "b13",
                            "text": "[14,"
                        },
                        {
                            "target": "b20",
                            "text": "21]"
                        },
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "2.3",
            "title": "Image Completion",
            "content": [
                "Similar to image fusion, also image completion has only recently become a playing field for deep learning methods. First approaches based on simple multilayer perceptrons (MLPs) [29] or CNNs [12] were targeted only to filling small holes in an image. However, with the introduction of GANs [10], the area quickly became dominated by GAN-based approaches, starting with the context encoders presented by Pathak et al. [47]. Many subsequent papers proposed extensions to this model in order to obtain fine-grained completions while preserving global coherence by introducing additional discriminators [22], searching for closest samples to the corrupted image in a latent embedding space conditioning on semantic labels [56], or designing additional specialized loss functions [36]. High resolution results were obtained recently by multi-scale approaches [66], iterative upsampling [70], and the application of contextual attention [55,65,68]. Another stream of current work focuses on multi-hypothesis image completion, leveraging probabilistic problem formulations [45,73]."
            ],
            "paragraphs": [
                {
                    "text": "Similar to image fusion, also image completion has only recently become a playing field for deep learning methods. First approaches based on simple multilayer perceptrons (MLPs) [29] or CNNs [12] were targeted only to filling small holes in an image. However, with the introduction of GANs [10], the area quickly became dominated by GAN-based approaches, starting with the context encoders presented by Pathak et al. [47]. Many subsequent papers proposed extensions to this model in order to obtain fine-grained completions while preserving global coherence by introducing additional discriminators [22], searching for closest samples to the corrupted image in a latent embedding space conditioning on semantic labels [56], or designing additional specialized loss functions [36]. High resolution results were obtained recently by multi-scale approaches [66], iterative upsampling [70], and the application of contextual attention [55,65,68]. Another stream of current work focuses on multi-hypothesis image completion, leveraging probabilistic problem formulations [45,73].",
                    "citations": [
                        {
                            "target": "b28",
                            "text": "[29]"
                        },
                        {
                            "target": "b11",
                            "text": "[12]"
                        },
                        {
                            "target": "b9",
                            "text": "[10]"
                        },
                        {
                            "target": "b45",
                            "text": "[47]"
                        },
                        {
                            "target": "b21",
                            "text": "[22]"
                        },
                        {
                            "target": "b54",
                            "text": "[56]"
                        },
                        {
                            "target": "b34",
                            "text": "[36]"
                        },
                        {
                            "target": "b64",
                            "text": "[66]"
                        },
                        {
                            "target": "b53",
                            "text": "[55,"
                        },
                        {
                            "target": "b63",
                            "text": "65,"
                        },
                        {
                            "target": "b66",
                            "text": "68]"
                        },
                        {
                            "target": "b43",
                            "text": "[45,"
                        },
                        {
                            "target": "",
                            "text": "73]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3",
            "title": "Background",
            "content": [
                "In this section, we outline the fundamentals of standard VAEs, conditional VAEs, and hierarchical VAEs upon which we build our approach. Another section is dedicated to aggregation methods for data fusion."
            ],
            "paragraphs": [
                {
                    "text": "In this section, we outline the fundamentals of standard VAEs, conditional VAEs, and hierarchical VAEs upon which we build our approach. Another section is dedicated to aggregation methods for data fusion.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.1",
            "title": "Standard VAE",
            "content": [
                "A variational autoencoder (VAE) [27] is a neural network consisting of a probabilistic encoder q(z|y) and a generative model p(y|z). The generator models a distribution over the input data y, conditioned on a latent variable z with prior distribution p θ (z). The encoder approximates the posterior distribution p(z|y) of the latent variables z given input data y and is trained along with the generative model by maximizing the evidence lower bound (ELBO) ELBO(y) = E q(z|y) [log p(y|z)] -KL(q(z|y)||p(z)), (1) where KL is the Kullback-Leibler divergence and log p(y) ≥ ELBO(y)."
            ],
            "paragraphs": [
                {
                    "text": "A variational autoencoder (VAE) [27] is a neural network consisting of a probabilistic encoder q(z|y) and a generative model p(y|z). The generator models a distribution over the input data y, conditioned on a latent variable z with prior distribution p θ (z). The encoder approximates the posterior distribution p(z|y) of the latent variables z given input data y and is trained along with the generative model by maximizing the evidence lower bound (ELBO) ELBO(y) = E q(z|y) [log p(y|z)] -KL(q(z|y)||p(z)), (1) where KL is the Kullback-Leibler divergence and log p(y) ≥ ELBO(y).",
                    "citations": [
                        {
                            "target": "b26",
                            "text": "[27]"
                        },
                        {
                            "target": "b0",
                            "text": "(1)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.2",
            "title": "Conditional VAE",
            "content": [
                "In VAEs, the generative model p(y|z) is unconditional. In contrast, conditional VAEs (CVAE) [53] consider a generative model for a conditional distribution p(y|x, z) where y is the target data, x is the conditional input variable, and z is a latent variable. The prior of the latent variable is p(z|x), while its approximate posterior distribution is given by q(z|x, y). The variational lower bound of the conditional log-likelihood can be written as follows log p(y|x) ≥ E q(z|x,y) [log p(y|x, z)] -KL(q(z|x, y)||p(z|x)).",
                "(2)"
            ],
            "paragraphs": [
                {
                    "text": "In VAEs, the generative model p(y|z) is unconditional. In contrast, conditional VAEs (CVAE) [53] consider a generative model for a conditional distribution p(y|x, z) where y is the target data, x is the conditional input variable, and z is a latent variable. The prior of the latent variable is p(z|x), while its approximate posterior distribution is given by q(z|x, y). The variational lower bound of the conditional log-likelihood can be written as follows log p(y|x) ≥ E q(z|x,y) [log p(y|x, z)] -KL(q(z|x, y)||p(z|x)).",
                    "citations": [
                        {
                            "target": "b51",
                            "text": "[53]"
                        }
                    ]
                },
                {
                    "text": "(2)",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.3",
            "title": "Hierarchical VAE",
            "content": [
                "In hierarchical VAEs [25,54,57], the latent variables z are divided into L disjoint groups z 1 , ..., z L in order to increase the expressiveness of both prior and approximate posterior which become",
                "p(z l |z <l ) and q(z|y)",
                "where z <l denotes the latent variables in all previous hierarchies. All the conditionals in the prior p(z l |z <l ) and in the approximate posterior q(z l |z <l , y) are modeled by factorial Gaussian distributions. Under this modelling choice, the ELBO from Eq. ( 1) turns into"
            ],
            "paragraphs": [
                {
                    "text": "In hierarchical VAEs [25,54,57], the latent variables z are divided into L disjoint groups z 1 , ..., z L in order to increase the expressiveness of both prior and approximate posterior which become",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "[25,"
                        },
                        {
                            "target": "b52",
                            "text": "54,"
                        },
                        {
                            "target": "b55",
                            "text": "57]"
                        }
                    ]
                },
                {
                    "text": "p(z l |z <l ) and q(z|y)",
                    "citations": []
                },
                {
                    "text": "where z <l denotes the latent variables in all previous hierarchies. All the conditionals in the prior p(z l |z <l ) and in the approximate posterior q(z l |z <l , y) are modeled by factorial Gaussian distributions. Under this modelling choice, the ELBO from Eq. ( 1) turns into",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.4",
            "title": "Aggregation Methods",
            "content": [
                "For fusing data within our approach, we consider different aggregation methods, such as mean aggregation, max aggregation, Bayesian aggregation [59] and pixelwise addition. All described aggregation methods fuse a set of feature tensors f 1 , ..., f K , obtained by encoding K input images {x i } K i=1 in a permutation invariant way [69]. In mean aggregation, multiple feature vectors are fused by taking the pixel-wise average f = 1 K K i=1 f i . For max aggregation, we take the pixel-wise maximum instead f = max i (f i ). Bayesian aggregation (BA) [59] considers an uncertainty estimate for the fused feature vectors. In order to obtain such an uncertainty estimate, the encoder has to predict means µ i and variances σ i of a factorized Gaussian distribution over the latent feature vectors",
                "instead of f i directly. Here enc µ and enc σ represent the encoding process which generates means and variances respectively. The predicted distributions over latent feature vectors for multiple input images can be fused iteratively using the Bayes rule [2] (detailed derivation is given in Appendix B)",
                "where 7) ⊘ and ⊙ denote element-wise division and multiplication respectively."
            ],
            "paragraphs": [
                {
                    "text": "For fusing data within our approach, we consider different aggregation methods, such as mean aggregation, max aggregation, Bayesian aggregation [59] and pixelwise addition. All described aggregation methods fuse a set of feature tensors f 1 , ..., f K , obtained by encoding K input images {x i } K i=1 in a permutation invariant way [69]. In mean aggregation, multiple feature vectors are fused by taking the pixel-wise average f = 1 K K i=1 f i . For max aggregation, we take the pixel-wise maximum instead f = max i (f i ). Bayesian aggregation (BA) [59] considers an uncertainty estimate for the fused feature vectors. In order to obtain such an uncertainty estimate, the encoder has to predict means µ i and variances σ i of a factorized Gaussian distribution over the latent feature vectors",
                    "citations": [
                        {
                            "target": "b57",
                            "text": "[59]"
                        },
                        {
                            "target": "",
                            "text": "[69]"
                        },
                        {
                            "target": "b57",
                            "text": "[59]"
                        }
                    ]
                },
                {
                    "text": "instead of f i directly. Here enc µ and enc σ represent the encoding process which generates means and variances respectively. The predicted distributions over latent feature vectors for multiple input images can be fused iteratively using the Bayes rule [2] (detailed derivation is given in Appendix B)",
                    "citations": [
                        {
                            "target": "b1",
                            "text": "[2]"
                        }
                    ]
                },
                {
                    "text": "where 7) ⊘ and ⊙ denote element-wise division and multiplication respectively.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4",
            "title": "Conditional Generative Models for Image Fusion",
            "content": [
                "We propose a deep hierarchical conditional variational autoencoder, called Fusion-VAE (Fusion Variational Auto-Encoder), that is able to fuse information from multiple sources and to infer the missing information in the images from a prior learned from the dataset. To the best of our knowledge, it is the first model that combines the generative ability of a hierarchical VAE to learn the underlying distribution of complex datasets with the ability to fuse multiple input images."
            ],
            "paragraphs": [
                {
                    "text": "We propose a deep hierarchical conditional variational autoencoder, called Fusion-VAE (Fusion Variational Auto-Encoder), that is able to fuse information from multiple sources and to infer the missing information in the images from a prior learned from the dataset. To the best of our knowledge, it is the first model that combines the generative ability of a hierarchical VAE to learn the underlying distribution of complex datasets with the ability to fuse multiple input images.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.1",
            "title": "Problem Formulation",
            "content": [
                "We consider image fusion problems that are concerned with generating the fused target image from multiple source images. Each source image contains partial information of the target image and the goal of the task is to recover the original target image given a finite set of source images. In particular, we denote the target image as y and the set of K source images as context x = {x 1 , x 2 , . . . , x K }, where each x i is one source image. Given training sample (x, y), we aim to maximize the conditional likelihood p(y|x)."
            ],
            "paragraphs": [
                {
                    "text": "We consider image fusion problems that are concerned with generating the fused target image from multiple source images. Each source image contains partial information of the target image and the goal of the task is to recover the original target image given a finite set of source images. In particular, we denote the target image as y and the set of K source images as context x = {x 1 , x 2 , . . . , x K }, where each x i is one source image. Given training sample (x, y), we aim to maximize the conditional likelihood p(y|x).",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.2",
            "title": "FusionVAE",
            "content": [
                "Our approach is designed to maximize the conditional likelihood p(y|x). However, optimizing this objective directly is intractable. Therefore, we derive a variational lower bound as follows (detailed derivation can be found in Appendix C)",
                "where we split the latent variables z into L disjoint groups {z 1 , z 2 , . . . , z L }. β and α l are annealing parameters that control the warming-up of the KL terms as in [57]. Inspired by [54], β is increased linearly from 0 to 1 during the first few training epochs to start training the reconstruction before introducing the KL term, which is increased gradually. α l is a KL balancing coefficient [58] that is used during the warm-up period to encourage the equal use of all latent groups and to avoid posterior collapse. FusionVAE consists of three main networks and a latent space as illustrated in Fig. 2: 1) a context encoder network which models the conditional prior p ϕ (z l |x, z <l ), 2) a target encoder which models the approximate posterior q ϕ (z|y), 3) a latent space comprising the L latent groups, and 4) a generator network p θ (y|x, z) that aims to reconstruct the target image."
            ],
            "paragraphs": [
                {
                    "text": "Our approach is designed to maximize the conditional likelihood p(y|x). However, optimizing this objective directly is intractable. Therefore, we derive a variational lower bound as follows (detailed derivation can be found in Appendix C)",
                    "citations": []
                },
                {
                    "text": "where we split the latent variables z into L disjoint groups {z 1 , z 2 , . . . , z L }. β and α l are annealing parameters that control the warming-up of the KL terms as in [57]. Inspired by [54], β is increased linearly from 0 to 1 during the first few training epochs to start training the reconstruction before introducing the KL term, which is increased gradually. α l is a KL balancing coefficient [58] that is used during the warm-up period to encourage the equal use of all latent groups and to avoid posterior collapse. FusionVAE consists of three main networks and a latent space as illustrated in Fig. 2: 1) a context encoder network which models the conditional prior p ϕ (z l |x, z <l ), 2) a target encoder which models the approximate posterior q ϕ (z|y), 3) a latent space comprising the L latent groups, and 4) a generator network p θ (y|x, z) that aims to reconstruct the target image.",
                    "citations": [
                        {
                            "target": "b55",
                            "text": "[57]"
                        },
                        {
                            "target": "b52",
                            "text": "[54]"
                        },
                        {
                            "target": "b56",
                            "text": "[58]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.3",
            "title": "Network Architecture",
            "content": [
                "Fig. 2 illustrates the network architecture of our FusionVAE for training. It is built in a hierarchical way inspired by [57]. In each latent hierarchy l ∈ 1, . . . , L we have a set of feature maps f lx , f ly and latent distributions p l , q l . The first gray box contains the context encoder network that obtains a stack of source images x and employs residual cells [15] as in [57] to extract features f lx . The second gray box shows the target encoder network that encodes the target Fig. 2: Overview of the proposed network architecture. h is a trainable parameter vector, ⊕ denotes concatenation, ⊕ max aggregation, and ⊕ pixel-wise addition.",
                "r r is a residual network like in [57]. The dotted lines between the residual networks indicate shared parameters.",
                "image y into the feature map f ly using the same residual cells as the context encoder. The third gray box illustrates the latent space which contains the prior distributions p ϕ (z l |x, z <l ) (denoted p 1 , ..., p L in Fig. 2) and the approximate posterior distributions q ϕ (z l |y, z <l ) (denoted q 1 , ..., q L in Fig. 2). The fourth gray box contains the generator network which aims to create different output samples ŷ. It employs a trainable parameter vector h, concatenates the information from all hierarchies, and decodes them using residual cells.",
                "In each latent hierarchy, we aggregate the context features f lx using pixelwise max aggregation. In all but the first hierarchy, we pixel-wisely add the corresponding feature map from the generator network to the aggregated context features and to the target image features f ly . Using 2D convolutional layers, we learn the prior distributions p l and the approximate posterior distributions q l . We propose to use the approximate posterior distributions q l as target distributions in order to learn good prior distributions p l . Therefore, q ϕ (z l |y, z <l ) is created from the target image features f ly as well as information from the generator network.",
                "During training the generator network aims to create a prediction ŷ based on samples of the posterior distributions q l and a trainable parameter vector h.",
                "For evaluation, we can omit the target image input y and sample from the prior distributions p l . In case no input image is given, we set p 1 to a standard normal distribution. Based on the samples and the trainable parameter vector h, our FusionVAE can generate new output images."
            ],
            "paragraphs": [
                {
                    "text": "Fig. 2 illustrates the network architecture of our FusionVAE for training. It is built in a hierarchical way inspired by [57]. In each latent hierarchy l ∈ 1, . . . , L we have a set of feature maps f lx , f ly and latent distributions p l , q l . The first gray box contains the context encoder network that obtains a stack of source images x and employs residual cells [15] as in [57] to extract features f lx . The second gray box shows the target encoder network that encodes the target Fig. 2: Overview of the proposed network architecture. h is a trainable parameter vector, ⊕ denotes concatenation, ⊕ max aggregation, and ⊕ pixel-wise addition.",
                    "citations": [
                        {
                            "target": "b55",
                            "text": "[57]"
                        },
                        {
                            "target": "b14",
                            "text": "[15]"
                        },
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                },
                {
                    "text": "r r is a residual network like in [57]. The dotted lines between the residual networks indicate shared parameters.",
                    "citations": [
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                },
                {
                    "text": "image y into the feature map f ly using the same residual cells as the context encoder. The third gray box illustrates the latent space which contains the prior distributions p ϕ (z l |x, z <l ) (denoted p 1 , ..., p L in Fig. 2) and the approximate posterior distributions q ϕ (z l |y, z <l ) (denoted q 1 , ..., q L in Fig. 2). The fourth gray box contains the generator network which aims to create different output samples ŷ. It employs a trainable parameter vector h, concatenates the information from all hierarchies, and decodes them using residual cells.",
                    "citations": []
                },
                {
                    "text": "In each latent hierarchy, we aggregate the context features f lx using pixelwise max aggregation. In all but the first hierarchy, we pixel-wisely add the corresponding feature map from the generator network to the aggregated context features and to the target image features f ly . Using 2D convolutional layers, we learn the prior distributions p l and the approximate posterior distributions q l . We propose to use the approximate posterior distributions q l as target distributions in order to learn good prior distributions p l . Therefore, q ϕ (z l |y, z <l ) is created from the target image features f ly as well as information from the generator network.",
                    "citations": []
                },
                {
                    "text": "During training the generator network aims to create a prediction ŷ based on samples of the posterior distributions q l and a trainable parameter vector h.",
                    "citations": []
                },
                {
                    "text": "For evaluation, we can omit the target image input y and sample from the prior distributions p l . In case no input image is given, we set p 1 to a standard normal distribution. Based on the samples and the trainable parameter vector h, our FusionVAE can generate new output images.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5",
            "title": "Experimental Setup",
            "content": [
                "To evaluate our approach, we conduct a series of experiments on three different datasets using data augmentation. Furthermore, we adapt traditional architec-tures for solving the same tasks in order to compare our results. Finally, we perform an ablation study to show the effects of specific design choices."
            ],
            "paragraphs": [
                {
                    "text": "To evaluate our approach, we conduct a series of experiments on three different datasets using data augmentation. Furthermore, we adapt traditional architec-tures for solving the same tasks in order to compare our results. Finally, we perform an ablation study to show the effects of specific design choices.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5.1",
            "title": "Datasets",
            "content": [
                "For training and evaluating our approach, we create three novel fusion datasets based on MNIST [32], CelebA [39], and T-LESS [18] as described in the following.",
                "FusionMNIST. Based on the dataset MNIST [32], we create an image fusion dataset called FusionMNIST. For each target image, it contains different noisy representations where only random parts of the target image are visible. The first three columns of Fig. 3 show different examples of FusionMNIST corresponding to the target images in the fourth column. To generate FusionMNIST, we applied zero padding to all MNIST images to obtain a resolution of 32 × 32. For creating a noisy representation, we generate a mask composed of the union of a varying number of ellipses with random size, shape and position. All parts of the given images outside the mask are blackened. Finally, we add Gaussian noise with a fixed variance and clip the pixel values afterwards to stay within [0, 1].",
                "FusionCelebA. We generate a similar fusion dataset based on the aligned and cropped version of CelebA [39] which we call FusionCelebA. Fig. 4 depicts different example images in the first three columns which belong to the target image in the fourth column. To generate FusionCelebA, we center-crop the CelebA images to 148 × 148 before scaling them down to 64 × 64 as proposed by [31]. As in Fusion-MNIST, we create different representations by using masks based on random ellipses."
            ],
            "paragraphs": [
                {
                    "text": "For training and evaluating our approach, we create three novel fusion datasets based on MNIST [32], CelebA [39], and T-LESS [18] as described in the following.",
                    "citations": [
                        {
                            "target": "b37",
                            "text": "[39]"
                        },
                        {
                            "target": "b17",
                            "text": "[18]"
                        }
                    ]
                },
                {
                    "text": "FusionMNIST. Based on the dataset MNIST [32], we create an image fusion dataset called FusionMNIST. For each target image, it contains different noisy representations where only random parts of the target image are visible. The first three columns of Fig. 3 show different examples of FusionMNIST corresponding to the target images in the fourth column. To generate FusionMNIST, we applied zero padding to all MNIST images to obtain a resolution of 32 × 32. For creating a noisy representation, we generate a mask composed of the union of a varying number of ellipses with random size, shape and position. All parts of the given images outside the mask are blackened. Finally, we add Gaussian noise with a fixed variance and clip the pixel values afterwards to stay within [0, 1].",
                    "citations": []
                },
                {
                    "text": "FusionCelebA. We generate a similar fusion dataset based on the aligned and cropped version of CelebA [39] which we call FusionCelebA. Fig. 4 depicts different example images in the first three columns which belong to the target image in the fourth column. To generate FusionCelebA, we center-crop the CelebA images to 148 × 148 before scaling them down to 64 × 64 as proposed by [31]. As in Fusion-MNIST, we create different representations by using masks based on random ellipses.",
                    "citations": [
                        {
                            "target": "b37",
                            "text": "[39]"
                        },
                        {
                            "target": "b30",
                            "text": "[31]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "FusionT-LESS.",
            "content": [
                "A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS [18] which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector [5] and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation."
            ],
            "paragraphs": [
                {
                    "text": "A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS [18] which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector [5] and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation.",
                    "citations": [
                        {
                            "target": "b17",
                            "text": "[18]"
                        },
                        {
                            "target": "b4",
                            "text": "[5]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "5.2",
            "title": "Data Augmentation",
            "content": [
                "During training we apply different augmentation methods on the datasets to avoid overfitting. For FusionMNIST, we apply the elliptical mask generation and the addition of Gaussian noise live during training so that we obtain an infinite number of different fusion tasks. For FusionT-LESS, almost the entire creation of occluded images is performed during training. We apply horizontal flips, rotations, scaling and movement of target and occluding images with random parameters before composing the different occluded representations. Solely the object cutting with the Canny edge detector is performed offline as a pre-processing step to keep the training time low. For FusionCelebA, we apply a horizontal flip of all images randomly in 50% of all occasions and also the elliptical mask generation is done live during training."
            ],
            "paragraphs": [
                {
                    "text": "During training we apply different augmentation methods on the datasets to avoid overfitting. For FusionMNIST, we apply the elliptical mask generation and the addition of Gaussian noise live during training so that we obtain an infinite number of different fusion tasks. For FusionT-LESS, almost the entire creation of occluded images is performed during training. We apply horizontal flips, rotations, scaling and movement of target and occluding images with random parameters before composing the different occluded representations. Solely the object cutting with the Canny edge detector is performed offline as a pre-processing step to keep the training time low. For FusionCelebA, we apply a horizontal flip of all images randomly in 50% of all occasions and also the elliptical mask generation is done live during training.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5.3",
            "title": "Architectures for Comparison",
            "content": [
                "To the best of our knowledge, FusionVAE is the first fusion network for multiple images with a generative ability to fill areas without input information based on prior knowledge about the dataset under consideration. For lack of a suitable other model from the literature which would allow a fair comparison on our multi-image fusion tasks, we compare our approach with standard architectures that we adapted to support our tasks.",
                "The first architecture for comparison is a CVAE with residual layers as employed in [57]. We use a shared encoder for processing the input images and applied max aggregation before the latent space as we did in our FusionVAE. The second architecture for comparison is a fully convolutional network (FCN) with shared encoder and max aggregation before the decoder.",
                "For both baseline architectures, we created a version with skip connections (+S) and a version without. When using skip connections, we applied max aggregation at each shortcut for merging the features from the encoder with the decoder's features. To allow for a fair comparison, we designed all architectures so that they have a similar number of trainable parameters."
            ],
            "paragraphs": [
                {
                    "text": "To the best of our knowledge, FusionVAE is the first fusion network for multiple images with a generative ability to fill areas without input information based on prior knowledge about the dataset under consideration. For lack of a suitable other model from the literature which would allow a fair comparison on our multi-image fusion tasks, we compare our approach with standard architectures that we adapted to support our tasks.",
                    "citations": []
                },
                {
                    "text": "The first architecture for comparison is a CVAE with residual layers as employed in [57]. We use a shared encoder for processing the input images and applied max aggregation before the latent space as we did in our FusionVAE. The second architecture for comparison is a fully convolutional network (FCN) with shared encoder and max aggregation before the decoder.",
                    "citations": [
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                },
                {
                    "text": "For both baseline architectures, we created a version with skip connections (+S) and a version without. When using skip connections, we applied max aggregation at each shortcut for merging the features from the encoder with the decoder's features. To allow for a fair comparison, we designed all architectures so that they have a similar number of trainable parameters.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5.4",
            "title": "Training Procedure",
            "content": [
                "We trained all networks in a supervised manner using the augmented target images y as described in Sec. 5.2. In order to teach the networks both to fuse information from a different number of input images and to learn prior knowledge about the dataset, we vary the number of input images x during the entire training. Specifically, we select a uniformly distributed random number between zero and three for each batch."
            ],
            "paragraphs": [
                {
                    "text": "We trained all networks in a supervised manner using the augmented target images y as described in Sec. 5.2. In order to teach the networks both to fuse information from a different number of input images and to learn prior knowledge about the dataset, we vary the number of input images x during the entire training. Specifically, we select a uniformly distributed random number between zero and three for each batch.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5.5",
            "title": "Evaluation Metrics",
            "content": [
                "For evaluation, we estimate the negative log-likelihood (NLL) in bits per dimension (BPD) using weighted importance sampling [4]. We use 100 samples for all experiments with FusionCelebA as well as FusionT-LESS and 1000 samples for FusionMNIST. Since we cannot estimate the NLL of the FCN, we used the minimum over all samples of the mean squared error (MSE min ) as second metric."
            ],
            "paragraphs": [
                {
                    "text": "For evaluation, we estimate the negative log-likelihood (NLL) in bits per dimension (BPD) using weighted importance sampling [4]. We use 100 samples for all experiments with FusionCelebA as well as FusionT-LESS and 1000 samples for FusionMNIST. Since we cannot estimate the NLL of the FCN, we used the minimum over all samples of the mean squared error (MSE min ) as second metric.",
                    "citations": [
                        {
                            "target": "b3",
                            "text": "[4]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "6",
            "title": "Results",
            "content": [
                "This section presents and discusses the quantitative and qualitative results of our research in comparison to the baseline methods mentioned in Sec. 5.3."
            ],
            "paragraphs": [
                {
                    "text": "This section presents and discusses the quantitative and qualitative results of our research in comparison to the baseline methods mentioned in Sec. 5.3.",
                    "citations": []
                }
            ]
        },
        {
            "n": "6.1",
            "title": "Quantitative Results",
            "content": [
                "Tabs. 1 to 3 show the NLL and the MSE min of all architectures on FusionMNIST, FusionCelebA, and FusionT-LESS respectively. The results are divided into the results based on zero to three input images and the average (avg) of it. We see that our FusionVAE outperforms all baseline methods on average. Regarding the NLL, our model surpasses the others additionally for 0 and 1 input images. For 2 and 3 images, CVAE+S reaches sometimes slightly better NLL values. However, our approach reaches the best MSE min values for each number of input images."
            ],
            "paragraphs": [
                {
                    "text": "Tabs. 1 to 3 show the NLL and the MSE min of all architectures on FusionMNIST, FusionCelebA, and FusionT-LESS respectively. The results are divided into the results based on zero to three input images and the average (avg) of it. We see that our FusionVAE outperforms all baseline methods on average. Regarding the NLL, our model surpasses the others additionally for 0 and 1 input images. For 2 and 3 images, CVAE+S reaches sometimes slightly better NLL values. However, our approach reaches the best MSE min values for each number of input images.",
                    "citations": []
                }
            ]
        },
        {
            "n": "6.2",
            "title": "Qualitative Results",
            "content": [
                "The outstanding performance of our architecture in comparison to the others is also obvious when looking at the qualitative results in Figs. 3 to 5. For every row, these figures show the input, target, and up to three output predictions for all architectures. For the FCN, we depict just a single output prediction per row as all of them look almost identical.",
                "In the first three rows when the network does not receive any input image, we see that our network provides very realistic images. This indicates that it is able to capture the underlying distribution of the used datasets very well and much better than the other architectures. Due to the difficulty of the FusionT-LESS dataset, none of the models is able to produce realistic images without any input. Still our model shows much better performance in generating object-like shapes. In case the models receive at least one input image (cf. rows 4 -12), all architectures are able to extract the available information from the given input images. In addition, all VAE approaches, ours included, are able to complete the given input data based on prior knowledge. It is clearly visible, however, that the predictions of our model are much more realistic than the ones of the standard CVAE approaches especially for the more difficult datasets like FusionCelebA and FusionT-LESS. Fig. 5: Predictions on FusionT-LESS for zero to three input images."
            ],
            "paragraphs": [
                {
                    "text": "The outstanding performance of our architecture in comparison to the others is also obvious when looking at the qualitative results in Figs. 3 to 5. For every row, these figures show the input, target, and up to three output predictions for all architectures. For the FCN, we depict just a single output prediction per row as all of them look almost identical.",
                    "citations": []
                },
                {
                    "text": "In the first three rows when the network does not receive any input image, we see that our network provides very realistic images. This indicates that it is able to capture the underlying distribution of the used datasets very well and much better than the other architectures. Due to the difficulty of the FusionT-LESS dataset, none of the models is able to produce realistic images without any input. Still our model shows much better performance in generating object-like shapes. In case the models receive at least one input image (cf. rows 4 -12), all architectures are able to extract the available information from the given input images. In addition, all VAE approaches, ours included, are able to complete the given input data based on prior knowledge. It is clearly visible, however, that the predictions of our model are much more realistic than the ones of the standard CVAE approaches especially for the more difficult datasets like FusionCelebA and FusionT-LESS. Fig. 5: Predictions on FusionT-LESS for zero to three input images.",
                    "citations": []
                }
            ]
        },
        {
            "n": "6.3",
            "title": "Ablation Studies",
            "content": [
                "We conducted ablation studies to show the effect of certain design choices, such as the selection of the approximate posterior and the aggregation method. All experiments are run on the FusionCelebA dataset. Tab. 4 compares the performance of our FusionVAE for two different approximate posterior distributions q. The approximate posterior we selected for our FusionVAE q(y), depends only on the given target image y. It performs slightly better on average compared to the same method using a posterior that is computed based on the input images x as well as the target image y. However, the latter approach is superior when fusing two or three input images.   Tab. 5 shows the performance of different aggregation methods which are applied to create the prior distributions p l of every latent group. In our Fusion-VAE, the prior is created by fusing the input image features f lx using max aggregation (MaxAgg) and adding them to the decoded features of the same latent group before applying a 2D convolution. We abbreviate that method with MaxAggAdd.",
                "In addition to MaxAgg, we examined mean aggregation (MeanAgg) and Bayesian aggregation (BayAgg) [59] for comparison. For each aggregation principle, we tried two different versions: 1) aggregation of the input image features f lx adding the corresponding information from the decoder in a pixel-wise manner (denoted by suffix Add), and 2) directly aggregating all features, i.e. both input image features f lx and decoder features (denoted by suffix All).",
                "For creating the prior p i when using BayAgg, we moved the 2D convolutions before the aggregation in order to create the parameters µ and σ of a latent Gaussian distribution. Unlike MaxAgg and MeanAgg, BayAgg directly outputs a new Gaussian distribution that does not need to be processed any further by a convolution.",
                "We can see that all variations of mean and max aggregation are significantly better than Bayesian aggregation. Also their training procedures are less often impaired due to numeric instabilities. Interestingly, the NLL is very similar independent of whether the aggregation is performed on all features or not. However, the MSE min is much better for the aggregation with addition. Since the expressiveness of the metrics is limited, we provide additional visualizations of this ablation in Appendix D."
            ],
            "paragraphs": [
                {
                    "text": "We conducted ablation studies to show the effect of certain design choices, such as the selection of the approximate posterior and the aggregation method. All experiments are run on the FusionCelebA dataset. Tab. 4 compares the performance of our FusionVAE for two different approximate posterior distributions q. The approximate posterior we selected for our FusionVAE q(y), depends only on the given target image y. It performs slightly better on average compared to the same method using a posterior that is computed based on the input images x as well as the target image y. However, the latter approach is superior when fusing two or three input images.   Tab. 5 shows the performance of different aggregation methods which are applied to create the prior distributions p l of every latent group. In our Fusion-VAE, the prior is created by fusing the input image features f lx using max aggregation (MaxAgg) and adding them to the decoded features of the same latent group before applying a 2D convolution. We abbreviate that method with MaxAggAdd.",
                    "citations": []
                },
                {
                    "text": "In addition to MaxAgg, we examined mean aggregation (MeanAgg) and Bayesian aggregation (BayAgg) [59] for comparison. For each aggregation principle, we tried two different versions: 1) aggregation of the input image features f lx adding the corresponding information from the decoder in a pixel-wise manner (denoted by suffix Add), and 2) directly aggregating all features, i.e. both input image features f lx and decoder features (denoted by suffix All).",
                    "citations": [
                        {
                            "target": "b57",
                            "text": "[59]"
                        }
                    ]
                },
                {
                    "text": "For creating the prior p i when using BayAgg, we moved the 2D convolutions before the aggregation in order to create the parameters µ and σ of a latent Gaussian distribution. Unlike MaxAgg and MeanAgg, BayAgg directly outputs a new Gaussian distribution that does not need to be processed any further by a convolution.",
                    "citations": []
                },
                {
                    "text": "We can see that all variations of mean and max aggregation are significantly better than Bayesian aggregation. Also their training procedures are less often impaired due to numeric instabilities. Interestingly, the NLL is very similar independent of whether the aggregation is performed on all features or not. However, the MSE min is much better for the aggregation with addition. Since the expressiveness of the metrics is limited, we provide additional visualizations of this ablation in Appendix D.",
                    "citations": []
                }
            ]
        },
        {
            "n": "7",
            "title": "Conclusion",
            "content": [
                "We have presented a novel deep hierarchical variational autoencoder for generative image fusion called FusionVAE. Our approach fuses multiple corrupted input images together with prior knowledge obtained during training. We created three challenging image fusion benchmarks based on common computer vision datasets. Moreover, we implemented four standard methods that we modified to support our tasks. We showed that our FusionVAE outperforms all other methods significantly while having a similar number of trainable parameters. The predicted images of our approach look very realistic and incorporate given input information almost perfectly. During ablation studies, we revealed the benefits of our design choices regarding the applied aggregation method and the used posterior distribution. In future work, our research could be extended by enabling the fusion of different modalities e.g. by using multiple encoders. Additionally, an explicit uncertainty estimation could be implemented that helps to weigh the impact of input information according to its uncertainty."
            ],
            "paragraphs": [
                {
                    "text": "We have presented a novel deep hierarchical variational autoencoder for generative image fusion called FusionVAE. Our approach fuses multiple corrupted input images together with prior knowledge obtained during training. We created three challenging image fusion benchmarks based on common computer vision datasets. Moreover, we implemented four standard methods that we modified to support our tasks. We showed that our FusionVAE outperforms all other methods significantly while having a similar number of trainable parameters. The predicted images of our approach look very realistic and incorporate given input information almost perfectly. During ablation studies, we revealed the benefits of our design choices regarding the applied aggregation method and the used posterior distribution. In future work, our research could be extended by enabling the fusion of different modalities e.g. by using multiple encoders. Additionally, an explicit uncertainty estimation could be implemented that helps to weigh the impact of input information according to its uncertainty.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "Supplementary Material",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "",
            "title": "A Implementation details",
            "content": [
                "For FusionMNIST and FusionT-LESS, we model the decoder's output by pixelwise independent Bernoulli distributions. For FusionCelebA, we use pixel-wise independent discretized logistic mixture distributions as proposed by Salimans et al. [51].",
                "The residual cells of the encoder are composed of batch normalization layers [23], Swish activation functions [49], convolutional layers, and Squeeze-and-Excitation (SE) blocks [19] as proposed in [57]. In the decoder, we also follow [57] and build the residual cells out of batch normalization layers, 1x1 convolutions, Swish activations, depthwise separable convolutions [9], and SE blocks. However, we omitted normalizing flow because in our experiments it showed to increase the training time without improving the prediction accuracy significantly.",
                "For each dataset, we chose the size of the architecture individually to achieve acceptable accuracy while keeping the training time reasonable. Tab. 6 provides details about the used hyperparameters.  In general, the number of latent groups L should be chosen depending on the complexity of the task at hand. We made our decision based on the L of the NVAE [57] but reduced it for computational reasons. For FusionCelebA and FusionT-LESS, we use 17 latent groups, for FusionMNIST only seven. Using more latent groups improves the results but increases the computational effort significantly.",
                "For all experiments, we used GPUs of type NVIDIA Tesla V100 with 32GB of memory and trained with an AdaMax optimizer [26]. We applied a cosine annealing schedule for the learning rate [40] starting at 0.01 and ending at 0.0001."
            ],
            "paragraphs": [
                {
                    "text": "For FusionMNIST and FusionT-LESS, we model the decoder's output by pixelwise independent Bernoulli distributions. For FusionCelebA, we use pixel-wise independent discretized logistic mixture distributions as proposed by Salimans et al. [51].",
                    "citations": [
                        {
                            "target": "b49",
                            "text": "[51]"
                        }
                    ]
                },
                {
                    "text": "The residual cells of the encoder are composed of batch normalization layers [23], Swish activation functions [49], convolutional layers, and Squeeze-and-Excitation (SE) blocks [19] as proposed in [57]. In the decoder, we also follow [57] and build the residual cells out of batch normalization layers, 1x1 convolutions, Swish activations, depthwise separable convolutions [9], and SE blocks. However, we omitted normalizing flow because in our experiments it showed to increase the training time without improving the prediction accuracy significantly.",
                    "citations": [
                        {
                            "target": "b22",
                            "text": "[23]"
                        },
                        {
                            "target": "b47",
                            "text": "[49]"
                        },
                        {
                            "target": "b18",
                            "text": "[19]"
                        },
                        {
                            "target": "b55",
                            "text": "[57]"
                        },
                        {
                            "target": "b55",
                            "text": "[57]"
                        },
                        {
                            "target": "b8",
                            "text": "[9]"
                        }
                    ]
                },
                {
                    "text": "For each dataset, we chose the size of the architecture individually to achieve acceptable accuracy while keeping the training time reasonable. Tab. 6 provides details about the used hyperparameters.  In general, the number of latent groups L should be chosen depending on the complexity of the task at hand. We made our decision based on the L of the NVAE [57] but reduced it for computational reasons. For FusionCelebA and FusionT-LESS, we use 17 latent groups, for FusionMNIST only seven. Using more latent groups improves the results but increases the computational effort significantly.",
                    "citations": [
                        {
                            "target": "b55",
                            "text": "[57]"
                        }
                    ]
                },
                {
                    "text": "For all experiments, we used GPUs of type NVIDIA Tesla V100 with 32GB of memory and trained with an AdaMax optimizer [26]. We applied a cosine annealing schedule for the learning rate [40] starting at 0.01 and ending at 0.0001.",
                    "citations": [
                        {
                            "target": "b25",
                            "text": "[26]"
                        },
                        {
                            "target": "b38",
                            "text": "[40]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "B Derivation of Bayesian Aggregation",
            "content": [
                "We use two related encoders to learn a latent observation µ i = enc µ (x i , y) with its corresponding variance values σ i = enc σ (x i , y).",
                "Assuming a factorized Gaussian prior distribution in the latent space p(z) = N (z|µ z,0 , diag(σ z,0 )), we can derive the factorized posterior distribution q ϕ (z|y) = N (z|µ z , diag(σ z ) in closed form using standard Gaussian conditioning [3] following [59] σ",
                "where ⊖ denotes element-wise inversion, ⊙ denotes element-wise multiplication, and ⊘ denotes element-wise division."
            ],
            "paragraphs": [
                {
                    "text": "We use two related encoders to learn a latent observation µ i = enc µ (x i , y) with its corresponding variance values σ i = enc σ (x i , y).",
                    "citations": []
                },
                {
                    "text": "Assuming a factorized Gaussian prior distribution in the latent space p(z) = N (z|µ z,0 , diag(σ z,0 )), we can derive the factorized posterior distribution q ϕ (z|y) = N (z|µ z , diag(σ z ) in closed form using standard Gaussian conditioning [3] following [59] σ",
                    "citations": [
                        {
                            "target": "b2",
                            "text": "[3]"
                        },
                        {
                            "target": "b57",
                            "text": "[59]"
                        }
                    ]
                },
                {
                    "text": "where ⊖ denotes element-wise inversion, ⊙ denotes element-wise multiplication, and ⊘ denotes element-wise division.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "C Derivation of FusionVAE's ELBO",
            "content": [
                "We start with the following KL divergence between the approximate posterior and the real posterior, KL(q θ (z|y)||p θ (z|x, y)) ≥ 0.",
                "Next, we apply the Bayes's theorem to obtain -q θ (z|y) log p(y|x, z)p(z|x) p(y|x)q θ (z|y) dz ≥ 0.",
                "This leads to -E q θ (z|y) [log p(y|x, z)] -KL(q θ (z|y)||p(z|x))",
                "+ q θ (z|y) log p(y|x)dz ≥ 0.",
                "The term log p(y|x) can be moved out from the third integral component, and leaves the integral becoming 1. Finally, we obtain the ELBO of the conditional log-likelihood log p(y|x) ≥ E q θ (z|y) [log p(y|x, z)] + KL(q θ (z|y)||p(z|x))."
            ],
            "paragraphs": [
                {
                    "text": "We start with the following KL divergence between the approximate posterior and the real posterior, KL(q θ (z|y)||p θ (z|x, y)) ≥ 0.",
                    "citations": []
                },
                {
                    "text": "Next, we apply the Bayes's theorem to obtain -q θ (z|y) log p(y|x, z)p(z|x) p(y|x)q θ (z|y) dz ≥ 0.",
                    "citations": []
                },
                {
                    "text": "This leads to -E q θ (z|y) [log p(y|x, z)] -KL(q θ (z|y)||p(z|x))",
                    "citations": []
                },
                {
                    "text": "+ q θ (z|y) log p(y|x)dz ≥ 0.",
                    "citations": []
                },
                {
                    "text": "The term log p(y|x) can be moved out from the third integral component, and leaves the integral becoming 1. Finally, we obtain the ELBO of the conditional log-likelihood log p(y|x) ≥ E q θ (z|y) [log p(y|x, z)] + KL(q θ (z|y)||p(z|x)).",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "D Ablation Studies",
            "content": [
                "This is a supplement for the aggregation ablation study in Sec. 6.3. In Tab. 5, we saw that the average NLL of all experiments using mean and max aggregation methods are similar. Fig. 6 shows the corresponding qualitative results. However, even though the NLL is very similar, the results of the aggregation of all features (MaxAggAll and MeanAggAll) are much more blurry than the results of the aggregation with addition (MaxAggAdd and MeanAggAdd). This is in conformity with the MSE min results. It indicates that the NLL alone is not always the best metric to assess the visual closeness to real faces. When carefully examining the images of the addition aggregations, you could argue that the predictions with zero input images look slightly more realistic for max aggregation while for three input images, mean aggregation seems to be marginally better. This again confirms the validity of the MSE min results even though the NLL results are also in accordance for this comparison."
            ],
            "paragraphs": [
                {
                    "text": "This is a supplement for the aggregation ablation study in Sec. 6.3. In Tab. 5, we saw that the average NLL of all experiments using mean and max aggregation methods are similar. Fig. 6 shows the corresponding qualitative results. However, even though the NLL is very similar, the results of the aggregation of all features (MaxAggAll and MeanAggAll) are much more blurry than the results of the aggregation with addition (MaxAggAdd and MeanAggAdd). This is in conformity with the MSE min results. It indicates that the NLL alone is not always the best metric to assess the visual closeness to real faces. When carefully examining the images of the addition aggregations, you could argue that the predictions with zero input images look slightly more realistic for max aggregation while for three input images, mean aggregation seems to be marginally better. This again confirms the validity of the MSE min results even though the NLL results are also in accordance for this comparison.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "E Statistic Significance of the Results",
            "content": [
                "All experiments for this publications were carefully designed and optimized so that the training procedures are stable and lead to reproducible results. However, the data processing pipelines introduce randomness which lead to non-deterministic training outcomes due to multi-GPU training. We therefore ran every experiment three times and reported the results of the best training in Sec. 6. In Tabs. 7 to 12 we provide the means and variances of the three training runs.    The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well."
            ],
            "paragraphs": [
                {
                    "text": "All experiments for this publications were carefully designed and optimized so that the training procedures are stable and lead to reproducible results. However, the data processing pipelines introduce randomness which lead to non-deterministic training outcomes due to multi-GPU training. We therefore ran every experiment three times and reported the results of the best training in Sec. 6. In Tabs. 7 to 12 we provide the means and variances of the three training runs.    The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "F Reconstruction",
            "content": [],
            "paragraphs": []
        }
    ],
    "references": [
        {
            "id": "b0",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "CVAE-GAN: fine-grained image generation through asymmetric training"
        },
        {
            "id": "b1",
            "title": "",
            "authors": [],
            "date": "2019",
            "journal": "Recurrent Kalman networks: Factorized inference in high-dimensional deep feature spaces"
        },
        {
            "id": "b2",
            "title": "",
            "authors": [],
            "date": "2006",
            "journal": "Pattern Recognition and Machine Learning"
        },
        {
            "id": "b3",
            "title": "Importance weighted autoencoders",
            "authors": [
                "Y Burda",
                "R Grosse",
                "R Salakhutdinov"
            ],
            "date": "2016",
            "journal": "ICLR"
        },
        {
            "id": "b4",
            "title": "A computational approach to edge detection",
            "authors": [
                "J Canny"
            ],
            "date": "1986",
            "journal": "IEEE TPAMI"
        },
        {
            "id": "b5",
            "title": "",
            "authors": [
                "X Chen",
                "D Kingma",
                "T Salimans",
                "Y Duan",
                "P Dhariwal",
                "J Schulman",
                "I Sutskever",
                "P Abbeel"
            ],
            "date": "2017",
            "journal": "Variational lossy autoencoder"
        },
        {
            "id": "b6",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Multi-view 3D object detection network for autonomous driving"
        },
        {
            "id": "b7",
            "title": "Very deep VAEs generalize autoregressive models and can outperform them on images",
            "authors": [
                "R Child"
            ],
            "date": "2021",
            "journal": "ICLR"
        },
        {
            "id": "b8",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Xception: Deep learning with depthwise separable convolutions"
        },
        {
            "id": "b9",
            "title": "Generative adversarial nets",
            "authors": [
                "I Goodfellow",
                "J Pouget-Abadie",
                "M Mirza",
                "B Xu",
                "D Warde-Farley",
                "S Ozair",
                "A Courville",
                "Y Bengio"
            ],
            "date": "2014",
            "journal": "NeurIPS"
        },
        {
            "id": "b10",
            "title": "DRAW: A recurrent neural network for image generation",
            "authors": [
                "K Gregor",
                "I Danihelka",
                "A Graves",
                "D Rezende",
                "D Wierstra"
            ],
            "date": "Jul 2015",
            "journal": "ICML. Proc. Machine Learning Research"
        },
        {
            "id": "b11",
            "title": "Recent advances in convolutional neural networks",
            "authors": [
                "J Gu",
                "Z Wang",
                "J Kuen",
                "L Ma",
                "A Shahroudy",
                "B Shuai",
                "T Liu",
                "X Wang",
                "G Wang",
                "J Cai"
            ],
            "date": "2018",
            "journal": "Pattern Recognition"
        },
        {
            "id": "b12",
            "title": "PixelVAE: A latent variable model for natural images",
            "authors": [
                "I Gulrajani",
                "K Kumar",
                "F Ahmed",
                "A Taiga",
                "F Visin",
                "D Vazquez",
                "A Courville"
            ],
            "date": "2017",
            "journal": "ICLR"
        },
        {
            "id": "b13",
            "title": "FuseGAN: Learning to fuse multi-focus image via conditional generative adversarial network",
            "authors": [
                "X Guo",
                "R Nie",
                "J Cao",
                "D Zhou",
                "L Mei",
                "K He"
            ],
            "date": "2019",
            "journal": "IEEE TMM"
        },
        {
            "id": "b14",
            "title": "",
            "authors": [],
            "date": "2016",
            "journal": "Deep residual learning for image recognition"
        },
        {
            "id": "b15",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "FFB6D: A full flow bidirectional fusion network for 6D pose estimation"
        },
        {
            "id": "b16",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "PVN3D: A deep point-wise 3D keypoints voting network for 6DoF pose estimation"
        },
        {
            "id": "b17",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects"
        },
        {
            "id": "b18",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Squeeze-and-excitation networks"
        },
        {
            "id": "b19",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Densely connected convolutional networks"
        },
        {
            "id": "b20",
            "title": "A generative adversarial network with adaptive constraints for multi-focus image fusion",
            "authors": [
                "J Huang",
                "Z Le",
                "Y Ma",
                "X Mei",
                "F Fan"
            ],
            "date": "2020",
            "journal": "Neural Computing and Applications"
        },
        {
            "id": "b21",
            "title": "Globally and locally consistent image completion",
            "authors": [
                "S Iizuka",
                "E Simo-Serra",
                "H Ishikawa"
            ],
            "date": "2017",
            "journal": "TOG"
        },
        {
            "id": "b22",
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "authors": [
                "S Ioffe",
                "C Szegedy"
            ],
            "date": "2015",
            "journal": "PMLR"
        },
        {
            "id": "b23",
            "title": "Unsupervised deep image fusion with structure tensor representations",
            "authors": [
                "H Jung",
                "Y Kim",
                "H Jang",
                "N Ha",
                "K Sohn"
            ],
            "date": "2020",
            "journal": "IEEE TIP"
        },
        {
            "id": "b24",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "SetVAE: Learning hierarchical composition for generative modeling of set-structured data"
        },
        {
            "id": "b25",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                "D Kingma",
                "J Ba"
            ],
            "date": "May 2015",
            "journal": "ICLR"
        },
        {
            "id": "b26",
            "title": "Auto-encoding variational bayes",
            "authors": [
                "D Kingma",
                "M Welling"
            ],
            "date": "2014",
            "journal": "ICLR"
        },
        {
            "id": "b27",
            "title": "Improving variational inference with inverse autoregressive flow",
            "authors": [
                "D Kingma",
                "T Salimans",
                "R Jozefowicz",
                "X Chen",
                "I Sutskever",
                "M Welling"
            ],
            "date": "2016",
            "journal": "NeurIPS"
        },
        {
            "id": "b28",
            "title": "Mask-specific inpainting with deep neural networks",
            "authors": [
                "R Köhler",
                "C Schuler",
                "B Schölkopf",
                "S Harmeling"
            ],
            "date": "2014",
            "journal": "German Conf. Pattern Recognition"
        },
        {
            "id": "b29",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Joint 3D proposal generation and object detection from view aggregation"
        },
        {
            "id": "b30",
            "title": "Autoencoding beyond pixels using a learned similarity metric",
            "authors": [
                "A Larsen",
                "S Sønderby",
                "H Larochelle",
                "O Winther"
            ],
            "date": "2016. 1998",
            "journal": "The MNIST database of handwritten digits"
        },
        {
            "id": "b31",
            "title": "DenseFuse: A fusion approach to infrared and visible images",
            "authors": [
                "H Li",
                "X Wu"
            ],
            "date": "2018",
            "journal": "IEEE TIP"
        },
        {
            "id": "b32",
            "title": "Infrared and visible image fusion with ResNet and zero-phase component analysis",
            "authors": [
                "H Li",
                "X Wu",
                "T Durrani"
            ],
            "date": "2019",
            "journal": "Infrared Physics & Technology"
        },
        {
            "id": "b33",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Infrared and visible image fusion using a deep learning framework"
        },
        {
            "id": "b34",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "Generative face completion"
        },
        {
            "id": "b35",
            "title": "A medical image fusion method based on convolutional neural networks",
            "authors": [
                "Y Liu",
                "X Chen",
                "J Cheng",
                "H Peng"
            ],
            "date": "2017",
            "journal": "Int. Conf. Information Fusion"
        },
        {
            "id": "b36",
            "title": "Multi-focus image fusion with a deep convolutional neural network",
            "authors": [
                "Y Liu",
                "X Chen",
                "H Peng",
                "Z Wang"
            ],
            "date": "2017",
            "journal": "Information Fusion"
        },
        {
            "id": "b37",
            "title": "Deep learning face attributes in the wild",
            "authors": [
                "Z Liu",
                "P Luo",
                "X Wang",
                "X Tang"
            ],
            "date": "Dec 2015",
            "journal": "ICCV"
        },
        {
            "id": "b38",
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "authors": [
                "I Loshchilov",
                "F Hutter"
            ],
            "date": "2017",
            "journal": "ICLR"
        },
        {
            "id": "b39",
            "title": "Infrared and visible image fusion via detail preserving adversarial learning",
            "authors": [
                "J Ma",
                "P Liang",
                "W Yu",
                "C Chen",
                "X Guo",
                "J Wu",
                "J Jiang"
            ],
            "date": "2020",
            "journal": "Information Fusion"
        },
        {
            "id": "b40",
            "title": "DDcGAN: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion",
            "authors": [
                "J Ma",
                "H Xu",
                "J Jiang",
                "X Mei",
                "X Zhang"
            ],
            "date": "2020",
            "journal": "IEEE TIP"
        },
        {
            "id": "b41",
            "title": "FusionGAN: A generative adversarial network for infrared and visible image fusion",
            "authors": [
                "J Ma",
                "W Yu",
                "P Liang",
                "C Li",
                "J Jiang"
            ],
            "date": "2019",
            "journal": "Information Fusion"
        },
        {
            "id": "b42",
            "title": "BIVA: A very deep hierarchy of latent variables for generative modeling",
            "authors": [
                "L Maaløe",
                "M Fraccaro",
                "V Liévin",
                "O Winther"
            ],
            "date": "2019",
            "journal": "NeurIPS"
        },
        {
            "id": "b43",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "Bayesian image reconstruction using deep generative models"
        },
        {
            "id": "b44",
            "title": "",
            "authors": [],
            "date": "2021",
            "journal": "Dual contradistinctive generative autoencoder"
        },
        {
            "id": "b45",
            "title": "",
            "authors": [],
            "date": "2016",
            "journal": "Context encoders: Feature learning by inpainting"
        },
        {
            "id": "b46",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "DeepFuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs"
        },
        {
            "id": "b47",
            "title": "Searching for activation functions",
            "authors": [
                "P Ramachandran",
                "B Zoph",
                "Q Le"
            ],
            "date": "2018",
            "journal": "ICLR"
        },
        {
            "id": "b48",
            "title": "",
            "authors": [],
            "date": "2019",
            "journal": "PixelVAE++: Improved PixelVAE with discrete prior"
        },
        {
            "id": "b49",
            "title": "PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications",
            "authors": [
                "T Salimans",
                "A Karpathy",
                "X Chen",
                "D Kingma"
            ],
            "date": "2017",
            "journal": "ICLR"
        },
        {
            "id": "b50",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                "K Simonyan",
                "A Zisserman"
            ],
            "date": "2015",
            "journal": "ICLR"
        },
        {
            "id": "b51",
            "title": "Learning structured output representation using deep conditional generative models",
            "authors": [
                "K Sohn",
                "H Lee",
                "X Yan"
            ],
            "date": "2015",
            "journal": "NeurIPS"
        },
        {
            "id": "b52",
            "title": "Ladder variational autoencoders",
            "authors": [
                "C Sønderby",
                "T Raiko",
                "L Maaløe",
                "S Sønderby",
                "O Winther"
            ],
            "date": "2016",
            "journal": "NeurIPS"
        },
        {
            "id": "b53",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Contextualbased image inpainting: Infer, match, and translate"
        },
        {
            "id": "b54",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "SPG-Net: Segmentation prediction and guidance network for image inpainting"
        },
        {
            "id": "b55",
            "title": "NVAE: A deep hierarchical variational autoencoder",
            "authors": [
                "A Vahdat",
                "J Kautz"
            ],
            "date": "2020",
            "journal": "NeurIPS"
        },
        {
            "id": "b56",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "DVAE++: Discrete variational autoencoders with overlapping transformations"
        },
        {
            "id": "b57",
            "title": "Bayesian context aggregation for neural processes",
            "authors": [
                "M Volpp",
                "F Flürenbrock",
                "L Grossberger",
                "C Daniel",
                "G Neumann"
            ],
            "date": "2020",
            "journal": "ICLR"
        },
        {
            "id": "b58",
            "title": "",
            "authors": [],
            "date": "2016",
            "journal": "An uncertain future: Forecasting from static images using variational autoencoders"
        },
        {
            "id": "b59",
            "title": "",
            "authors": [],
            "date": "2019",
            "journal": "DenseFusion: 6D object pose estimation by iterative dense fusion"
        },
        {
            "id": "b60",
            "title": "GraspFusionNet: a two-stage multi-parameter grasp detection network based on RGB-XYZ fusion in dense clutter",
            "authors": [
                "W Wang",
                "W Liu",
                "J Hu",
                "Y Fang",
                "Q Shao",
                "J Qi"
            ],
            "date": "2020",
            "journal": "Machine Vision and Applications"
        },
        {
            "id": "b61",
            "title": "Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators",
            "authors": [
                "H Xu",
                "P Liang",
                "W Yu",
                "J Jiang",
                "J Ma"
            ],
            "date": "2019",
            "journal": "IJCAI"
        },
        {
            "id": "b62",
            "title": "FusionDN: A unified densely connected network for image fusion",
            "authors": [
                "H Xu",
                "J Ma",
                "Z Le",
                "J Jiang",
                "X Guo"
            ],
            "date": "04 2020",
            "journal": "AAAI"
        },
        {
            "id": "b63",
            "title": "Shift-Net: Image inpainting via deep feature rearrangement",
            "authors": [
                "Z Yan",
                "X Li",
                "M Li",
                "W Zuo",
                "S Shan"
            ],
            "date": "2018",
            "journal": "ECCV"
        },
        {
            "id": "b64",
            "title": "",
            "authors": [],
            "date": "2017",
            "journal": "High-resolution image inpainting using multi-scale neural patch synthesis"
        },
        {
            "id": "b65",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "D-CVF: Generating joint camera and lidar features using cross-view spatial feature fusion for 3D object detection"
        },
        {
            "id": "b66",
            "title": "",
            "authors": [],
            "date": "2018",
            "journal": "Generative image inpainting with contextual attention"
        }
    ]
}