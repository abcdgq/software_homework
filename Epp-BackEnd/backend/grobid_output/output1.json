{
    "metadata": {
        "title": "SOFT: Softmax-free Transformer with Linear Complexity",
        "authors": [
            {
                "name": "Jiachen Lu",
                "affiliations": [
                    {
                        "name": "Fudan University",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Jinghan Yao",
                "affiliations": [
                    {
                        "name": "Fudan University",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Junge Zhang",
                "affiliations": [
                    {
                        "name": "Fudan University",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Xiatian Zhu",
                "affiliations": [
                    {
                        "name": "University of Surrey",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Hang Xu",
                "affiliations": [
                    {
                        "name": "Huawei Noah's Ark Lab https://fudan-zvg.github.io/SOFT",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Weiguo Gao",
                "affiliations": [
                    {
                        "name": "Fudan University",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Chunjing Xu",
                "affiliations": [
                    {
                        "name": "Huawei Noah's Ark Lab https://fudan-zvg.github.io/SOFT",
                        "type": "laboratory"
                    }
                ]
            },
            {
                "name": "Tao Xiang",
                "affiliations": [
                    {
                        "name": "University of Surrey",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Li Zhang",
                "affiliations": [
                    {
                        "name": "Fudan University",
                        "type": "institution"
                    }
                ]
            }
        ],
        "abstract": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the selfattention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity."
    },
    "sections": [
        {
            "n": "1",
            "title": "Introduction",
            "content": [
                "Recently the step change brought by Transformers [34] in natural language processing (NLP) [10,4] seems to have arrived in vision [11,42,48,47]. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants have shown to be able to outperform CNNs on various vision tasks [8,16]. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity O(n 2 ) with the token sequence length n in both computation (time) and memory (space) since an n × n sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks. Figure 1: Top1-Accuracy on ImageNet [9] validation set with respect to parameters and the memory usage corresponding to the token sequence length in practice compared to other methods. (a) Comparison with CNN models: RegNet [27], ResNet [14] and Transformer models: PVT [36], DeiT [32],",
                "ViT [11], T2T-ViT [42], Twins-SVT [6] and SAN10 [46]; (b) Comparison with Transformer [34], Linformer [35], Nyströformer [40] and Performer [5]. The memory usage is measured with a batch size of 1 on a 16GB Tesla V100.",
                "A natural solution is to reduce the complexity of self-attention computation via approximation. Indeed, there have been a number of attempts in NLP [35,5,19,40]. For example, [35] takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, [5,18] both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention. [19] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [40] uses Nyström matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2).",
                "In this work, we identify that the limitations of existing efficient Transformers are caused by the use of softmax self-attention, and for the first time propose a softmax-free Transformer. More specifically, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors [34]. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity O(n) in both space and time. Specifically, SOFT uses Gaussian kernel to define the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix.",
                "We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure 1b), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on ImageNet [9] classification in the accuracy/complexity trade-off (Figure 1a)."
            ],
            "paragraphs": [
                {
                    "text": "Recently the step change brought by Transformers [34] in natural language processing (NLP) [10,4] seems to have arrived in vision [11,42,48,47]. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants have shown to be able to outperform CNNs on various vision tasks [8,16]. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity O(n 2 ) with the token sequence length n in both computation (time) and memory (space) since an n × n sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks. Figure 1: Top1-Accuracy on ImageNet [9] validation set with respect to parameters and the memory usage corresponding to the token sequence length in practice compared to other methods. (a) Comparison with CNN models: RegNet [27], ResNet [14] and Transformer models: PVT [36], DeiT [32],",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        },
                        {
                            "target": "b9",
                            "text": "[10,"
                        },
                        {
                            "target": "b3",
                            "text": "4]"
                        },
                        {
                            "target": "b10",
                            "text": "[11,"
                        },
                        {
                            "target": "b42",
                            "text": "42,"
                        },
                        {
                            "target": "b48",
                            "text": "48,"
                        },
                        {
                            "target": "b47",
                            "text": "47]"
                        },
                        {
                            "target": "b10",
                            "text": "[11]"
                        },
                        {
                            "target": "b7",
                            "text": "[8,"
                        },
                        {
                            "target": "b15",
                            "text": "16]"
                        },
                        {
                            "target": "b8",
                            "text": "[9]"
                        },
                        {
                            "target": "b26",
                            "text": "[27]"
                        },
                        {
                            "target": "b13",
                            "text": "[14]"
                        },
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b31",
                            "text": "[32]"
                        }
                    ]
                },
                {
                    "text": "ViT [11], T2T-ViT [42], Twins-SVT [6] and SAN10 [46]; (b) Comparison with Transformer [34], Linformer [35], Nyströformer [40] and Performer [5]. The memory usage is measured with a batch size of 1 on a 16GB Tesla V100.",
                    "citations": [
                        {
                            "target": "b10",
                            "text": "[11]"
                        },
                        {
                            "target": "b42",
                            "text": "[42]"
                        },
                        {
                            "target": "b5",
                            "text": "[6]"
                        },
                        {
                            "target": "b46",
                            "text": "[46]"
                        },
                        {
                            "target": "b33",
                            "text": "[34]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b4",
                            "text": "[5]"
                        }
                    ]
                },
                {
                    "text": "A natural solution is to reduce the complexity of self-attention computation via approximation. Indeed, there have been a number of attempts in NLP [35,5,19,40]. For example, [35] takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, [5,18] both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention. [19] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [40] uses Nyström matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2).",
                    "citations": [
                        {
                            "target": "b34",
                            "text": "[35,"
                        },
                        {
                            "target": "b4",
                            "text": "5,"
                        },
                        {
                            "target": "b18",
                            "text": "19,"
                        },
                        {
                            "target": "b39",
                            "text": "40]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b4",
                            "text": "[5,"
                        },
                        {
                            "target": "b17",
                            "text": "18]"
                        },
                        {
                            "target": "b18",
                            "text": "[19]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "In this work, we identify that the limitations of existing efficient Transformers are caused by the use of softmax self-attention, and for the first time propose a softmax-free Transformer. More specifically, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors [34]. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity O(n) in both space and time. Specifically, SOFT uses Gaussian kernel to define the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure 1b), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on ImageNet [9] classification in the accuracy/complexity trade-off (Figure 1a).",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "[9]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "2",
            "title": "Related work",
            "content": [
                "Vision Transformers There is a surge of research interests recently in exploiting Transformers for visual recognition tasks [37,36,42,32,45], inspired by their remarkable success in NLP [34,10,4].",
                "Core to these NLP and vision transformers is the same self-attention mechanism [34] that computes a self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity with the sequence length in both space and time, which thus limits the scalability of Transformers in dealing with long sequences. This limitation is more serious in vision than NLP: To process an image with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control the computational cost. Given higher resolution images, the patch size also needs to be enlarged proportionally sacrificing the spatial resolution. This limits the capability of Transformers, e.g., learning fine-grained feature representation as required in many visual recognition tasks.",
                "Linear Transformers Recently, there have been a number of linear/efficient variants [5,35,18,19,31,25,17] of Transformers in NLP. For example, [35] learns to shrink the length of Key and Value based on a low-rank assumption. [19] adopts a hashing strategy to selective the most similar pairs and only compute attention among them. [5,18] utilize different kernel functions for approximating softmax-based self-attention matrix. [25] applies random feature mapping on the sequences to approach the original softmax function. [17] decreases the time and memory consumption of the attention matrix by replacing the softmax function with its linear-complexity recurrent alternative. When applied to visual recognition tasks, however, we show that these models have considerable performance degradation compared to the standard Transformers [34] (see Sec. 4.2).",
                "The most related work to SOFT is [40] which uses the Nyström matrix decomposition to avoid computing the full attention matrix. However, this method suffers from several theoretical defects:",
                "(1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention matrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply applied to all the ingredient matrices in [40]. Such an approximation is not guaranteed theoretically.",
                "(2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix inverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the above methods, in this paper we propose a softmax-free self-attention mechanism that facilitates matrix decomposition for complexity minimization with theoretical guarantees.",
                "3 Method"
            ],
            "paragraphs": [
                {
                    "text": "Vision Transformers There is a surge of research interests recently in exploiting Transformers for visual recognition tasks [37,36,42,32,45], inspired by their remarkable success in NLP [34,10,4].",
                    "citations": [
                        {
                            "target": "b36",
                            "text": "[37,"
                        },
                        {
                            "target": "b35",
                            "text": "36,"
                        },
                        {
                            "target": "b42",
                            "text": "42,"
                        },
                        {
                            "target": "b31",
                            "text": "32,"
                        },
                        {
                            "target": "b45",
                            "text": "45]"
                        },
                        {
                            "target": "b33",
                            "text": "[34,"
                        },
                        {
                            "target": "b9",
                            "text": "10,"
                        },
                        {
                            "target": "b3",
                            "text": "4]"
                        }
                    ]
                },
                {
                    "text": "Core to these NLP and vision transformers is the same self-attention mechanism [34] that computes a self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity with the sequence length in both space and time, which thus limits the scalability of Transformers in dealing with long sequences. This limitation is more serious in vision than NLP: To process an image with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control the computational cost. Given higher resolution images, the patch size also needs to be enlarged proportionally sacrificing the spatial resolution. This limits the capability of Transformers, e.g., learning fine-grained feature representation as required in many visual recognition tasks.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "Linear Transformers Recently, there have been a number of linear/efficient variants [5,35,18,19,31,25,17] of Transformers in NLP. For example, [35] learns to shrink the length of Key and Value based on a low-rank assumption. [19] adopts a hashing strategy to selective the most similar pairs and only compute attention among them. [5,18] utilize different kernel functions for approximating softmax-based self-attention matrix. [25] applies random feature mapping on the sequences to approach the original softmax function. [17] decreases the time and memory consumption of the attention matrix by replacing the softmax function with its linear-complexity recurrent alternative. When applied to visual recognition tasks, however, we show that these models have considerable performance degradation compared to the standard Transformers [34] (see Sec. 4.2).",
                    "citations": [
                        {
                            "target": "b4",
                            "text": "[5,"
                        },
                        {
                            "target": "b34",
                            "text": "35,"
                        },
                        {
                            "target": "b17",
                            "text": "18,"
                        },
                        {
                            "target": "b18",
                            "text": "19,"
                        },
                        {
                            "target": "b30",
                            "text": "31,"
                        },
                        {
                            "target": "b24",
                            "text": "25,"
                        },
                        {
                            "target": "b16",
                            "text": "17]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b18",
                            "text": "[19]"
                        },
                        {
                            "target": "b4",
                            "text": "[5,"
                        },
                        {
                            "target": "b17",
                            "text": "18]"
                        },
                        {
                            "target": "b24",
                            "text": "[25]"
                        },
                        {
                            "target": "b16",
                            "text": "[17]"
                        },
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "The most related work to SOFT is [40] which uses the Nyström matrix decomposition to avoid computing the full attention matrix. However, this method suffers from several theoretical defects:",
                    "citations": [
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "(1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention matrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply applied to all the ingredient matrices in [40]. Such an approximation is not guaranteed theoretically.",
                    "citations": [
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "(2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix inverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the above methods, in this paper we propose a softmax-free self-attention mechanism that facilitates matrix decomposition for complexity minimization with theoretical guarantees.",
                    "citations": []
                },
                {
                    "text": "3 Method",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.1",
            "title": "Softmax-free self-attention formulation",
            "content": [
                "A schematic illustration of our model is given in Figure 2. Let's first look at our attention module design. Given a sequence of n tokens X ∈ R n×d with each token represented by a d-dimensional feature vector, self-attention [34] aims to discover the correlations of all token pairs exhaustively.",
                "Formally, X is first linearly projected into three d e -dimensional spaces (query, key, and values) as:",
                "where W q , W k , W v ∈ R d×de are learnable matrices. Self-attention can be expressed in a generic formulation as:",
                "where is the Hadamard product, and i, j ∈ {1, • • • , n} index the tokens. The key self-attention function α : R de × R de → R is composed of a nonlinear function β : R → R and a relation function γ : R de × R de → R. A dominant instantiation of α is the scaled dot-product based softmax self-attention [34], defined as",
                "Whilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed earlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we introduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel as:",
                "(4)",
                "Figure 2: Schematic illustration of the proposed softmax-free self-attention (SOFT) method. P.E.: Position embedding. Dash lines: linear projection. dh: the hidden dim of each attention head. • denotes the matrix dot product.",
                "To preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices W q and W k in Eq (1) identical (i.e., Q = K). Our self-attention matrix is then written as:",
                "(",
                "For notation simplicity, we define the matrix formulation as: S = exp (Q K).",
                "Remarks Our self-attention matrix S has three important properties: (1) It is symmetric;",
                "(2) All the elements lie in a unit range of [0, 1]; (3) All diagonal elements hold the largest value 1 (selfreinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to 0.",
                "As Gaussian kernel is a positive definite kernel [12], S is deemed a Gram matrix. However, we find that when using our kernel-based self-attention matrix S without linearization, the training of a transformer fails to converge. This might explain why softmax dot-product based self-attention [34] is so popular in vanilla transformers."
            ],
            "paragraphs": [
                {
                    "text": "A schematic illustration of our model is given in Figure 2. Let's first look at our attention module design. Given a sequence of n tokens X ∈ R n×d with each token represented by a d-dimensional feature vector, self-attention [34] aims to discover the correlations of all token pairs exhaustively.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "Formally, X is first linearly projected into three d e -dimensional spaces (query, key, and values) as:",
                    "citations": []
                },
                {
                    "text": "where W q , W k , W v ∈ R d×de are learnable matrices. Self-attention can be expressed in a generic formulation as:",
                    "citations": []
                },
                {
                    "text": "where is the Hadamard product, and i, j ∈ {1, • • • , n} index the tokens. The key self-attention function α : R de × R de → R is composed of a nonlinear function β : R → R and a relation function γ : R de × R de → R. A dominant instantiation of α is the scaled dot-product based softmax self-attention [34], defined as",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "Whilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed earlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we introduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel as:",
                    "citations": []
                },
                {
                    "text": "(4)",
                    "citations": []
                },
                {
                    "text": "Figure 2: Schematic illustration of the proposed softmax-free self-attention (SOFT) method. P.E.: Position embedding. Dash lines: linear projection. dh: the hidden dim of each attention head. • denotes the matrix dot product.",
                    "citations": []
                },
                {
                    "text": "To preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices W q and W k in Eq (1) identical (i.e., Q = K). Our self-attention matrix is then written as:",
                    "citations": []
                },
                {
                    "text": "(",
                    "citations": []
                },
                {
                    "text": "For notation simplicity, we define the matrix formulation as: S = exp (Q K).",
                    "citations": []
                },
                {
                    "text": "Remarks Our self-attention matrix S has three important properties: (1) It is symmetric;",
                    "citations": []
                },
                {
                    "text": "(2) All the elements lie in a unit range of [0, 1]; (3) All diagonal elements hold the largest value 1 (selfreinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to 0.",
                    "citations": []
                },
                {
                    "text": "As Gaussian kernel is a positive definite kernel [12], S is deemed a Gram matrix. However, we find that when using our kernel-based self-attention matrix S without linearization, the training of a transformer fails to converge. This might explain why softmax dot-product based self-attention [34] is so popular in vanilla transformers.",
                    "citations": [
                        {
                            "target": "b11",
                            "text": "[12]"
                        },
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.2",
            "title": "Low-rank regularization via matrix decomposition with linear complexity",
            "content": [
                "To solve the convergence and quadratic complexity problems, we leverage matrix decomposition as a unified solution with low-rank regularization. In particular, we consider Nyström [39], which is originally a low-rank matrix approximation algorithm. This enables our model's complexity to be reduced significantly without computing the full self-attention matrix S.",
                "We make this choice because our S is positive semi-definite (i.e., a Gram matrix) without follow-up normalization which are all necessary conditions for Nyström. In contrast, [40] totally ignores these requirements, leading to theoretical flaw in its approximation.",
                "To define the Nyström method formally, let us express S = exp (Q K) as a block matrix:",
                "where",
                "Through Nyström decomposition (see derivative details in Appendix A.1), an approximation can be represented as:",
                "and A † is the Moore-Penrose (a generalized) inverse of A."
            ],
            "paragraphs": [
                {
                    "text": "To solve the convergence and quadratic complexity problems, we leverage matrix decomposition as a unified solution with low-rank regularization. In particular, we consider Nyström [39], which is originally a low-rank matrix approximation algorithm. This enables our model's complexity to be reduced significantly without computing the full self-attention matrix S.",
                    "citations": [
                        {
                            "target": "b38",
                            "text": "[39]"
                        }
                    ]
                },
                {
                    "text": "We make this choice because our S is positive semi-definite (i.e., a Gram matrix) without follow-up normalization which are all necessary conditions for Nyström. In contrast, [40] totally ignores these requirements, leading to theoretical flaw in its approximation.",
                    "citations": [
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "To define the Nyström method formally, let us express S = exp (Q K) as a block matrix:",
                    "citations": []
                },
                {
                    "text": "where",
                    "citations": []
                },
                {
                    "text": "Through Nyström decomposition (see derivative details in Appendix A.1), an approximation can be represented as:",
                    "citations": []
                },
                {
                    "text": "and A † is the Moore-Penrose (a generalized) inverse of A.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "Sampling",
            "content": [
                "In the standard Nyström formulation, A and B are sub-matrices of S obtained by randomly sampled m tokens, denoted as Q. We call the sampled Q as bottleneck tokens. However, Algorithm 1: SOFT: Softmax-free attention",
                "we find empirically that random sampling is considerably sensitive to the choice of m. We hence explore two additional options by leveraging the structural prior of visual data: (1) Using one convolutional layer with kernel size k and stride k to learn Q, and (2) Using average pooling with kernel size k and stride k to generate Q. For both, we need to reshape Q to the form of R H×W ×de . Each slide of convolution or pooling produces a token. We set k according to the length of Q such that m tokens can be obtained. Our experiments show that a convolution layer performs better in accuracy. We therefore use a convolution layer by default.",
                "As K is identical to Q, we have K = Q. Given these m tokens, we then compute A and P as:",
                "We finally obtain the regularized self-attention matrix Ŝ of SOFT as:",
                "leading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the attention score between any two tokens, we first correlate each of them with sampled tokens using our self-attention function (Eq (5)); With this correlation representation we then compute their similarity under the modulation of the generalized inverse of Q's correlation matrix. Similar as standard Nyström, our design associates the input tokens w.r.t. a small space spanned by sampled tokens, giving a proper estimation of the original attention relationships subject to a low-rank constraint. The correctness of this method is proved in Appendix A.1.",
                "Moore-Penrose inverse An accurate and commonly used way to calculate the Moore-Penrose inverse is to use Singular Value Decomposition (SVD). Given A ∈ R m×m and its SVD form A = U ΣV where U, V are m × m unitary matrices and Σ is a m × m diagonal matrix, the Moore-Penrose inverse of A is A † = V Σ † U . Nevertheless, SVD is not friendly to the training process on GPU hence harming the model training efficiency. To solve this issue, we adopt the Newton-Raphson method. It is an iterative algorithm with the (k + 1)-th iteration formulated given the previous iteration as:",
                "We now prove that A k finally converges to Moore-Penrose inverse of A m×m , if α is sufficiently small [3].",
                "Though α = 2/ A 2 1 which ensures good convergence behavior in Algorithm 2 (see more details in Appendix A.2.1), in practice, we find that using an alternative form gives more stable training and faster convergence. Specifically, in",
                "1 ≤ 1 where β equals to 0.5, we find the smallest n i that holds this inequality. Then, we initialize α as α =",
                "The following proposition comes with the proof of Theorem 1:",
                "The detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is sufficiently accurate for matrix decomposition, subject to that our SOFT attention is regularized.",
                "As we keep m (m n) a fixed constant in our model, both time and space complexity are O(n), making SOFT a linear self-attention."
            ],
            "paragraphs": [
                {
                    "text": "In the standard Nyström formulation, A and B are sub-matrices of S obtained by randomly sampled m tokens, denoted as Q. We call the sampled Q as bottleneck tokens. However, Algorithm 1: SOFT: Softmax-free attention",
                    "citations": []
                },
                {
                    "text": "we find empirically that random sampling is considerably sensitive to the choice of m. We hence explore two additional options by leveraging the structural prior of visual data: (1) Using one convolutional layer with kernel size k and stride k to learn Q, and (2) Using average pooling with kernel size k and stride k to generate Q. For both, we need to reshape Q to the form of R H×W ×de . Each slide of convolution or pooling produces a token. We set k according to the length of Q such that m tokens can be obtained. Our experiments show that a convolution layer performs better in accuracy. We therefore use a convolution layer by default.",
                    "citations": []
                },
                {
                    "text": "As K is identical to Q, we have K = Q. Given these m tokens, we then compute A and P as:",
                    "citations": []
                },
                {
                    "text": "We finally obtain the regularized self-attention matrix Ŝ of SOFT as:",
                    "citations": []
                },
                {
                    "text": "leading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the attention score between any two tokens, we first correlate each of them with sampled tokens using our self-attention function (Eq (5)); With this correlation representation we then compute their similarity under the modulation of the generalized inverse of Q's correlation matrix. Similar as standard Nyström, our design associates the input tokens w.r.t. a small space spanned by sampled tokens, giving a proper estimation of the original attention relationships subject to a low-rank constraint. The correctness of this method is proved in Appendix A.1.",
                    "citations": []
                },
                {
                    "text": "Moore-Penrose inverse An accurate and commonly used way to calculate the Moore-Penrose inverse is to use Singular Value Decomposition (SVD). Given A ∈ R m×m and its SVD form A = U ΣV where U, V are m × m unitary matrices and Σ is a m × m diagonal matrix, the Moore-Penrose inverse of A is A † = V Σ † U . Nevertheless, SVD is not friendly to the training process on GPU hence harming the model training efficiency. To solve this issue, we adopt the Newton-Raphson method. It is an iterative algorithm with the (k + 1)-th iteration formulated given the previous iteration as:",
                    "citations": []
                },
                {
                    "text": "We now prove that A k finally converges to Moore-Penrose inverse of A m×m , if α is sufficiently small [3].",
                    "citations": [
                        {
                            "target": "b2",
                            "text": "[3]"
                        }
                    ]
                },
                {
                    "text": "Though α = 2/ A 2 1 which ensures good convergence behavior in Algorithm 2 (see more details in Appendix A.2.1), in practice, we find that using an alternative form gives more stable training and faster convergence. Specifically, in",
                    "citations": []
                },
                {
                    "text": "1 ≤ 1 where β equals to 0.5, we find the smallest n i that holds this inequality. Then, we initialize α as α =",
                    "citations": []
                },
                {
                    "text": "The following proposition comes with the proof of Theorem 1:",
                    "citations": []
                },
                {
                    "text": "The detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is sufficiently accurate for matrix decomposition, subject to that our SOFT attention is regularized.",
                    "citations": []
                },
                {
                    "text": "As we keep m (m n) a fixed constant in our model, both time and space complexity are O(n), making SOFT a linear self-attention.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.3",
            "title": "Instantiations",
            "content": [
                "Figure 2 shows how our proposed softmax-free self-attention block (SOFT block) can be implemented in a neural network. We replace the self-attention block with our SOFT block in the traditional Transformer, that is, we stack a SOFT block with a feed forward residual block [11] to form a softmax-free Transformer layer (SOFT layer).",
                "Focusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyramidal Transformer architecture [36] to form our final model SOFT. Further, several improvements are introduced in patch embedding (i.e., tokenization). Specifically, unlike [36] that uses a combination of non-overlapping convolution and layer normalization [1], we adopt a stack of overlapping convolutions, batch normalization [15] and ReLU non-linearity. Concretely, the STEM is implemented by 3 units of 3x3 Conv→BN→ReLU, with the stride of 2, 1, 2 respectively. Then, one such unit is applied to each of three following down-sampling operations with stride of 2 in the multi-stage architecture.",
                "The architecture hyper-parameters of SOFT are: d: the input channel dimension of SOFT layer. d e : the embedding dimension of tokens in SOFT block. In practice, we set d e = d. h: the head number of SOFT block. d h : the channel dimension of each head and d h = d e /h. n: the input token sequence length of a SOFT block. m: the bottleneck token sequence length of SOFT block. sp: the sampling ratio of token sequence length sampling, which is the ratio between input token sequence length and the bottleneck token sequence length. e: the expansion ratio of the 2-layer feed forward block. In SOFT, for all the stages we set d h = 32, e = 4 and m = 49, sp varies in each stage according to the input token sequence length. Table 2 details the family of our SOFT configurations with varying capacities (depth and width)."
            ],
            "paragraphs": [
                {
                    "text": "Figure 2 shows how our proposed softmax-free self-attention block (SOFT block) can be implemented in a neural network. We replace the self-attention block with our SOFT block in the traditional Transformer, that is, we stack a SOFT block with a feed forward residual block [11] to form a softmax-free Transformer layer (SOFT layer).",
                    "citations": [
                        {
                            "target": "b10",
                            "text": "[11]"
                        }
                    ]
                },
                {
                    "text": "Focusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyramidal Transformer architecture [36] to form our final model SOFT. Further, several improvements are introduced in patch embedding (i.e., tokenization). Specifically, unlike [36] that uses a combination of non-overlapping convolution and layer normalization [1], we adopt a stack of overlapping convolutions, batch normalization [15] and ReLU non-linearity. Concretely, the STEM is implemented by 3 units of 3x3 Conv→BN→ReLU, with the stride of 2, 1, 2 respectively. Then, one such unit is applied to each of three following down-sampling operations with stride of 2 in the multi-stage architecture.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b0",
                            "text": "[1]"
                        },
                        {
                            "target": "b14",
                            "text": "[15]"
                        }
                    ]
                },
                {
                    "text": "The architecture hyper-parameters of SOFT are: d: the input channel dimension of SOFT layer. d e : the embedding dimension of tokens in SOFT block. In practice, we set d e = d. h: the head number of SOFT block. d h : the channel dimension of each head and d h = d e /h. n: the input token sequence length of a SOFT block. m: the bottleneck token sequence length of SOFT block. sp: the sampling ratio of token sequence length sampling, which is the ratio between input token sequence length and the bottleneck token sequence length. e: the expansion ratio of the 2-layer feed forward block. In SOFT, for all the stages we set d h = 32, e = 4 and m = 49, sp varies in each stage according to the input token sequence length. Table 2 details the family of our SOFT configurations with varying capacities (depth and width).",
                    "citations": []
                }
            ]
        },
        {
            "n": "4",
            "title": "Experiments",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "4.1",
            "title": "Setup",
            "content": [
                "Dataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset [9] with 1.28M training images and 50K validation images from 1,000 classes. Following the common practice, we train a model on the training set and evaluate on the validation set. Metrics: For model performance, the top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the model size and floating point operations (i.e., FLOPs). Implementation details: We use the code base [38] with the default setting to train and test all the models. Specifically, we use weight decay of 0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an AdamW optimizer and decreasing learning rate with the cosine annealing schedule. During training, random flipping, mixup [44] and cutmix [43] are adopted for data augmentation. Label smoothing [29] is used for loss calculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We also implement our method using the Mindspore [23]."
            ],
            "paragraphs": [
                {
                    "text": "Dataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset [9] with 1.28M training images and 50K validation images from 1,000 classes. Following the common practice, we train a model on the training set and evaluate on the validation set. Metrics: For model performance, the top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the model size and floating point operations (i.e., FLOPs). Implementation details: We use the code base [38] with the default setting to train and test all the models. Specifically, we use weight decay of 0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an AdamW optimizer and decreasing learning rate with the cosine annealing schedule. During training, random flipping, mixup [44] and cutmix [43] are adopted for data augmentation. Label smoothing [29] is used for loss calculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We also implement our method using the Mindspore [23].",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "[9]"
                        },
                        {
                            "target": "b37",
                            "text": "[38]"
                        },
                        {
                            "target": "b44",
                            "text": "[44]"
                        },
                        {
                            "target": "b43",
                            "text": "[43]"
                        },
                        {
                            "target": "b28",
                            "text": "[29]"
                        },
                        {
                            "target": "b22",
                            "text": "[23]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.2",
            "title": "Comparison with existing linear Transformers",
            "content": [
                "We compare our method with three existing linear Transformer models: Linformer [35], Performer [5], Nyströmformer [40] in terms of model complexity and accuracy.",
                "Two experimental settings are adopted. Under the first setting, for all methods we use the same Tiny (Table 2) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT with each baseline's own attention block with the rest of the architecture unchanged. Note that the spatial reduction module of [36] is a special case of Linformer [35]. We set the reduction ratio to be identical to ours. With the same uniform sampling idea, we replace the 1D window averaging of Nyströmformer [40] (for NLP tasks) with 2D average pooling (for images). The downsampling ratio remains identical to ours. It is also worth mentioning that there is no official code released for Reformer [19] and the local Sensitive Hash (LSH) module has strict requirements on the length of input tokens. We thus do not include this method in our comparison.",
                "From Table 1 we can make the following observations: (i) Linear Transformer methods substantially reduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer on the Tiny architecture; (ii) Our approach SOFT achieves the best classification accuracy among all the linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers and our training speed is slightly slower than Nystromformer and both are slower than Performer and Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson iteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse. In summary, due to the on-par inference speed we consider the training cost increase is a price worth paying for our superior accuracy.",
                "Under the second setting, we focus on the memory efficiency of SOFT against the baselines. Here we follow the ViT [11] network structure, stacking 12 attention layers with hidden dimension d = 384, heads h = 12, bottleneck token sequence length m = 49. Different attention blocks from the three linearized Transformer variants, Linformer [35], Performer [5], and Nyströmformer [40] are studied.",
                "For each Transformer variant, we adjust its token sequence length n in a linear increment. Specifically, we use a token sequence length of 784 × p where p = 1, 2, 3, 4, 5, 6, 7, 8 and set batch size 1 to verify whether the memory consumption increases \"quadratically\" or \"linearly\". Figure 1b shows all compared transformer variants including our SOFT indeed have a linear memory usage complexity. This is in contrast with the standard Transformer which cannot cope with long token sequences with a quadratic complexity. Table 3: Evaluation results on ILSVRC-2012 ImageNet-1K [9] validation set. We report the results using the input size of 224x224 pixels center cropped from resized images with 256x256 pixels. M.S.Out. stands for whether the model is designed for multi-scale output. †: Corrected FLOPs by taking into account the cost of attention matrix multiplication overlooked in the origin paper."
            ],
            "paragraphs": [
                {
                    "text": "We compare our method with three existing linear Transformer models: Linformer [35], Performer [5], Nyströmformer [40] in terms of model complexity and accuracy.",
                    "citations": [
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b4",
                            "text": "[5]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "Two experimental settings are adopted. Under the first setting, for all methods we use the same Tiny (Table 2) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT with each baseline's own attention block with the rest of the architecture unchanged. Note that the spatial reduction module of [36] is a special case of Linformer [35]. We set the reduction ratio to be identical to ours. With the same uniform sampling idea, we replace the 1D window averaging of Nyströmformer [40] (for NLP tasks) with 2D average pooling (for images). The downsampling ratio remains identical to ours. It is also worth mentioning that there is no official code released for Reformer [19] and the local Sensitive Hash (LSH) module has strict requirements on the length of input tokens. We thus do not include this method in our comparison.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b18",
                            "text": "[19]"
                        }
                    ]
                },
                {
                    "text": "From Table 1 we can make the following observations: (i) Linear Transformer methods substantially reduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer on the Tiny architecture; (ii) Our approach SOFT achieves the best classification accuracy among all the linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers and our training speed is slightly slower than Nystromformer and both are slower than Performer and Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson iteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse. In summary, due to the on-par inference speed we consider the training cost increase is a price worth paying for our superior accuracy.",
                    "citations": []
                },
                {
                    "text": "Under the second setting, we focus on the memory efficiency of SOFT against the baselines. Here we follow the ViT [11] network structure, stacking 12 attention layers with hidden dimension d = 384, heads h = 12, bottleneck token sequence length m = 49. Different attention blocks from the three linearized Transformer variants, Linformer [35], Performer [5], and Nyströmformer [40] are studied.",
                    "citations": [
                        {
                            "target": "b10",
                            "text": "[11]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b4",
                            "text": "[5]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        }
                    ]
                },
                {
                    "text": "For each Transformer variant, we adjust its token sequence length n in a linear increment. Specifically, we use a token sequence length of 784 × p where p = 1, 2, 3, 4, 5, 6, 7, 8 and set batch size 1 to verify whether the memory consumption increases \"quadratically\" or \"linearly\". Figure 1b shows all compared transformer variants including our SOFT indeed have a linear memory usage complexity. This is in contrast with the standard Transformer which cannot cope with long token sequences with a quadratic complexity. Table 3: Evaluation results on ILSVRC-2012 ImageNet-1K [9] validation set. We report the results using the input size of 224x224 pixels center cropped from resized images with 256x256 pixels. M.S.Out. stands for whether the model is designed for multi-scale output. †: Corrected FLOPs by taking into account the cost of attention matrix multiplication overlooked in the origin paper.",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "[9]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.3",
            "title": "Comparison with state-of-the-art CNNs and ViTs",
            "content": [
                "We compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K validation set. FLOPs are calculated at batch size 1. From Figure 1a and Table 3, the following observations are made: (i) Overall, ViT and its variants yield better classification accuracy over CNNs.",
                "(ii) We achieve the best performance among the recent pure vision Transformer based methods including ViT [11] and DeiT [32], as well as the state-of-the-art CNN RegNet [27]. (iii) Our SOFT outperforms the most similar (in architecture configuration) Transformer counterparts PVT [36] at all variants. Since the attention module is the main difference, this validates directly the effectiveness of our model. (iv) We can also beat the latest ViT variants Twins [6] which is designed to address the efficiency limitation of ViT. We have done so with less parameters and fewer float point computation.",
                "To gain some insights into how attention is learned using our SOFT and the alternatives, Figure 3 shows the attention masks of various compared models. For each model, we show the output from the first two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local and long distance relations among pixels. It is interesting to note that, although SOFT is trained on an object categorization dataset in ImageNet [9], it seems to be able to learn both semantic concepts shared across instances in the same category and instance specific features. For instance, in the bottom-right example of a bird class, one attention head focuses on the black bird only, while the other attend to both birds in the image. More examples are shown in Appendix A.4."
            ],
            "paragraphs": [
                {
                    "text": "We compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K validation set. FLOPs are calculated at batch size 1. From Figure 1a and Table 3, the following observations are made: (i) Overall, ViT and its variants yield better classification accuracy over CNNs.",
                    "citations": []
                },
                {
                    "text": "(ii) We achieve the best performance among the recent pure vision Transformer based methods including ViT [11] and DeiT [32], as well as the state-of-the-art CNN RegNet [27]. (iii) Our SOFT outperforms the most similar (in architecture configuration) Transformer counterparts PVT [36] at all variants. Since the attention module is the main difference, this validates directly the effectiveness of our model. (iv) We can also beat the latest ViT variants Twins [6] which is designed to address the efficiency limitation of ViT. We have done so with less parameters and fewer float point computation.",
                    "citations": [
                        {
                            "target": "b10",
                            "text": "[11]"
                        },
                        {
                            "target": "b31",
                            "text": "[32]"
                        },
                        {
                            "target": "b26",
                            "text": "[27]"
                        },
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b5",
                            "text": "[6]"
                        }
                    ]
                },
                {
                    "text": "To gain some insights into how attention is learned using our SOFT and the alternatives, Figure 3 shows the attention masks of various compared models. For each model, we show the output from the first two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local and long distance relations among pixels. It is interesting to note that, although SOFT is trained on an object categorization dataset in ImageNet [9], it seems to be able to learn both semantic concepts shared across instances in the same category and instance specific features. For instance, in the bottom-right example of a bird class, one attention head focuses on the black bird only, while the other attend to both birds in the image. More examples are shown in Appendix A.4.",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "[9]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.4",
            "title": "Ablation studies",
            "content": [
                "Pyramidal architecture: Unlike the earlier non-pyramidal vision Transformers (e.g., ViT [11]), most recent pyramidal (multi-scale) Transformers (e.g., PVT [36]) use convolution layers to reduce the spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT with a pyramidal architecture (our default SOFT-Small), SOFT w/o a pyramidal architecture and DeiT-S [32] (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer to get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs. Table 5a shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal design, and our non-pyramidal counterpart is even slightly better than DeiT-S [32] whilst enjoying linear complexity. Bottleneck token sequence length: In this study, we examine how the bottleneck token sequence length m, sampled from n tokens, influences the model's performance. We change the bottleneck token sequence length in all stages to 36, 49, 64, 81. Table 4a shows that longer bottleneck token would increase the memory cost and the computational overhead. m = 49 seems to give the best trade-off between the performance and computational overhead. The memory usage is measured with the batch size of 128.",
                "Token sampling: The sampling function in SOFT can assume different forms. Convolution: The sequence Q ∈ R n×de is first reshaped to a feature map R H×W ×de . r × r convolution kernel with stride of r is applied for downsampling, where r = √ sp. The output channel size is also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average pooling: using a r × r kernel and r stride, where r = √ sp. Random sampling: m tokens are randomly picked from n tokens. Biased sampling: We pick m tokens with a biased policy. Here, the first m tokens are picked. Table 4b shows that average pooling yields the best performance while with less computational overhead comparing to convolution. Biased sampling can miss the most salient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen samples. This result thus justifies the choice of using average pooling in SOFT. Newton-Raphson's convergence: We study how many iterations the Newton-Raphson method needs to converge when computing the Moore-Penrose inverse A † . We use AA k A -A p / A p with p = 2 (see Proposition 1) as the convergence metric to quantify the difference between A k and A † . Figure 4 shows that our approximation converges within 20 iterations across all stages."
            ],
            "paragraphs": [
                {
                    "text": "Pyramidal architecture: Unlike the earlier non-pyramidal vision Transformers (e.g., ViT [11]), most recent pyramidal (multi-scale) Transformers (e.g., PVT [36]) use convolution layers to reduce the spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT with a pyramidal architecture (our default SOFT-Small), SOFT w/o a pyramidal architecture and DeiT-S [32] (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer to get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs. Table 5a shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal design, and our non-pyramidal counterpart is even slightly better than DeiT-S [32] whilst enjoying linear complexity. Bottleneck token sequence length: In this study, we examine how the bottleneck token sequence length m, sampled from n tokens, influences the model's performance. We change the bottleneck token sequence length in all stages to 36, 49, 64, 81. Table 4a shows that longer bottleneck token would increase the memory cost and the computational overhead. m = 49 seems to give the best trade-off between the performance and computational overhead. The memory usage is measured with the batch size of 128.",
                    "citations": [
                        {
                            "target": "b10",
                            "text": "[11]"
                        },
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b31",
                            "text": "[32]"
                        },
                        {
                            "target": "b31",
                            "text": "[32]"
                        }
                    ]
                },
                {
                    "text": "Token sampling: The sampling function in SOFT can assume different forms. Convolution: The sequence Q ∈ R n×de is first reshaped to a feature map R H×W ×de . r × r convolution kernel with stride of r is applied for downsampling, where r = √ sp. The output channel size is also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average pooling: using a r × r kernel and r stride, where r = √ sp. Random sampling: m tokens are randomly picked from n tokens. Biased sampling: We pick m tokens with a biased policy. Here, the first m tokens are picked. Table 4b shows that average pooling yields the best performance while with less computational overhead comparing to convolution. Biased sampling can miss the most salient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen samples. This result thus justifies the choice of using average pooling in SOFT. Newton-Raphson's convergence: We study how many iterations the Newton-Raphson method needs to converge when computing the Moore-Penrose inverse A † . We use AA k A -A p / A p with p = 2 (see Proposition 1) as the convergence metric to quantify the difference between A k and A † . Figure 4 shows that our approximation converges within 20 iterations across all stages.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "Bottleneck Memory FLOPs",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "4.5",
            "title": "Additional experiments on NLP tasks",
            "content": [
                "In this section, we evaluate our method against other linear counterparts on four tasks of the Long Range Arena (LRA) [30] benchmark e.g., Listops [24], byte-level IMDb reviews text classification [22], byte-level document retrieval [26], and image classification on sequences of pixels [20].",
                "Implementations. We use the Pytorch version of LRA [30] benchmark, provided by [40]. For the evaluation protocol, we strictly follow [30,40]. We omit the Pathfinder(1K) task as we cannot replicate the result of Nyströmformer [40]. For our SOFT, we simply use the average pooling with window size 4, stride 4 to sample the bottlenecks. We follow the configurations of [40], with 2 layers, 64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table 6 shows that our SOFT outperforms both the standard and alternative efficient Transformers on three out of four tasks, as well as the average result.   [30], based on its official configuration. Our SOFT surpasses previous efficient methods on three tasks."
            ],
            "paragraphs": [
                {
                    "text": "In this section, we evaluate our method against other linear counterparts on four tasks of the Long Range Arena (LRA) [30] benchmark e.g., Listops [24], byte-level IMDb reviews text classification [22], byte-level document retrieval [26], and image classification on sequences of pixels [20].",
                    "citations": [
                        {
                            "target": "b29",
                            "text": "[30]"
                        },
                        {
                            "target": "b23",
                            "text": "[24]"
                        },
                        {
                            "target": "b21",
                            "text": "[22]"
                        },
                        {
                            "target": "b25",
                            "text": "[26]"
                        },
                        {
                            "target": "b19",
                            "text": "[20]"
                        }
                    ]
                },
                {
                    "text": "Implementations. We use the Pytorch version of LRA [30] benchmark, provided by [40]. For the evaluation protocol, we strictly follow [30,40]. We omit the Pathfinder(1K) task as we cannot replicate the result of Nyströmformer [40]. For our SOFT, we simply use the average pooling with window size 4, stride 4 to sample the bottlenecks. We follow the configurations of [40], with 2 layers, 64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table 6 shows that our SOFT outperforms both the standard and alternative efficient Transformers on three out of four tasks, as well as the average result.   [30], based on its official configuration. Our SOFT surpasses previous efficient methods on three tasks.",
                    "citations": [
                        {
                            "target": "b29",
                            "text": "[30]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b29",
                            "text": "[30,"
                        },
                        {
                            "target": "b39",
                            "text": "40]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b29",
                            "text": "[30]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "5",
            "title": "Conclusions",
            "content": [
                "We have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Transformer's complexity in space and time. Unlike existing linear Transformers that aim to approximate the conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention which eliminates the need for softmax normalization. This design enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments show that SOFT yields superior trade-off in accuracy and complexity."
            ],
            "paragraphs": [
                {
                    "text": "We have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Transformer's complexity in space and time. Unlike existing linear Transformers that aim to approximate the conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention which eliminates the need for softmax normalization. This design enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments show that SOFT yields superior trade-off in accuracy and complexity.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "A Appendix",
            "content": [
                "A.1 Nyström method",
                "Nyström method [39] aims to calculate a low-rank approximation for a Gram matrix. For Transformers, the self-attention matrix can be viewed as a Gram matrix S with a Gaussian kernel k applied to the query Q, with each element S ij expressed as:",
                "k(x, y) means operating Gaussian kernel k to (x, y), which can be written in the feature space as:",
                "n is the dimension of a feature space, λ i denotes the eigenvalue and φ i denotes the eigenfunction of kernel k. According to the eigenfunction's definition, we can get:",
                "where p(x) is the probability distribution of x. And {φ i (x)} are p-orthogonal:",
                "δ ij is 0 when i = j, 1 when i = j. To get an approximation of the eigenfunctions, we sample {x 1 , x 2 , • • • , x q } from p(x), then:",
                "This inspires us to approximate the Gram matrix S. Let S (m) be a submatrix of S, consisting of m × m elements from S. Gram matrix is a symmetric positive semi-definite matrix, so it has a spectral decomposition:",
                "where U (m) is column orthogonal and Λ (m) is a diagonal matrix with the diagonal elements as the eigenvalues of S (m) . Substituting the y to x j and applying the approximation above to S, we can get:",
                "is the eigenvalue of S (m) . Denote S as the rank-m approximation of S and Ũ , Λ as the approximation for spectral decomposition of S. Now we can get an approximation of S with rank m:",
                "Similarly, we have:",
                "Thus",
                "Then we get an approximation of S: S ≈ S n,m S † m,m S m,n . S has a block representation below:",
                "A.2 Newton method A.2.1 Proof of theorem 1 Proof A.1 A is a symmetric positive semi-definite matrix and",
                "A 0 is chosen to be αA, so the A k can be written as A k = C k A = AD k for some matrix C k , D k , leading to the fact that",
                "This is because A k+1 = A k (2I n -AA k ) = (2I n -A k A)A k and A 0 = αA. We make a difference between A † and A k+1 :",
                "We norm both sides of the equation above:",
                "And we left multiply A on the both sides of (26), then norm the equation:",
                "We choose α sufficiently small so that the initial value satisfy AA † -AA 0 < 1. We set α = 2",
                "to ensure it is small enough [3]. Then the AA † -AA k → 0, when k → ∞. The inequality (27) implies that A k → A † , k → ∞."
            ],
            "paragraphs": [
                {
                    "text": "A.1 Nyström method",
                    "citations": []
                },
                {
                    "text": "Nyström method [39] aims to calculate a low-rank approximation for a Gram matrix. For Transformers, the self-attention matrix can be viewed as a Gram matrix S with a Gaussian kernel k applied to the query Q, with each element S ij expressed as:",
                    "citations": [
                        {
                            "target": "b38",
                            "text": "[39]"
                        }
                    ]
                },
                {
                    "text": "k(x, y) means operating Gaussian kernel k to (x, y), which can be written in the feature space as:",
                    "citations": []
                },
                {
                    "text": "n is the dimension of a feature space, λ i denotes the eigenvalue and φ i denotes the eigenfunction of kernel k. According to the eigenfunction's definition, we can get:",
                    "citations": []
                },
                {
                    "text": "where p(x) is the probability distribution of x. And {φ i (x)} are p-orthogonal:",
                    "citations": []
                },
                {
                    "text": "δ ij is 0 when i = j, 1 when i = j. To get an approximation of the eigenfunctions, we sample {x 1 , x 2 , • • • , x q } from p(x), then:",
                    "citations": []
                },
                {
                    "text": "This inspires us to approximate the Gram matrix S. Let S (m) be a submatrix of S, consisting of m × m elements from S. Gram matrix is a symmetric positive semi-definite matrix, so it has a spectral decomposition:",
                    "citations": []
                },
                {
                    "text": "where U (m) is column orthogonal and Λ (m) is a diagonal matrix with the diagonal elements as the eigenvalues of S (m) . Substituting the y to x j and applying the approximation above to S, we can get:",
                    "citations": []
                },
                {
                    "text": "is the eigenvalue of S (m) . Denote S as the rank-m approximation of S and Ũ , Λ as the approximation for spectral decomposition of S. Now we can get an approximation of S with rank m:",
                    "citations": []
                },
                {
                    "text": "Similarly, we have:",
                    "citations": []
                },
                {
                    "text": "Thus",
                    "citations": []
                },
                {
                    "text": "Then we get an approximation of S: S ≈ S n,m S † m,m S m,n . S has a block representation below:",
                    "citations": []
                },
                {
                    "text": "A.2 Newton method A.2.1 Proof of theorem 1 Proof A.1 A is a symmetric positive semi-definite matrix and",
                    "citations": []
                },
                {
                    "text": "A 0 is chosen to be αA, so the A k can be written as A k = C k A = AD k for some matrix C k , D k , leading to the fact that",
                    "citations": []
                },
                {
                    "text": "This is because A k+1 = A k (2I n -AA k ) = (2I n -A k A)A k and A 0 = αA. We make a difference between A † and A k+1 :",
                    "citations": []
                },
                {
                    "text": "We norm both sides of the equation above:",
                    "citations": []
                },
                {
                    "text": "And we left multiply A on the both sides of (26), then norm the equation:",
                    "citations": []
                },
                {
                    "text": "We choose α sufficiently small so that the initial value satisfy AA † -AA 0 < 1. We set α = 2",
                    "citations": []
                },
                {
                    "text": "to ensure it is small enough [3]. Then the AA † -AA k → 0, when k → ∞. The inequality (27) implies that A k → A † , k → ∞.",
                    "citations": [
                        {
                            "target": "b2",
                            "text": "[3]"
                        },
                        {
                            "target": "b26",
                            "text": "(27)"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "A.2.2 Proof of proposition 1",
            "content": [
                "Proof A.2 Note that when we multiply A on both sides of (26), the equation turns to be:",
                "Similarly norm both sides of (29), considering that AA † -AA k → 0 and AA † -AA k < 1 always holds, A-AA k A monotonically decreases to 0. The inequality (27) implies that A k -A † decreases to 0 monotonously .",
                "Note that although A -AA k A monotonically decreases to 0, A k AA k -A k cannot be proved to monotonically decrease to 0."
            ],
            "paragraphs": [
                {
                    "text": "Proof A.2 Note that when we multiply A on both sides of (26), the equation turns to be:",
                    "citations": []
                },
                {
                    "text": "Similarly norm both sides of (29), considering that AA † -AA k → 0 and AA † -AA k < 1 always holds, A-AA k A monotonically decreases to 0. The inequality (27) implies that A k -A † decreases to 0 monotonously .",
                    "citations": [
                        {
                            "target": "b26",
                            "text": "(27)"
                        }
                    ]
                },
                {
                    "text": "Note that although A -AA k A monotonically decreases to 0, A k AA k -A k cannot be proved to monotonically decrease to 0.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "A.3 Non-linearized gaussian kernel attention",
            "content": [
                "In our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated. More specifically, the relation between any two tokens is reconstructed via sampled bottleneck tokens. As the number m (e.g., 49), of bottleneck tokens is much smaller than the token sequence length, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now focuses the attentive learning on latent salient information captured by the m bottleneck tokens. (II)",
                "The model becomes more robust against the underlying token noise due to the auto-encoder style reconstruction [13].",
                "This explains why the model with an approximated gram matrix performs better than the one with a directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training difficulties. We first hypothesized that this might be due to lacking normalization (as normalization often helps with training stability and convergence), and tested a variant with softmax on top of an exact Gaussian kernel attention matrix. However, it turns out to suffer from a similar failure. We cannot find a solid hypothesis so far and will keep investigate this problem."
            ],
            "paragraphs": [
                {
                    "text": "In our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated. More specifically, the relation between any two tokens is reconstructed via sampled bottleneck tokens. As the number m (e.g., 49), of bottleneck tokens is much smaller than the token sequence length, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now focuses the attentive learning on latent salient information captured by the m bottleneck tokens. (II)",
                    "citations": []
                },
                {
                    "text": "The model becomes more robust against the underlying token noise due to the auto-encoder style reconstruction [13].",
                    "citations": [
                        {
                            "target": "b12",
                            "text": "[13]"
                        }
                    ]
                },
                {
                    "text": "This explains why the model with an approximated gram matrix performs better than the one with a directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training difficulties. We first hypothesized that this might be due to lacking normalization (as normalization often helps with training stability and convergence), and tested a variant with softmax on top of an exact Gaussian kernel attention matrix. However, it turns out to suffer from a similar failure. We cannot find a solid hypothesis so far and will keep investigate this problem.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "A.4 Attention visualization",
            "content": [
                "Figure 5 shows more visualization of the attention masks by various Transformers [34,5,40] and our SOFT. For each model, we show the output from the first two attention heads (up and down row). It is noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other methods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to more precisely capture all these objects."
            ],
            "paragraphs": [
                {
                    "text": "Figure 5 shows more visualization of the attention masks by various Transformers [34,5,40] and our SOFT. For each model, we show the output from the first two attention heads (up and down row). It is noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other methods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to more precisely capture all these objects.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34,"
                        },
                        {
                            "target": "b4",
                            "text": "5,"
                        },
                        {
                            "target": "b39",
                            "text": "40]"
                        }
                    ]
                }
            ]
        }
    ],
    "references": []
}