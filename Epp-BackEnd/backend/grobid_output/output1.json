{
    "metadata": {
        "title": "Location-Sensitive Visual Recognition with Cross-IOU Loss",
        "authors": [
            {
                "name": "Kaiwen Duan",
                "affiliations": [
                    {
                        "name": "University of Chinese Academy of Sciences",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Lingxi Xie",
                "affiliations": [
                    {
                        "name": "Huawei Inc",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Honggang Qi",
                "affiliations": [
                    {
                        "name": "University of Chinese Academy of Sciences",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Song Bai",
                "affiliations": [
                    {
                        "name": "Huazhong University of Science and Technology",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Qingming Huang",
                "affiliations": [
                    {
                        "name": "University of Chinese Academy of Sciences",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "Qi Tian",
                "affiliations": [
                    {
                        "name": "Huawei Inc",
                        "type": "institution"
                    }
                ]
            }
        ],
        "abstract": "Object detection, instance segmentation, and pose estimation are popular visual recognition tasks which require localizing the object by internal or boundary landmarks. This paper summarizes these tasks as locationsensitive visual recognition and proposes a unified solution named location-sensitive network (LSNet). Based on a deep neural network as the backbone, LSNet predicts an anchor point and a set of landmarks which together define the shape of the target object. The key to optimizing the LSNet lies in the ability of fitting various scales, for which we design a novel loss function named cross-IOU loss that computes the cross-IOU of each anchor-landmark pair to approximate the global IOU between the prediction and ground-truth. The flexibly located and accurately predicted landmarks also enable LSNet to incorporate richer contextual information for visual recognition. Evaluated on the MS-COCO dataset, LSNet set the new state-of-theart accuracy for anchor-free object detection (a 53.5% box AP) and instance segmentation (a 40.2% mask AP), and shows promising performance in detecting multi-scale human poses. Code is available at"
    },
    "sections": [
        {
            "n": "1.",
            "title": "Introduction",
            "content": [
                "Object recognition is a fundamental task in computer vision. Beyond image classification [16] that depicts an image using a single semantic label, there exist other recognition tasks that not only predict the class of the object but also localize it using fine-scaled information. In this paper, we consider three popular examples including object detection [19,36], instance segmentation [19,13], and human pose estimation [1,36]. We notice that, despite the fact that the rapid progress of deep learning [32] has introduced"
            ],
            "paragraphs": [
                {
                    "text": "Object recognition is a fundamental task in computer vision. Beyond image classification [16] that depicts an image using a single semantic label, there exist other recognition tasks that not only predict the class of the object but also localize it using fine-scaled information. In this paper, we consider three popular examples including object detection [19,36], instance segmentation [19,13], and human pose estimation [1,36]. We notice that, despite the fact that the rapid progress of deep learning [32] has introduced",
                    "citations": [
                        {
                            "target": "b15",
                            "text": "[16]"
                        },
                        {
                            "target": "b18",
                            "text": "[19,"
                        },
                        {
                            "target": "b35",
                            "text": "36]"
                        },
                        {
                            "target": "b18",
                            "text": "[19,"
                        },
                        {
                            "target": "b12",
                            "text": "13]"
                        },
                        {
                            "target": "b0",
                            "text": "[1,"
                        },
                        {
                            "target": "b35",
                            "text": "36]"
                        },
                        {
                            "target": "b31",
                            "text": "[32]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "Object Detection Instance Segmentation Pose Estimation",
            "content": [
                "Figure 1: The location-sensitive visual recognition tasks, including object detection, instance segmentation, and human pose estimation, can be formulated into localizing an anchor point (in red) and a set of landmarks (in green). Our work aims to offer a unified framework for these tasks. powerful deep networks as the backbone [25,60,20], the designs of the head module for detection [38,48,49,52,65,31,30,63,6], segmentation [59,24,44,7], and pose estimation [51,57,5,40,66] have fallen into individual subfields. This is mainly due to the difference in the prediction target, i.e., a bounding box for detection, a pixel-level mask for segmentation, and a set of keypoints for pose estimation, respectively.",
                "Going one step further, we merge the aforementioned three tasks into one, named the location-sensitive visual recognition (LSVR). On the basis of the definition, we propose a location-sensitive network (LSNet) as a unified formulation to deal with them all. The LSNet is built upon any network backbone, e.g., those designed for image classification. The key is to relate an object to an anchor point and a set of landmarks that accurately localize the object. In particular, the landmarks should correspond to the four extreme points for object detection, sufficiently dense boundary pixels for instance segmentation, and the keypoints for human pose estimation. Note that the anchor point as well arXiv:2104.04899v1 [cs.CV] 11 Apr 2021 as landmarks can be also used for extracting discriminative features of the object and thus assisting recognition. Figure 1 illustrates the overall idea.",
                "The major difficulty of optimizing the LSNet lies in the requirement of fitting objects of different scales and different properties, which existing methods including the smooth-1 loss and the IOU loss suffer cannot satisfy. This motivates us to design a novel loss function named the cross-IOU loss. It assumes that the landmarks are uniformly distributed around the anchor point and thus approximates the IOU (between the prediction and ground-truth) using the coordinate of the offset vectors. The cross-IOU loss is easily implemented in a few lines of codes. Compared to other loss functions, it achieves a better trade-off between the global and local properties and transplants easier to multi-scale feature maps without specific parameter tuning.",
                "We perform all three tasks on the MS-COCO dataset [36]. LSNet, equipped with the cross-IOU loss, achieves competitive recognition accuracy. We further equip the LSNet with a pyramid of deformable convolution that extracts discriminative visual cues around the landmarks. As a result, LSNet reports a 53.5% box AP and a 40.2% mask AP, both of which surpass all existing anchorfree methods. For human pose estimation, LSNet reports competitive results without using the heatmaps, offering a new possibility to the community. Moreover, LSNet shows a promising ability in detecting human poses in various scales, some of which were not annotated in the dataset.",
                "On top of these results, we claim two-fold contributions of this work. First, we present the formulation of locationsensitive visual recognition that inspires the community to consider the common property of these tasks. Second, we propose the LSNet as a unified framework in which the key technical contribution is the cross-IOU loss."
            ],
            "paragraphs": [
                {
                    "text": "Figure 1: The location-sensitive visual recognition tasks, including object detection, instance segmentation, and human pose estimation, can be formulated into localizing an anchor point (in red) and a set of landmarks (in green). Our work aims to offer a unified framework for these tasks. powerful deep networks as the backbone [25,60,20], the designs of the head module for detection [38,48,49,52,65,31,30,63,6], segmentation [59,24,44,7], and pose estimation [51,57,5,40,66] have fallen into individual subfields. This is mainly due to the difference in the prediction target, i.e., a bounding box for detection, a pixel-level mask for segmentation, and a set of keypoints for pose estimation, respectively.",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "[25,"
                        },
                        {
                            "target": "b59",
                            "text": "60,"
                        },
                        {
                            "target": "b19",
                            "text": "20]"
                        },
                        {
                            "target": "b37",
                            "text": "[38,"
                        },
                        {
                            "target": "b47",
                            "text": "48,"
                        },
                        {
                            "target": "b48",
                            "text": "49,"
                        },
                        {
                            "target": "b51",
                            "text": "52,"
                        },
                        {
                            "target": "b64",
                            "text": "65,"
                        },
                        {
                            "target": "b30",
                            "text": "31,"
                        },
                        {
                            "target": "b29",
                            "text": "30,"
                        },
                        {
                            "target": "b62",
                            "text": "63,"
                        },
                        {
                            "target": "b5",
                            "text": "6]"
                        },
                        {
                            "target": "b58",
                            "text": "[59,"
                        },
                        {
                            "target": "b23",
                            "text": "24,"
                        },
                        {
                            "target": "b43",
                            "text": "44,"
                        },
                        {
                            "target": "b6",
                            "text": "7]"
                        },
                        {
                            "target": "b50",
                            "text": "[51,"
                        },
                        {
                            "target": "b56",
                            "text": "57,"
                        },
                        {
                            "target": "b4",
                            "text": "5,"
                        },
                        {
                            "target": "b39",
                            "text": "40,"
                        },
                        {
                            "target": "b65",
                            "text": "66]"
                        }
                    ]
                },
                {
                    "text": "Going one step further, we merge the aforementioned three tasks into one, named the location-sensitive visual recognition (LSVR). On the basis of the definition, we propose a location-sensitive network (LSNet) as a unified formulation to deal with them all. The LSNet is built upon any network backbone, e.g., those designed for image classification. The key is to relate an object to an anchor point and a set of landmarks that accurately localize the object. In particular, the landmarks should correspond to the four extreme points for object detection, sufficiently dense boundary pixels for instance segmentation, and the keypoints for human pose estimation. Note that the anchor point as well arXiv:2104.04899v1 [cs.CV] 11 Apr 2021 as landmarks can be also used for extracting discriminative features of the object and thus assisting recognition. Figure 1 illustrates the overall idea.",
                    "citations": []
                },
                {
                    "text": "The major difficulty of optimizing the LSNet lies in the requirement of fitting objects of different scales and different properties, which existing methods including the smooth-1 loss and the IOU loss suffer cannot satisfy. This motivates us to design a novel loss function named the cross-IOU loss. It assumes that the landmarks are uniformly distributed around the anchor point and thus approximates the IOU (between the prediction and ground-truth) using the coordinate of the offset vectors. The cross-IOU loss is easily implemented in a few lines of codes. Compared to other loss functions, it achieves a better trade-off between the global and local properties and transplants easier to multi-scale feature maps without specific parameter tuning.",
                    "citations": []
                },
                {
                    "text": "We perform all three tasks on the MS-COCO dataset [36]. LSNet, equipped with the cross-IOU loss, achieves competitive recognition accuracy. We further equip the LSNet with a pyramid of deformable convolution that extracts discriminative visual cues around the landmarks. As a result, LSNet reports a 53.5% box AP and a 40.2% mask AP, both of which surpass all existing anchorfree methods. For human pose estimation, LSNet reports competitive results without using the heatmaps, offering a new possibility to the community. Moreover, LSNet shows a promising ability in detecting human poses in various scales, some of which were not annotated in the dataset.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        }
                    ]
                },
                {
                    "text": "On top of these results, we claim two-fold contributions of this work. First, we present the formulation of locationsensitive visual recognition that inspires the community to consider the common property of these tasks. Second, we propose the LSNet as a unified framework in which the key technical contribution is the cross-IOU loss.",
                    "citations": []
                }
            ]
        },
        {
            "n": "2.",
            "title": "Related Work",
            "content": [
                "Deep neural networks have been widely applied for visual recognition tasks. Among them, image classification [16] is the fundamental task that facilitates the design of powerful network backbones [25,60,20]. Beyond image-level description, there exist fine-scaled tasks, including object detection, instance segmentation, and pose estimation, which focus on depicting different aspects of the object. For example, the bounding boxes locate objects simply and efficiently but lack the details, while masks and keypoints reflect the shape and pose of the objects but usually need the bounding boxes to locate object firstly. According to the different properties of different tasks, many representative methods have been developed.",
                "The object detection methods can be roughly categorized into anchor-based and anchor-free. The anchor-based methods detect objects by placing a pre-defined set of anchor boxes, predicting the class and score for each anchor, and finally regressing the preserved boxes tightly around the objects. The representative methods include Fast R-CNN [21], Faster RCNN [49], R-FCN [14], SSD [38], Reti-naNet [35], Cascade R-CNN [4], etc. On another line, the anchor-free methods usually represent an object as a combination of geometry. Among them, CornerNet [30] and DeNet [55] generated a bounding box by predicting a pair of corner keypoints, and CenterNet (keypoint triplets) [17] and CPNDet [18] applied semantic information within the objects to filter out the incorrect corner pairs. FCOS [52], RepPoints [61], FoveaBox [29], SAPD [68], CenterNet (objects as points) [66], YOLO [47], etc., defined a bounding box by placing a single point (called anchor point) within the object and predicting its distances to the object boundary.",
                "For instance segmentation, there are mainly two kinds of methods, namely, the pixel-based and the contour-based methods. The pixel-based methods consider the segmentation problem as predicting the class of each single pixel. One of the representative work is Mask R-CNN [24], which first predicted the bounding box to help locating the objects, and then used pixel-wise classification to determine the object mask. The contour-based methods instead represent an object by the contour. They often start with a set points that are roughly located around the object boundary, and the points gradually get closer to the object boundary under iteration. The early representative methods are the snake series [27,11,23,12] and the recent efforts include using deep neural network [22] and the idea of anchor-free to improve the features, such as DeepSnake [44] and PolarMask [59].",
                "There are two mainstreams for human pose estimation, namely, the bottom-up methods [3,10,26,39,5] and topdown methods [66,54,58,37]. The bottom-up methods first detect the human parts and then locates the keypoints in each object, while the top-down first locates all the keypoints in the human body and then composes the individual parts into a person. The keypoints are often sparsely distributed in an image and thus are difficult to be accurately located. A practical solution is to detect the keypoints in a high-resolution feature map, called heatmap [40,10,10]. However, applying the heatmap makes the optimization hard and introduces a complex post-processing operation. CenterNet [66] proposes a neat and simple method, which only predicts a center heatmap and the keypoints are obtained by regressing the vector from the center within objects to the keypoints.",
                "This paper particularly focuses on the anchor-free methods for visual recognition. These methods originated from object detection and have drawn a lot of attention recently. They do not rely on the pre-defined anchor boxes to locate objects but by points and distance. Therefor, the anchor-free methods enjoy the ability to extend into any directions. This offers the researchers a possibility to unify the visual recognition tasks. Recent trends have spent effort in extending the anchor-free methods in object detection into other tasks, e.g., PolarMask [59] tries to extend the anchor-free into instance segmentation, while Center-Net [66] applies it into the pose estimation. Compared with our framework, both of them have limitations, which we will give a detailed discussion in section 3.2."
            ],
            "paragraphs": [
                {
                    "text": "Deep neural networks have been widely applied for visual recognition tasks. Among them, image classification [16] is the fundamental task that facilitates the design of powerful network backbones [25,60,20]. Beyond image-level description, there exist fine-scaled tasks, including object detection, instance segmentation, and pose estimation, which focus on depicting different aspects of the object. For example, the bounding boxes locate objects simply and efficiently but lack the details, while masks and keypoints reflect the shape and pose of the objects but usually need the bounding boxes to locate object firstly. According to the different properties of different tasks, many representative methods have been developed.",
                    "citations": [
                        {
                            "target": "b15",
                            "text": "[16]"
                        },
                        {
                            "target": "b24",
                            "text": "[25,"
                        },
                        {
                            "target": "b59",
                            "text": "60,"
                        },
                        {
                            "target": "b19",
                            "text": "20]"
                        }
                    ]
                },
                {
                    "text": "The object detection methods can be roughly categorized into anchor-based and anchor-free. The anchor-based methods detect objects by placing a pre-defined set of anchor boxes, predicting the class and score for each anchor, and finally regressing the preserved boxes tightly around the objects. The representative methods include Fast R-CNN [21], Faster RCNN [49], R-FCN [14], SSD [38], Reti-naNet [35], Cascade R-CNN [4], etc. On another line, the anchor-free methods usually represent an object as a combination of geometry. Among them, CornerNet [30] and DeNet [55] generated a bounding box by predicting a pair of corner keypoints, and CenterNet (keypoint triplets) [17] and CPNDet [18] applied semantic information within the objects to filter out the incorrect corner pairs. FCOS [52], RepPoints [61], FoveaBox [29], SAPD [68], CenterNet (objects as points) [66], YOLO [47], etc., defined a bounding box by placing a single point (called anchor point) within the object and predicting its distances to the object boundary.",
                    "citations": [
                        {
                            "target": "b20",
                            "text": "[21]"
                        },
                        {
                            "target": "b48",
                            "text": "[49]"
                        },
                        {
                            "target": "b13",
                            "text": "[14]"
                        },
                        {
                            "target": "b37",
                            "text": "[38]"
                        },
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b3",
                            "text": "[4]"
                        },
                        {
                            "target": "b29",
                            "text": "[30]"
                        },
                        {
                            "target": "b54",
                            "text": "[55]"
                        },
                        {
                            "target": "b16",
                            "text": "[17]"
                        },
                        {
                            "target": "b17",
                            "text": "[18]"
                        },
                        {
                            "target": "b51",
                            "text": "[52]"
                        },
                        {
                            "target": "b60",
                            "text": "[61]"
                        },
                        {
                            "target": "b28",
                            "text": "[29]"
                        },
                        {
                            "target": "b67",
                            "text": "[68]"
                        },
                        {
                            "target": "b65",
                            "text": "[66]"
                        },
                        {
                            "target": "b46",
                            "text": "[47]"
                        }
                    ]
                },
                {
                    "text": "For instance segmentation, there are mainly two kinds of methods, namely, the pixel-based and the contour-based methods. The pixel-based methods consider the segmentation problem as predicting the class of each single pixel. One of the representative work is Mask R-CNN [24], which first predicted the bounding box to help locating the objects, and then used pixel-wise classification to determine the object mask. The contour-based methods instead represent an object by the contour. They often start with a set points that are roughly located around the object boundary, and the points gradually get closer to the object boundary under iteration. The early representative methods are the snake series [27,11,23,12] and the recent efforts include using deep neural network [22] and the idea of anchor-free to improve the features, such as DeepSnake [44] and PolarMask [59].",
                    "citations": [
                        {
                            "target": "b23",
                            "text": "[24]"
                        },
                        {
                            "target": "b26",
                            "text": "[27,"
                        },
                        {
                            "target": "b10",
                            "text": "11,"
                        },
                        {
                            "target": "b22",
                            "text": "23,"
                        },
                        {
                            "target": "b11",
                            "text": "12]"
                        },
                        {
                            "target": "b21",
                            "text": "[22]"
                        },
                        {
                            "target": "b43",
                            "text": "[44]"
                        },
                        {
                            "target": "b58",
                            "text": "[59]"
                        }
                    ]
                },
                {
                    "text": "There are two mainstreams for human pose estimation, namely, the bottom-up methods [3,10,26,39,5] and topdown methods [66,54,58,37]. The bottom-up methods first detect the human parts and then locates the keypoints in each object, while the top-down first locates all the keypoints in the human body and then composes the individual parts into a person. The keypoints are often sparsely distributed in an image and thus are difficult to be accurately located. A practical solution is to detect the keypoints in a high-resolution feature map, called heatmap [40,10,10]. However, applying the heatmap makes the optimization hard and introduces a complex post-processing operation. CenterNet [66] proposes a neat and simple method, which only predicts a center heatmap and the keypoints are obtained by regressing the vector from the center within objects to the keypoints.",
                    "citations": [
                        {
                            "target": "b2",
                            "text": "[3,"
                        },
                        {
                            "target": "b9",
                            "text": "10,"
                        },
                        {
                            "target": "b25",
                            "text": "26,"
                        },
                        {
                            "target": "b38",
                            "text": "39,"
                        },
                        {
                            "target": "b4",
                            "text": "5]"
                        },
                        {
                            "target": "b65",
                            "text": "[66,"
                        },
                        {
                            "target": "b53",
                            "text": "54,"
                        },
                        {
                            "target": "b57",
                            "text": "58,"
                        },
                        {
                            "target": "b36",
                            "text": "37]"
                        },
                        {
                            "target": "b39",
                            "text": "[40,"
                        },
                        {
                            "target": "b9",
                            "text": "10,"
                        },
                        {
                            "target": "b9",
                            "text": "10]"
                        },
                        {
                            "target": "b65",
                            "text": "[66]"
                        }
                    ]
                },
                {
                    "text": "This paper particularly focuses on the anchor-free methods for visual recognition. These methods originated from object detection and have drawn a lot of attention recently. They do not rely on the pre-defined anchor boxes to locate objects but by points and distance. Therefor, the anchor-free methods enjoy the ability to extend into any directions. This offers the researchers a possibility to unify the visual recognition tasks. Recent trends have spent effort in extending the anchor-free methods in object detection into other tasks, e.g., PolarMask [59] tries to extend the anchor-free into instance segmentation, while Center-Net [66] applies it into the pose estimation. Compared with our framework, both of them have limitations, which we will give a detailed discussion in section 3.2.",
                    "citations": [
                        {
                            "target": "b58",
                            "text": "[59]"
                        },
                        {
                            "target": "b65",
                            "text": "[66]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "3.",
            "title": "Our Approach",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "3.1.",
            "title": "Location-Sensitive Visual Recognition",
            "content": [
                "Visual recognition tasks start with an image, X. Image classification aims to assign a class label c for the entire image, yet there are more challenging tasks for finescaled recognition. These tasks often focus on the instances (i.e., individual objects) in the image and depict the object properties from different aspects. Typical examples include object detection that uses a rectangular box that tightly cover the object, instance segmentation that finds out each pixel that belongs to the object, and human pose estimation that localizes the landmarks of the object (i.e., human keypoints). We use b ∈ R 4 , s ∈ [0, 1] W ×H , and k ∈ R K×2 to indicate the target of these tasks, where W and H are the image width and height, and K is the number of keypoints.",
                "An important motivation of our work is that, although these tasks differ from each other in the form of description, they share the common requirement that the model should be sensitive to the location of the anchor and/or landmarks. Throughout the remaining part of this paper, we refer to these tasks as location-sensitive visual recognition and design a unified framework for them."
            ],
            "paragraphs": [
                {
                    "text": "Visual recognition tasks start with an image, X. Image classification aims to assign a class label c for the entire image, yet there are more challenging tasks for finescaled recognition. These tasks often focus on the instances (i.e., individual objects) in the image and depict the object properties from different aspects. Typical examples include object detection that uses a rectangular box that tightly cover the object, instance segmentation that finds out each pixel that belongs to the object, and human pose estimation that localizes the landmarks of the object (i.e., human keypoints). We use b ∈ R 4 , s ∈ [0, 1] W ×H , and k ∈ R K×2 to indicate the target of these tasks, where W and H are the image width and height, and K is the number of keypoints.",
                    "citations": []
                },
                {
                    "text": "An important motivation of our work is that, although these tasks differ from each other in the form of description, they share the common requirement that the model should be sensitive to the location of the anchor and/or landmarks. Throughout the remaining part of this paper, we refer to these tasks as location-sensitive visual recognition and design a unified framework for them.",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.2.",
            "title": "LSNet: A Unified Framework",
            "content": [
                "The proposed location-sensitive network (LSNet) starts with a backbone (e.g., the ResNet [25], ResNeXt [60], etc.) that extracts features from the input image. We denote the process using x = f (X). Next, an anchor point and a few landmarks are predicted on top of x, denoted as p = g(x).",
                "Here we define p ∈ R (N +1)×2 where N is the number of landmarks, p 0 ∈ Rfoot_1 is the anchor point, and",
                "As a unified framework, the key is to relate the prediction targets (i.e., b ∈ R 4 , s ∈ [0, 1] W ×H , and k ∈ R K×2 as aforementioned) to p. For object detection, this is done by finding an extreme point (a pixel that belongs to the object and is tangent to the bounding box) on each edge of the bounding boxfoot_0 , i.e., N = 4. For instance segmenta-tion, we locate a fixed number (e.g., 36 in the experiments, N = 36) of landmarks along the contour and thus use the formed polygon to approximate the shape of the object 2 . For human pose estimation, we follow the definition of the dataset to learn a fixed number of keypoints, e.g., in the MS-COCO dataset, N = 17.",
                "Figure 2 shows the pipeline of LSNet. It belongs to the category of anchor-free methods, i.e., there is no need to pre-define a set of anchor boxes for localizing the object. LSNet is partitioned into two stages, where the first stage predicts an anchor point from the FPN head and relates it with a set of landmarks, and the second stage composes the landmarks into an object with the desired geometry (e.g., a bounding box). To facilitate accurate localization, we use the ATSS assigner [64] to assign more anchor points for each object and extract features with deformable convolution (DCN) upon the predicted landmarks. The entire model is an end-to-end trainable learnable function.",
                "LSNet receives two sources of supervision for localization and classification, elaborated in Sections 3.3 and 3.4, respectively. The localization loss is added to both stages, where the major contribution is a unified loss that fits the properties of different tasks, and the classification loss is added to the second stage upon the DCN features.",
                "LSNet extends the border of anchor-free methods for location-sensitive visual recognition. We briefly review two counterparts. (i) CenterNet predicted horizontal or vertical offsets beyond the anchor point for object detection. This limits its ability in finding the extreme points and extracting discriminative features, yet it cannot perform instance segmentation. (ii) PolarMask used a polar coordinate system for instance segmentation, making it difficult to process the situation that a ray intersects the object multiple times at some direction. In comparison, LSNet easily handles the challenging scenarios and report superior performance (see the experimental part, section 4)."
            ],
            "paragraphs": [
                {
                    "text": "The proposed location-sensitive network (LSNet) starts with a backbone (e.g., the ResNet [25], ResNeXt [60], etc.) that extracts features from the input image. We denote the process using x = f (X). Next, an anchor point and a few landmarks are predicted on top of x, denoted as p = g(x).",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "[25]"
                        },
                        {
                            "target": "b59",
                            "text": "[60]"
                        }
                    ]
                },
                {
                    "text": "Here we define p ∈ R (N +1)×2 where N is the number of landmarks, p 0 ∈ Rfoot_1 is the anchor point, and",
                    "citations": []
                },
                {
                    "text": "As a unified framework, the key is to relate the prediction targets (i.e., b ∈ R 4 , s ∈ [0, 1] W ×H , and k ∈ R K×2 as aforementioned) to p. For object detection, this is done by finding an extreme point (a pixel that belongs to the object and is tangent to the bounding box) on each edge of the bounding boxfoot_0 , i.e., N = 4. For instance segmenta-tion, we locate a fixed number (e.g., 36 in the experiments, N = 36) of landmarks along the contour and thus use the formed polygon to approximate the shape of the object 2 . For human pose estimation, we follow the definition of the dataset to learn a fixed number of keypoints, e.g., in the MS-COCO dataset, N = 17.",
                    "citations": []
                },
                {
                    "text": "Figure 2 shows the pipeline of LSNet. It belongs to the category of anchor-free methods, i.e., there is no need to pre-define a set of anchor boxes for localizing the object. LSNet is partitioned into two stages, where the first stage predicts an anchor point from the FPN head and relates it with a set of landmarks, and the second stage composes the landmarks into an object with the desired geometry (e.g., a bounding box). To facilitate accurate localization, we use the ATSS assigner [64] to assign more anchor points for each object and extract features with deformable convolution (DCN) upon the predicted landmarks. The entire model is an end-to-end trainable learnable function.",
                    "citations": [
                        {
                            "target": "b63",
                            "text": "[64]"
                        }
                    ]
                },
                {
                    "text": "LSNet receives two sources of supervision for localization and classification, elaborated in Sections 3.3 and 3.4, respectively. The localization loss is added to both stages, where the major contribution is a unified loss that fits the properties of different tasks, and the classification loss is added to the second stage upon the DCN features.",
                    "citations": []
                },
                {
                    "text": "LSNet extends the border of anchor-free methods for location-sensitive visual recognition. We briefly review two counterparts. (i) CenterNet predicted horizontal or vertical offsets beyond the anchor point for object detection. This limits its ability in finding the extreme points and extracting discriminative features, yet it cannot perform instance segmentation. (ii) PolarMask used a polar coordinate system for instance segmentation, making it difficult to process the situation that a ray intersects the object multiple times at some direction. In comparison, LSNet easily handles the challenging scenarios and report superior performance (see the experimental part, section 4).",
                    "citations": []
                }
            ]
        },
        {
            "n": "3.3.",
            "title": "Cross IOU Loss",
            "content": [
                "The unified framework raises new challenges to the supervision of localization, because the function needs to consider both the global and local properties of the object. To clarify, we notice that the evaluation of object detection and instance segmentation judges if an object is correctly recognized by the global IOU between the prediction and the ground-truth, while pose estimation measures the accuracy by each individual keypoint.",
                "To this end, we design the cross-IOU loss as the unified supervision.",
                "The loss is defined upon the predicted and ground-truth objects, and we use p , where p = (p x , p y ), to denote the corresponding ground-truth of the anchor point and landmarks. In the left part, C3-C5 denote the feature maps in the backbone that are connected to the feature pyramid [34], and P3-P7 denote the FPN layers used for final prediction. LSNet is partitioned into two stages. In the first stage, we predict a set of offset vectors that relate the anchor point to the extreme points under the supervision of the cross-IOU loss. In the second stage, the predicted vectors are used as the offsets of the deformable convolution (DCN) [15] to extract complementary visual features around the extreme points. These features are used for refining the localization and predicting the object class.",
                "We compute the offset from the anchor point to the landmarks in a cross-coordinate system, i.e., q n = (p n,x -p 0,x ) -, (p n,x -p 0,x ) + , (p n,y -p 0,y ) -, (p n,y -p 0,y )",
                "for n = 1, 2, . . . , N , where (a)",
                "-and (a)",
                "+ denotes max{-a, 0} and max{a, 0}, respectively. Finally, we write the cross-IOU loss as:",
                "where |•| 1 indicates the 1 -norm. In other words, the cross-IOU function rewards the components (q n and q n ) of similar length (in which case the prediction and the groundtruth maximally overlap) and penalizes the components on different directions. Based on the Eqn (1), we define the cross-IOU loss as",
                "Obviously, when p n = p n for all n, we have L cIOU = 0 as expected 3 .",
                "The cross-IOU loss brings a direct benefit that it fits different scales of features without the need of specific parameter tuning. This alleviates the difficulty of integrating multi-scale information, e.g., using the feature pyramid [34]. In comparison, the smooth-1 loss [21] is sensitive to the scale of vector (e.g., the loss value tends to be large when the feature resolution is large) and neglects the relationship between the components that are from the same vector. Moreover, by approximating the IOU using individual components, the cross-IOU loss is flexibly transplanted to instance segmentation and human pose estimation, unlike the original IOU loss [63] that is difficult to compute on polygons (for segmentation) and undefined for discrete keypoints (for pose estimation)."
            ],
            "paragraphs": [
                {
                    "text": "The unified framework raises new challenges to the supervision of localization, because the function needs to consider both the global and local properties of the object. To clarify, we notice that the evaluation of object detection and instance segmentation judges if an object is correctly recognized by the global IOU between the prediction and the ground-truth, while pose estimation measures the accuracy by each individual keypoint.",
                    "citations": []
                },
                {
                    "text": "To this end, we design the cross-IOU loss as the unified supervision.",
                    "citations": []
                },
                {
                    "text": "The loss is defined upon the predicted and ground-truth objects, and we use p , where p = (p x , p y ), to denote the corresponding ground-truth of the anchor point and landmarks. In the left part, C3-C5 denote the feature maps in the backbone that are connected to the feature pyramid [34], and P3-P7 denote the FPN layers used for final prediction. LSNet is partitioned into two stages. In the first stage, we predict a set of offset vectors that relate the anchor point to the extreme points under the supervision of the cross-IOU loss. In the second stage, the predicted vectors are used as the offsets of the deformable convolution (DCN) [15] to extract complementary visual features around the extreme points. These features are used for refining the localization and predicting the object class.",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        },
                        {
                            "target": "b14",
                            "text": "[15]"
                        }
                    ]
                },
                {
                    "text": "We compute the offset from the anchor point to the landmarks in a cross-coordinate system, i.e., q n = (p n,x -p 0,x ) -, (p n,x -p 0,x ) + , (p n,y -p 0,y ) -, (p n,y -p 0,y )",
                    "citations": []
                },
                {
                    "text": "for n = 1, 2, . . . , N , where (a)",
                    "citations": []
                },
                {
                    "text": "-and (a)",
                    "citations": []
                },
                {
                    "text": "+ denotes max{-a, 0} and max{a, 0}, respectively. Finally, we write the cross-IOU loss as:",
                    "citations": []
                },
                {
                    "text": "where |•| 1 indicates the 1 -norm. In other words, the cross-IOU function rewards the components (q n and q n ) of similar length (in which case the prediction and the groundtruth maximally overlap) and penalizes the components on different directions. Based on the Eqn (1), we define the cross-IOU loss as",
                    "citations": []
                },
                {
                    "text": "Obviously, when p n = p n for all n, we have L cIOU = 0 as expected 3 .",
                    "citations": []
                },
                {
                    "text": "The cross-IOU loss brings a direct benefit that it fits different scales of features without the need of specific parameter tuning. This alleviates the difficulty of integrating multi-scale information, e.g., using the feature pyramid [34]. In comparison, the smooth-1 loss [21] is sensitive to the scale of vector (e.g., the loss value tends to be large when the feature resolution is large) and neglects the relationship between the components that are from the same vector. Moreover, by approximating the IOU using individual components, the cross-IOU loss is flexibly transplanted to instance segmentation and human pose estimation, unlike the original IOU loss [63] that is difficult to compute on polygons (for segmentation) and undefined for discrete keypoints (for pose estimation).",
                    "citations": [
                        {
                            "target": "b33",
                            "text": "[34]"
                        },
                        {
                            "target": "b20",
                            "text": "[21]"
                        },
                        {
                            "target": "b62",
                            "text": "[63]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "FPN layers",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "3.4.",
            "title": "Pyramid DCN",
            "content": [
                "To enhance discriminative information for recognition, we use deformable convolution (DCN) [15,70] to extract features from the landmarks. The standard DCN has 9 off-sets, while the number of offsets is 4, 36, and 17 for detection, segmentation, and pose estimation, respectively. In the latter two cases, to avoid redundant features extracted from close areas, we sample 9 landmarks uniformly from the candidates. We further build the feature extraction module upon the feature pyramid [34]. The offsets are adjusted to different stages by accordingly rescaling the vectors.",
                "We name the proposed method Pyramid-DCN, and illustrate it in Figure 3. As shown in experiments, both feature extraction from the landmarks and using the pyramid structure improve recognition accuracy."
            ],
            "paragraphs": [
                {
                    "text": "To enhance discriminative information for recognition, we use deformable convolution (DCN) [15,70] to extract features from the landmarks. The standard DCN has 9 off-sets, while the number of offsets is 4, 36, and 17 for detection, segmentation, and pose estimation, respectively. In the latter two cases, to avoid redundant features extracted from close areas, we sample 9 landmarks uniformly from the candidates. We further build the feature extraction module upon the feature pyramid [34]. The offsets are adjusted to different stages by accordingly rescaling the vectors.",
                    "citations": [
                        {
                            "target": "b14",
                            "text": "[15,"
                        },
                        {
                            "target": "b69",
                            "text": "70]"
                        },
                        {
                            "target": "b33",
                            "text": "[34]"
                        }
                    ]
                },
                {
                    "text": "We name the proposed method Pyramid-DCN, and illustrate it in Figure 3. As shown in experiments, both feature extraction from the landmarks and using the pyramid structure improve recognition accuracy.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.",
            "title": "Experiments",
            "content": [],
            "paragraphs": []
        },
        {
            "n": "4.1.",
            "title": "Dataset and Evaluation Metrics",
            "content": [
                "We evaluate our framework on the MS-COCO 2017 dataset [36], which is a popular, large-scale object detection, segmentation, and human pose dataset. For object detection and segmentation, it contains over 118K training images, 5K validation images and 20K test-dev images covering 80 object categories. For human pose, the person instance is labeled with 17 keypoints, containing over 57K training images with 150K person instances, 5K validation images and 20K test-dev images.",
                "The average precision (AP) metric is applied to characterize the performance of our method as well as other competitors. There are subtle differences in the definition of AP for different tasks. For object detection, AP is calculated the average precision under different bounding box IOU thresholds(from 0.5 to 0.95), while the bounding box IOU is replaced with the mask IOU in instance segmentation. In the human pose task, AP is calculated based on the object keypoint similarity (OKS), which reflects the distance between the predicted keypoints and the annotations."
            ],
            "paragraphs": [
                {
                    "text": "We evaluate our framework on the MS-COCO 2017 dataset [36], which is a popular, large-scale object detection, segmentation, and human pose dataset. For object detection and segmentation, it contains over 118K training images, 5K validation images and 20K test-dev images covering 80 object categories. For human pose, the person instance is labeled with 17 keypoints, containing over 57K training images with 150K person instances, 5K validation images and 20K test-dev images.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        }
                    ]
                },
                {
                    "text": "The average precision (AP) metric is applied to characterize the performance of our method as well as other competitors. There are subtle differences in the definition of AP for different tasks. For object detection, AP is calculated the average precision under different bounding box IOU thresholds(from 0.5 to 0.95), while the bounding box IOU is replaced with the mask IOU in instance segmentation. In the human pose task, AP is calculated based on the object keypoint similarity (OKS), which reflects the distance between the predicted keypoints and the annotations.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.2.",
            "title": "Implementation Details",
            "content": [
                "We use ResNet [25], ResNeXt [60] and Res2Net [20] with the weights pre-trained on ImageNet [16] as our backbones, respectively. The feature pyramid network (FPN) [34] is applied to deal with objects with different scales. For object detection, we set four vectors for each object to learn to find the four extreme points (top, left, bottom, right). We refer to ExtremeNet [67] to obtain extreme point annotations from the object mask 4 . For instance segmentation and human pose estimation, we set 36 vectors for each instance to regress the location of contour points and 17 vectors to regress the 17 keypoints.",
                "Training and Inference. We train our framework on eight NVIDIA Tesla-V100 GPUs with two images on each GPU. The initial learning rate is set as 0.01, the weight decay as 0.0001 and momentum as 0.9. In the ablation study, we use a ResNet-50 [25] pre-trained on ImageNet [16] as the backbone, and fine-tune the model for 12 epochs using a single-scale of [800, 1333] and augment the training images with random horizontal flipping. The learning rate decays by a factor of 10 at after the 8th and 11th epochs, respectively. We also use stronger backbones and longer training epochs (24 epochs for object detection, 30 epochs for instance segmentation and 60 epochs for human pose with the learning rate decayed by a factor of 10 after the 16th, 22nd, and 50th epochs, respectively) and multi-scale input images (from [400, 1333] to [960, 1333]) to further improve the recognition accuracy. In the first stage, we only select the anchor point closest to the center of the object as a positive sample. In the second stage, we use the ATSS [64] assigner to assign the anchor points for each object. The overall loss function is",
                "where L cls and L vector denote the Focal loss [35] and our cross-IOU loss, respectively. We set the balancing coefficients, β and γ, to be 1.0 and 2.0 in the experiments. During the inference, both the single-scale testing and multi-scale testing strategy are applied. We use the scale of [800, 1333] for single-scale testing. For the multi-scale testing, we refer to ATSS [64] to set the image scales. We also use the nonmaximum suppression (NMS) strategy with a threshold of 0.6 to remove the redundant results."
            ],
            "paragraphs": [
                {
                    "text": "We use ResNet [25], ResNeXt [60] and Res2Net [20] with the weights pre-trained on ImageNet [16] as our backbones, respectively. The feature pyramid network (FPN) [34] is applied to deal with objects with different scales. For object detection, we set four vectors for each object to learn to find the four extreme points (top, left, bottom, right). We refer to ExtremeNet [67] to obtain extreme point annotations from the object mask 4 . For instance segmentation and human pose estimation, we set 36 vectors for each instance to regress the location of contour points and 17 vectors to regress the 17 keypoints.",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "[25]"
                        },
                        {
                            "target": "b59",
                            "text": "[60]"
                        },
                        {
                            "target": "b19",
                            "text": "[20]"
                        },
                        {
                            "target": "b15",
                            "text": "[16]"
                        },
                        {
                            "target": "b33",
                            "text": "[34]"
                        },
                        {
                            "target": "b66",
                            "text": "[67]"
                        }
                    ]
                },
                {
                    "text": "Training and Inference. We train our framework on eight NVIDIA Tesla-V100 GPUs with two images on each GPU. The initial learning rate is set as 0.01, the weight decay as 0.0001 and momentum as 0.9. In the ablation study, we use a ResNet-50 [25] pre-trained on ImageNet [16] as the backbone, and fine-tune the model for 12 epochs using a single-scale of [800, 1333] and augment the training images with random horizontal flipping. The learning rate decays by a factor of 10 at after the 8th and 11th epochs, respectively. We also use stronger backbones and longer training epochs (24 epochs for object detection, 30 epochs for instance segmentation and 60 epochs for human pose with the learning rate decayed by a factor of 10 after the 16th, 22nd, and 50th epochs, respectively) and multi-scale input images (from [400, 1333] to [960, 1333]) to further improve the recognition accuracy. In the first stage, we only select the anchor point closest to the center of the object as a positive sample. In the second stage, we use the ATSS [64] assigner to assign the anchor points for each object. The overall loss function is",
                    "citations": [
                        {
                            "target": "b24",
                            "text": "[25]"
                        },
                        {
                            "target": "b15",
                            "text": "[16]"
                        },
                        {
                            "target": "b63",
                            "text": "[64]"
                        }
                    ]
                },
                {
                    "text": "where L cls and L vector denote the Focal loss [35] and our cross-IOU loss, respectively. We set the balancing coefficients, β and γ, to be 1.0 and 2.0 in the experiments. During the inference, both the single-scale testing and multi-scale testing strategy are applied. We use the scale of [800, 1333] for single-scale testing. For the multi-scale testing, we refer to ATSS [64] to set the image scales. We also use the nonmaximum suppression (NMS) strategy with a threshold of 0.6 to remove the redundant results.",
                    "citations": [
                        {
                            "target": "b34",
                            "text": "[35]"
                        },
                        {
                            "target": "b63",
                            "text": "[64]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.3.",
            "title": "Object Detection",
            "content": [
                "Comparisons to SOTA. We evaluate the detection accuracy of LSNet on the MS-COCO test-dev set, the results are shown in Table 1. As Table 1 shows, our method is an anchor-free detector, with a backbone of ResNet-50, LSNet achieves a box AP of 44.8% with 12.7 FPS, which has been competitive with other detectors that equipped with deeper backbones. When equipped with stronger backbones, LSNet performs even better. This benefits from our proposed cross-IOU loss. It helps the LSNet to locate the landmarks with high accuracy, the rich global information contained in the landmarks further promote the cross-IOU loss to regress the landmarks more accurately. With the additional corner point verification (CPV) [9] and multi-scale testing [64], LSNet achieves a box AP of 53.5%, which outperforms all the anchor-free detectors as we know.",
                "Cross-IOU Loss for Vector Regression. To evaluate the performance of cross-IOU loss, we design four contrast experiments on the MS COCO [36] validation set, which are (i) the GIOU loss [50] (a variant of the IOU loss) for rectangle bounding box regression, (ii) the smooth-1 loss for rect- LSNet surpasses all competitors in the anchor-free group. The abbreviations are: 'R' -ResNet [25], 'X' -ResNeXt [60], 'HG' -Hourglass network [40], 'R2' -Res2Net [20], 'CPV' -corner point verification [9], 'MS train ' -multi-scale training, ' †' -multi-scale testing [64].  angle bounding box regression, (iii) the smooth-1 loss for extreme bounding box regression, and (iv) the cross-IOU loss for extreme bounding box regression, respectively. All the experiments are done in the first stage in our framework (shown in Figure 2) with ResNet-50 [25] as the backbone, and we train the model for each experiment for 12 epochs. Table 2 summarizes the results. We can see that the smooth- Table 3: The detection accuracy (%) of using different features. PA, PD, and PP denote using the anchor point features along, anchor point features with the single-scale extreme point features, and with the pyramid extreme point features, respectively.",
                "boxes, respectively. This reveals that it is more difficult to regress an angled vector than a straight vector. By contrast, the cross-IOU loss performs much better than the smooth-",
                "Figure 4: The detection classification loss and average IOU with respect to the number of elapsed epochs. 'DCN feat', 'point feat' and 'extreme feat' denote the features adaptively learned by the DCN kernel, the features at the anchor points and extreme points, respectively. 'Smooth', 'GIOU' and 'Ours' denote the smooth-1 loss, the GIOU loss [50], and the cross-IOU loss, respectively. tion of the cross-IOU loss and landmark feature extraction significantly boosts recognition accuracy.",
                "Landmark Features Improve Precision. The landmarks (in particular, the extreme points in the detection task) are often related to discriminative appearance features, which may benefit visual recognition. To confirm this, we investigate different settings by using the anchor point features alone and integrating the anchor point features with either DCN or extreme point features, respectively. We still report the detection accuracy with all other settings remaining the same as in the previous experiments (studying the cross IOU loss). The results are shown in Figure 4. For the smooth-1 loss and the GIOU loss, we regress the rectangle bounding boxes, and the DCN features are extracted by the adaptively learned DCN kernel; for the cross-IOU loss, we use two sets of vectors both of which regress the extreme bounding boxes -the first set is trained to predict the extreme points and extract the extreme features, and we use the extreme features along with the anchor point features to train the second set from scratch. As shown in Figure 4, both the extreme and DCN features boost the classification accuracy. Recall that the prior experiments suggested the usefulness of the extreme features are useful for localization, combining the current results, we verify that the features around the landmarks are discriminative and thus benefit visual recognition.",
                "Pyramid DCN Improves Precision. We further equip the LSNet with the pyramid DCN to extract the multi-scale features around the landmarks. Table 3 shows our method achieves an AP of 36.2% with the features extracted by the Pyramid DCN, which outperforms the AP with single-scale features by a margin of 0.7%."
            ],
            "paragraphs": [
                {
                    "text": "Comparisons to SOTA. We evaluate the detection accuracy of LSNet on the MS-COCO test-dev set, the results are shown in Table 1. As Table 1 shows, our method is an anchor-free detector, with a backbone of ResNet-50, LSNet achieves a box AP of 44.8% with 12.7 FPS, which has been competitive with other detectors that equipped with deeper backbones. When equipped with stronger backbones, LSNet performs even better. This benefits from our proposed cross-IOU loss. It helps the LSNet to locate the landmarks with high accuracy, the rich global information contained in the landmarks further promote the cross-IOU loss to regress the landmarks more accurately. With the additional corner point verification (CPV) [9] and multi-scale testing [64], LSNet achieves a box AP of 53.5%, which outperforms all the anchor-free detectors as we know.",
                    "citations": [
                        {
                            "target": "b8",
                            "text": "[9]"
                        },
                        {
                            "target": "b63",
                            "text": "[64]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "",
            "title": "FusionT-LESS.",
            "content": [
                "A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS [18] which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector [5] and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation."
            ],
            "paragraphs": [
                {
                    "text": "Cross-IOU Loss for Vector Regression. To evaluate the performance of cross-IOU loss, we design four contrast experiments on the MS COCO [36] validation set, which are (i) the GIOU loss [50] (a variant of the IOU loss) for rectangle bounding box regression, (ii) the smooth-1 loss for rect- LSNet surpasses all competitors in the anchor-free group. The abbreviations are: 'R' -ResNet [25], 'X' -ResNeXt [60], 'HG' -Hourglass network [40], 'R2' -Res2Net [20], 'CPV' -corner point verification [9], 'MS train ' -multi-scale training, ' †' -multi-scale testing [64].  angle bounding box regression, (iii) the smooth-1 loss for extreme bounding box regression, and (iv) the cross-IOU loss for extreme bounding box regression, respectively. All the experiments are done in the first stage in our framework (shown in Figure 2) with ResNet-50 [25] as the backbone, and we train the model for each experiment for 12 epochs. Table 2 summarizes the results. We can see that the smooth- Table 3: The detection accuracy (%) of using different features. PA, PD, and PP denote using the anchor point features along, anchor point features with the single-scale extreme point features, and with the pyramid extreme point features, respectively.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        },
                        {
                            "target": "b49",
                            "text": "[50]"
                        },
                        {
                            "target": "b24",
                            "text": "[25]"
                        },
                        {
                            "target": "b59",
                            "text": "[60]"
                        },
                        {
                            "target": "b39",
                            "text": "[40]"
                        },
                        {
                            "target": "b19",
                            "text": "[20]"
                        },
                        {
                            "target": "b8",
                            "text": "[9]"
                        },
                        {
                            "target": "b63",
                            "text": "[64]"
                        },
                        {
                            "target": "b24",
                            "text": "[25]"
                        }
                    ]
                },
                {
                    "text": "boxes, respectively. This reveals that it is more difficult to regress an angled vector than a straight vector. By contrast, the cross-IOU loss performs much better than the smooth-",
                    "citations": []
                },
                {
                    "text": "Figure 4: The detection classification loss and average IOU with respect to the number of elapsed epochs. 'DCN feat', 'point feat' and 'extreme feat' denote the features adaptively learned by the DCN kernel, the features at the anchor points and extreme points, respectively. 'Smooth', 'GIOU' and 'Ours' denote the smooth-1 loss, the GIOU loss [50], and the cross-IOU loss, respectively. tion of the cross-IOU loss and landmark feature extraction significantly boosts recognition accuracy.",
                    "citations": [
                        {
                            "target": "b49",
                            "text": "[50]"
                        }
                    ]
                },
                {
                    "text": "Landmark Features Improve Precision. The landmarks (in particular, the extreme points in the detection task) are often related to discriminative appearance features, which may benefit visual recognition. To confirm this, we investigate different settings by using the anchor point features alone and integrating the anchor point features with either DCN or extreme point features, respectively. We still report the detection accuracy with all other settings remaining the same as in the previous experiments (studying the cross IOU loss). The results are shown in Figure 4. For the smooth-1 loss and the GIOU loss, we regress the rectangle bounding boxes, and the DCN features are extracted by the adaptively learned DCN kernel; for the cross-IOU loss, we use two sets of vectors both of which regress the extreme bounding boxes -the first set is trained to predict the extreme points and extract the extreme features, and we use the extreme features along with the anchor point features to train the second set from scratch. As shown in Figure 4, both the extreme and DCN features boost the classification accuracy. Recall that the prior experiments suggested the usefulness of the extreme features are useful for localization, combining the current results, we verify that the features around the landmarks are discriminative and thus benefit visual recognition.",
                    "citations": []
                },
                {
                    "text": "Pyramid DCN Improves Precision. We further equip the LSNet with the pyramid DCN to extract the multi-scale features around the landmarks. Table 3 shows our method achieves an AP of 36.2% with the features extracted by the Pyramid DCN, which outperforms the AP with single-scale features by a margin of 0.7%.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.4.",
            "title": "Instance Segmentation",
            "content": [
                "Comparisons to SOTA. We show the instance segmentation inference results evaluated on the MS-COCO test-dev set [36] on Table 4. LSNet achieves a mask AP of 38.0% and 40.2% using the single-scale and multi-scale testing protocols, respectively, surpassing all published contourbased methods to the best of our knowledge, and the accuracy is even competitive among the pixel-based approaches.",
                "Comparisons with PolarMask [59]. It is interesting to further compare our method with PolarMask, the previous best contour-based approach for instance segmentation. The major difference is that PolarMask assumed the entire object boundary to be seen by the anchor point, but this may not be the case especially for some complicated objects. Once the ray along some direction intersects with the border more than once, the method considered only one and thus in-  curred accuracy loss (a typical example is the 'motorcycle' contour in Figure 5). In our approach, this issue is solved by ranking the landmarks more flexibly, being compatible to complicated shapes. The Number of Landmarks. LSNet represents each instance using a polygon. Using a larger number of landmarks improves the upper-bound of accuracy, but can also incur heavy computational costs and cause the landmark prediction module difficult to be optimized. To choose a proper number of landmarks, we refer to the ground-truth masks of the MS-COCO validation set and quantize each mask into a polygon that best describes it. We find that using 18, 36, and 72 landmarks achieves APs of 89.0%, 97.4%, and 99.2%, respectively, and we consider N = 36 to be a nice tradeoff."
            ],
            "paragraphs": [
                {
                    "text": "Comparisons to SOTA. We show the instance segmentation inference results evaluated on the MS-COCO test-dev set [36] on Table 4. LSNet achieves a mask AP of 38.0% and 40.2% using the single-scale and multi-scale testing protocols, respectively, surpassing all published contourbased methods to the best of our knowledge, and the accuracy is even competitive among the pixel-based approaches.",
                    "citations": [
                        {
                            "target": "b35",
                            "text": "[36]"
                        }
                    ]
                },
                {
                    "text": "Comparisons with PolarMask [59]. It is interesting to further compare our method with PolarMask, the previous best contour-based approach for instance segmentation. The major difference is that PolarMask assumed the entire object boundary to be seen by the anchor point, but this may not be the case especially for some complicated objects. Once the ray along some direction intersects with the border more than once, the method considered only one and thus in-  curred accuracy loss (a typical example is the 'motorcycle' contour in Figure 5). In our approach, this issue is solved by ranking the landmarks more flexibly, being compatible to complicated shapes. The Number of Landmarks. LSNet represents each instance using a polygon. Using a larger number of landmarks improves the upper-bound of accuracy, but can also incur heavy computational costs and cause the landmark prediction module difficult to be optimized. To choose a proper number of landmarks, we refer to the ground-truth masks of the MS-COCO validation set and quantize each mask into a polygon that best describes it. We find that using 18, 36, and 72 landmarks achieves APs of 89.0%, 97.4%, and 99.2%, respectively, and we consider N = 36 to be a nice tradeoff.",
                    "citations": [
                        {
                            "target": "b58",
                            "text": "[59]"
                        }
                    ]
                }
            ]
        },
        {
            "n": "4.5.",
            "title": "Human Pose Estimation",
            "content": [
                "Comparisons to SOTA. Unlike most of the human pose estimation methods that predict the keypoints using the heatmaps, LSNet predicts the keypoints using regression only. In the experiment, we use the object bounding boxes ('obj-box') and keypoint-boxes ('kps-box') to assign training samples, respectively. We will give a detailed discussion of the difference between the two methods in the Appendix B. On the MS-COCO test-dev set, LSNet reports an AP of 55.7% w/ obj-box and 59.0% w/ kps-box, respectively, which outperform CenterNet-reg [66] with the Hourglass-104 backbone. However, LSNet does not perform as well as the heatmap-based methods, and we analyze the reason as follows.",
                "Error Analysis. We can observe that the LSNet struggles particularly in the high OKS regimes, e.g., compared to Pose-AE [39], the deficit of AP 50 (for LSNet w/ obj-box) is 3.3% while that of AP 75 grows to 9.0%. Note that using keypoint regression is not as accurate as using the heatmaps for refinement, and thus LSNet is less sensitive in the pixellevel prediction. However, the AP metric of pose estimation is largely impacted by this factor. To show this, we artificially add an average deviation of 1, 2, and 3 pixels to the prediction results of CenterNet-jd [66] (with a backbone of Hourglass-104). The AP on the MS-COCO validation set is significantly reduced from 64.0% (corresponding to the test AP of 63.0% in Table 5) to 61.1%, 53.4%, and 44.0%, respectively.",
                "On the other hand, we use the heatmaps produced by CenterNet-jd (Hourglass-104) to refine the prediction of LSNet w/ obj-box. As a result, the AP on the MS-COCO validation set is improved from 56.5% (corresponding to the test AP of 55.7% in Table 5) to 60.7%. This suggests that LSNet still needs further manipulation of high-resolution features towards higher pixel-level accuracy. The Benefit of LSNet. Despite the relatively weak pixellevel localization, LSNet (w/ obj-box) enjoys the ability of perceiving multi-scale human instances, many of which are not annotated in the dataset. Some examples are shown in the right side of the Table 5. Since the ground-truth is not available to evaluate the impact, we refer to the heatmaps of CenterNet-jd (Hourglass-104) to deliberately remove these 'false positives'. Consequently, AP is further improved from 60.7% to 63.0%, comparable with the heatmap-based methods, though the improvement seems less meaningful."
            ],
            "paragraphs": [
                {
                    "text": "Comparisons to SOTA. Unlike most of the human pose estimation methods that predict the keypoints using the heatmaps, LSNet predicts the keypoints using regression only. In the experiment, we use the object bounding boxes ('obj-box') and keypoint-boxes ('kps-box') to assign training samples, respectively. We will give a detailed discussion of the difference between the two methods in the Appendix B. On the MS-COCO test-dev set, LSNet reports an AP of 55.7% w/ obj-box and 59.0% w/ kps-box, respectively, which outperform CenterNet-reg [66] with the Hourglass-104 backbone. However, LSNet does not perform as well as the heatmap-based methods, and we analyze the reason as follows.",
                    "citations": [
                        {
                            "target": "b65",
                            "text": "[66]"
                        }
                    ]
                },
                {
                    "text": "Error Analysis. We can observe that the LSNet struggles particularly in the high OKS regimes, e.g., compared to Pose-AE [39], the deficit of AP 50 (for LSNet w/ obj-box) is 3.3% while that of AP 75 grows to 9.0%. Note that using keypoint regression is not as accurate as using the heatmaps for refinement, and thus LSNet is less sensitive in the pixellevel prediction. However, the AP metric of pose estimation is largely impacted by this factor. To show this, we artificially add an average deviation of 1, 2, and 3 pixels to the prediction results of CenterNet-jd [66] (with a backbone of Hourglass-104). The AP on the MS-COCO validation set is significantly reduced from 64.0% (corresponding to the test AP of 63.0% in Table 5) to 61.1%, 53.4%, and 44.0%, respectively.",
                    "citations": [
                        {
                            "target": "b38",
                            "text": "[39]"
                        },
                        {
                            "target": "b65",
                            "text": "[66]"
                        }
                    ]
                },
                {
                    "text": "On the other hand, we use the heatmaps produced by CenterNet-jd (Hourglass-104) to refine the prediction of LSNet w/ obj-box. As a result, the AP on the MS-COCO validation set is improved from 56.5% (corresponding to the test AP of 55.7% in Table 5) to 60.7%. This suggests that LSNet still needs further manipulation of high-resolution features towards higher pixel-level accuracy. The Benefit of LSNet. Despite the relatively weak pixellevel localization, LSNet (w/ obj-box) enjoys the ability of perceiving multi-scale human instances, many of which are not annotated in the dataset. Some examples are shown in the right side of the Table 5. Since the ground-truth is not available to evaluate the impact, we refer to the heatmaps of CenterNet-jd (Hourglass-104) to deliberately remove these 'false positives'. Consequently, AP is further improved from 60.7% to 63.0%, comparable with the heatmap-based methods, though the improvement seems less meaningful.",
                    "citations": []
                }
            ]
        },
        {
            "n": "4.6.",
            "title": "Qualitative results for LSNet",
            "content": [
                "We show some visualized results of LSNet in Figure 5, inlcuding object detection, instance segmentation and human pose estimation. Please refer to the appendix for more qualitative results."
            ],
            "paragraphs": [
                {
                    "text": "We show some visualized results of LSNet in Figure 5, inlcuding object detection, instance segmentation and human pose estimation. Please refer to the appendix for more qualitative results.",
                    "citations": []
                }
            ]
        },
        {
            "n": "5.",
            "title": "Conclusions",
            "content": [
                "This paper unifies three location-sensitive visual recognition tasks (object detection, instance segmentation, and human pose estimation) using the location-sensitive network (LSNet). The key module that supports the framework is a novel cross-IOU loss that is friendly to receiving supervision from multiple scales. Equipped with a pyramid DCN, LSNet achieves the state-of-the-art performance on anchor-free detection and segmentation. This work suggests that using keypoints to define and localize objects is a promising direction, and we hope to extend our approach to achieve a stronger ability of generalization."
            ],
            "paragraphs": [
                {
                    "text": "This paper unifies three location-sensitive visual recognition tasks (object detection, instance segmentation, and human pose estimation) using the location-sensitive network (LSNet). The key module that supports the framework is a novel cross-IOU loss that is friendly to receiving supervision from multiple scales. Equipped with a pyramid DCN, LSNet achieves the state-of-the-art performance on anchor-free detection and segmentation. This work suggests that using keypoints to define and localize objects is a promising direction, and we hope to extend our approach to achieve a stronger ability of generalization.",
                    "citations": []
                }
            ]
        },
        {
            "n": "",
            "title": "F Reconstruction",
            "content": [],
            "paragraphs": []
        }
    ],
    "references": [
        {
            "id": "b0",
            "title": "2d human pose estimation: New benchmark and state of the art analysis",
            "authors": [
                "M Andriluka",
                "L Pishchulin",
                "P Gehler",
                "B Schiele"
            ],
            "date": "2014",
            "journal": "Proceedings of the IEEE Conference on computer Vision and Pattern Recognition"
        },
        {
            "id": "b1",
            "title": "Yolact: Realtime instance segmentation",
            "authors": [
                "D Bolya",
                "C Zhou",
                "F Xiao",
                "Y Lee"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
        },
        {
            "id": "b2",
            "title": "Learning delicate local representations for multi-person pose estimation",
            "authors": [
                "Y Cai",
                "Z Wang",
                "Z Luo",
                "B Yin",
                "A Du",
                "H Wang",
                "X Zhang",
                "X Zhou",
                "E Zhou",
                "J Sun"
            ],
            "date": "2020",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b3",
            "title": "Cascade r-cnn: Delving into high quality object detection",
            "authors": [
                "Z Cai",
                "N Vasconcelos"
            ],
            "date": "2018",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b4",
            "title": "Openpose: realtime multi-person 2d pose estimation using part affinity fields",
            "authors": [
                "Z Cao",
                "G Hidalgo",
                "T Simon",
                "S.-E Wei",
                "Y Sheikh"
            ],
            "date": "2019. 1, 2, 8",
            "journal": "IEEE transactions on pattern analysis and machine intelligence"
        },
        {
            "id": "b5",
            "title": "End-to-end object detection with transformers",
            "authors": [
                "N Carion",
                "F Massa",
                "G Synnaeve",
                "N Usunier",
                "A Kirillov",
                "S Zagoruyko"
            ],
            "date": "2020",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b6",
            "title": "Hybrid task cascade for instance segmentation",
            "authors": [
                "K Chen",
                "J Pang",
                "J Wang",
                "Y Xiong",
                "X Li",
                "S Sun",
                "W Feng",
                "Z Liu",
                "J Shi",
                "W Ouyang"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b7",
            "title": "Tensormask: A foundation for dense object segmentation",
            "authors": [
                "X Chen",
                "R Girshick",
                "K He",
                "P Dollár"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
        },
        {
            "id": "b8",
            "title": "Reppoints v2: Verification meets regression for object detection",
            "authors": [
                "Y Chen",
                "Z Zhang",
                "Y Cao",
                "L Wang",
                "S Lin",
                "H Hu"
            ],
            "date": "2020",
            "journal": "Advances in Neural Information Processing Systems"
        },
        {
            "id": "b9",
            "title": "Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation",
            "authors": [
                "B Cheng",
                "B Xiao",
                "J Wang",
                "H Shi",
                "T Huang",
                "L Zhang"
            ],
            "date": "2020",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognitio"
        },
        {
            "id": "b10",
            "title": "On active contour models and balloons",
            "authors": [
                "L Cohen"
            ],
            "date": "1991",
            "journal": "CVGIP: Image understanding"
        },
        {
            "id": "b11",
            "title": "Active shape models-their training and application",
            "authors": [
                "T Cootes",
                "C Taylor",
                "D Cooper",
                "J Graham"
            ],
            "date": "1995",
            "journal": "Computer vision and image understanding"
        },
        {
            "id": "b12",
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "authors": [
                "M Cordts",
                "M Omran",
                "S Ramos",
                "T Rehfeld",
                "M Enzweiler",
                "R Benenson",
                "U Franke",
                "S Roth",
                "B Schiele"
            ],
            "date": "2016",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b13",
            "title": "R-fcn: Object detection via region-based fully convolutional networks",
            "authors": [
                "J Dai",
                "Y Li",
                "K He",
                "J Sun"
            ],
            "date": "2016",
            "journal": "Advances in neural information processing systems"
        },
        {
            "id": "b14",
            "title": "Deformable convolutional networks",
            "authors": [
                "J Dai",
                "H Qi",
                "Y Xiong",
                "Y Li",
                "G Zhang",
                "H Hu",
                "Y Wei"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b15",
            "title": "Imagenet: A large-scale hierarchical image database",
            "authors": [
                "J Deng",
                "W Dong",
                "R Socher",
                "L.-J Li",
                "K Li",
                "L Fei-Fei"
            ],
            "date": "2009. 1, 2, 5",
            "journal": "2009 IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b16",
            "title": "Centernet: Keypoint triplets for object detection",
            "authors": [
                "K Duan",
                "S Bai",
                "L Xie",
                "H Qi",
                "Q Huang",
                "Q Tian"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE International Conference on Computer Vision"
        },
        {
            "id": "b17",
            "title": "Corner proposal network for anchor-free, two-stage object detection",
            "authors": [
                "K Duan",
                "L Xie",
                "H Qi",
                "S Bai",
                "Q Huang",
                "Q Tian"
            ],
            "date": "2020",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b18",
            "title": "The pascal visual object classes (voc) challenge",
            "authors": [
                "M Everingham",
                "L Van Gool",
                "C Williams",
                "J Winn",
                "A Zisserman"
            ],
            "date": "2010",
            "journal": "International journal of computer vision"
        },
        {
            "id": "b19",
            "title": "Res2net: A new multi-scale backbone architecture",
            "authors": [
                "S Gao",
                "M.-M Cheng",
                "K Zhao",
                "X.-Y Zhang",
                "M.-H Yang",
                "P Torr"
            ],
            "date": "2019. 1, 2, 5, 6, 7",
            "journal": "IEEE transactions on pattern analysis and machine intelligence"
        },
        {
            "id": "b20",
            "title": "Fast r-cnn",
            "authors": [
                "R Girshick"
            ],
            "date": "2015",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b21",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                "R Girshick",
                "J Donahue",
                "T Darrell",
                "J Malik"
            ],
            "date": "2014",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b22",
            "title": "A robust snake implementation; a dual active contour",
            "authors": [
                "S Gunn",
                "M Nixon"
            ],
            "date": "1997",
            "journal": "IEEE Transactions on pattern analysis and machine intelligence"
        },
        {
            "id": "b23",
            "title": "Mask r-cnn",
            "authors": [
                "K He",
                "G Gkioxari",
                "P Dollár",
                "R Girshick"
            ],
            "date": "2017. 1, 2, 7, 8",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b24",
            "title": "Deep residual learning for image recognition",
            "authors": [
                "K He",
                "X Zhang",
                "S Ren",
                "J Sun"
            ],
            "date": "2016. 1, 2, 3, 5, 6, 7",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b25",
            "title": "Deepercut: A deeper, stronger, and faster multiperson pose estimation model",
            "authors": [
                "E Insafutdinov",
                "L Pishchulin",
                "B Andres",
                "M Andriluka",
                "B Schiele"
            ],
            "date": "2016",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b26",
            "title": "Snakes: Active contour models",
            "authors": [
                "M Kass",
                "A Witkin",
                "D Terzopoulos"
            ],
            "date": "1988",
            "journal": "International journal of computer vision"
        },
        {
            "id": "b27",
            "title": "Probabilistic anchor assignment with iou prediction for object detection",
            "authors": [
                "K Kim",
                "H Lee"
            ],
            "date": "2020",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b28",
            "title": "Foveabox: Beyound anchor-based object detection",
            "authors": [
                "T Kong",
                "F Sun",
                "H Liu",
                "Y Jiang",
                "L Li",
                "J Shi"
            ],
            "date": "2020",
            "journal": "IEEE Transactions on Image Processing"
        },
        {
            "id": "b29",
            "title": "Cornernet: Detecting objects as paired keypoints",
            "authors": [
                "H Law",
                "J Deng"
            ],
            "date": "2018. 1, 2, 6",
            "journal": "European conference on computer vision"
        },
        {
            "id": "b30",
            "title": "",
            "authors": [],
            "date": "2019",
            "journal": "Cornernetlite: Efficient keypoint based object detection"
        },
        {
            "id": "b31",
            "title": "Deep learning",
            "authors": [
                "Y Lecun",
                "Y Bengio",
                "G Hinton"
            ],
            "date": "2015",
            "journal": "nature"
        },
        {
            "id": "b32",
            "title": "Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection",
            "authors": [
                "X Li",
                "W Wang",
                "X Hu",
                "J Li",
                "J Tang",
                "J Yang"
            ],
            "date": "2021",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b33",
            "title": "Feature pyramid networks for object detection",
            "authors": [
                "T.-Y Lin",
                "P Dollár",
                "R Girshick",
                "K He",
                "B Hariharan",
                "S Belongie"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b34",
            "title": "Focal loss for dense object detection",
            "authors": [
                "T.-Y Lin",
                "P Goyal",
                "R Girshick",
                "K He",
                "P Dollár"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b35",
            "title": "Microsoft coco: Common objects in context",
            "authors": [
                "T.-Y Lin",
                "M Maire",
                "S Belongie",
                "J Hays",
                "P Perona",
                "D Ramanan",
                "P Dollár",
                "C Zitnick"
            ],
            "date": "2014. 1, 2, 5, 7",
            "journal": "European conference on computer vision"
        },
        {
            "id": "b36",
            "title": "Improving convolutional networks with self-calibrated convolutions",
            "authors": [
                "J.-J Liu",
                "Q Hou",
                "M.-M Cheng",
                "C Wang",
                "J Feng"
            ],
            "date": "2020",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b37",
            "title": "Ssd: Single shot multibox detector",
            "authors": [
                "W Liu",
                "D Anguelov",
                "D Erhan",
                "C Szegedy",
                "S Reed",
                "C.-Y Fu",
                "A Berg"
            ],
            "date": "2016",
            "journal": "European conference on computer vision"
        },
        {
            "id": "b38",
            "title": "Associative embedding: End-to-end learning for joint detection and grouping",
            "authors": [
                "A Newell",
                "Z Huang",
                "J Deng"
            ],
            "date": "2017. 2, 8",
            "journal": "Advances in neural information processing systems"
        },
        {
            "id": "b39",
            "title": "Stacked hourglass networks for human pose estimation",
            "authors": [
                "A Newell",
                "K Yang",
                "J Deng"
            ],
            "date": "2016. 1, 2, 6, 7",
            "journal": "European conference on computer vision"
        },
        {
            "id": "b40",
            "title": "Libra r-cnn: Towards balanced learning for object detection",
            "authors": [
                "J Pang",
                "K Chen",
                "J Shi",
                "H Feng",
                "W Ouyang",
                "D Lin"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b41",
            "title": "Extreme clicking for efficient object annotation",
            "authors": [
                "D Papadopoulos",
                "J Uijlings",
                "F Keller",
                "V Ferrari"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b42",
            "title": "Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model",
            "authors": [
                "G Papandreou",
                "T Zhu",
                "L.-C Chen",
                "S Gidaris",
                "J Tompson",
                "K Murphy"
            ],
            "date": "2018",
            "journal": "Proceedings of the European Conference on Computer Vision (ECCV)"
        },
        {
            "id": "b43",
            "title": "Deep snake for real-time instance segmentation",
            "authors": [
                "S Peng",
                "W Jiang",
                "H Pi",
                "X Li",
                "H Bao",
                "X Zhou"
            ],
            "date": "2020. 1, 2, 7",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b44",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution"
        },
        {
            "id": "b45",
            "title": "Borderdet: Border feature for dense object detection",
            "authors": [
                "H Qiu",
                "Y Ma",
                "Z Li",
                "S Liu",
                "J Sun"
            ],
            "date": "2020",
            "journal": "European Conference on Computer Vision"
        },
        {
            "id": "b46",
            "title": "You only look once: Unified, real-time object detection",
            "authors": [
                "J Redmon",
                "S Divvala",
                "R Girshick",
                "A Farhadi"
            ],
            "date": "2016",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b47",
            "title": "Yolo9000: better, faster, stronger",
            "authors": [
                "J Redmon",
                "A Farhadi"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b48",
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "authors": [
                "S Ren",
                "K He",
                "R Girshick",
                "J Sun"
            ],
            "date": "2015",
            "journal": "Advances in neural information processing systems"
        },
        {
            "id": "b49",
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "authors": [
                "H Rezatofighi",
                "N Tsoi",
                "J Gwak",
                "A Sadeghian",
                "I Reid",
                "S Savarese"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b50",
            "title": "Deep high-resolution representation learning for human pose estimation",
            "authors": [
                "K Sun",
                "B Xiao",
                "D Liu",
                "J Wang"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b51",
            "title": "Fcos: Fully convolutional one-stage object detection",
            "authors": [
                "Z Tian",
                "C Shen",
                "H Chen",
                "T He"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE International Conference on Computer Vision"
        },
        {
            "id": "b52",
            "title": "Fcos: A simple and strong anchor-free object detector",
            "authors": [
                "Z Tian",
                "C Shen",
                "H Chen",
                "T He"
            ],
            "date": "2020",
            "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
        },
        {
            "id": "b53",
            "title": "Deeppose: Human pose estimation via deep neural networks",
            "authors": [
                "A Toshev",
                "C Szegedy"
            ],
            "date": "2014",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b54",
            "title": "Denet: Scalable realtime object detection with directed sparse sampling",
            "authors": [
                "L Tychsen-Smith",
                "L Petersson"
            ],
            "date": "2017",
            "journal": "Proceedings of the IEEE international conference on computer vision"
        },
        {
            "id": "b55",
            "title": "",
            "authors": [
                "C.-Y Wang",
                "A Bochkovskiy",
                "H.-Y Liao"
            ],
            "date": "2020",
            "journal": "Scaling cross stage partial network"
        },
        {
            "id": "b56",
            "title": "",
            "authors": [],
            "date": "2020",
            "journal": "Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence"
        },
        {
            "id": "b57",
            "title": "Convolutional pose machines",
            "authors": [
                "S.-E Wei",
                "V Ramakrishna",
                "T Kanade",
                "Y Sheikh"
            ],
            "date": "2016",
            "journal": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b58",
            "title": "Polarmask: Single shot instance segmentation with polar representation",
            "authors": [
                "E Xie",
                "P Sun",
                "X Song",
                "W Wang",
                "X Liu",
                "D Liang",
                "C Shen",
                "P Luo"
            ],
            "date": "2020. 1, 2, 3, 7",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b59",
            "title": "Aggregated residual transformations for deep neural networks",
            "authors": [
                "S Xie",
                "R Girshick",
                "P Dollár",
                "Z Tu",
                "K He"
            ],
            "date": "2017. 1, 2, 3, 5, 6, 7",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b60",
            "title": "Reppoints: Point set representation for object detection",
            "authors": [
                "Z Yang",
                "S Liu",
                "H Hu",
                "L Wang",
                "S Lin"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE International Conference on Computer Vision"
        },
        {
            "id": "b61",
            "title": "Deep layer aggregation",
            "authors": [
                "F Yu",
                "D Wang",
                "E Shelhamer",
                "T Darrell"
            ],
            "date": "2018",
            "journal": "Proceedings of the IEEE conference on computer vision and pattern recognition"
        },
        {
            "id": "b62",
            "title": "Unitbox: An advanced object detection network",
            "authors": [
                "J Yu",
                "Y Jiang",
                "Z Wang",
                "Z Cao",
                "T Huang"
            ],
            "date": "2016",
            "journal": "Proceedings of the 24th ACM international conference on Multimedia"
        },
        {
            "id": "b63",
            "title": "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection",
            "authors": [
                "S Zhang",
                "C Chi",
                "Y Yao",
                "Z Lei",
                "S Li"
            ],
            "date": "2020. 3, 5, 6, 7",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b64",
            "title": "Freeanchor: Learning to match anchors for visual object detection",
            "authors": [
                "X Zhang",
                "F Wan",
                "C Liu",
                "R Ji",
                "Q Ye"
            ],
            "date": "2019",
            "journal": "Advances in Neural Information Processing Systems"
        },
        {
            "id": "b65",
            "title": "",
            "authors": [],
            "date": "2019. 1, 2, 3, 8, 9",
            "journal": "Objects as points"
        },
        {
            "id": "b66",
            "title": "Bottom-up object detection by grouping extreme and center points",
            "authors": [
                "X Zhou",
                "J Zhuo",
                "P Krahenbuhl"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b67",
            "title": "",
            "authors": [],
            "date": "2019. 2, 6",
            "journal": "Soft anchorpoint object detection"
        },
        {
            "id": "b68",
            "title": "Feature selective anchorfree module for single-shot object detection",
            "authors": [
                "C Zhu",
                "Y He",
                "M Savvides"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
            "id": "b69",
            "title": "Deformable convnets v2: More deformable, better results",
            "authors": [
                "X Zhu",
                "H Hu",
                "S Lin",
                "J Dai"
            ],
            "date": "2019",
            "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        }
    ]
}