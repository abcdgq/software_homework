{
    "metadata": {
        "title": "FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion",
        "authors": [
            {
                "name": "Fabian Duffhauss",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Ngo Vien",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Hanna Ziesche",
                "affiliations": [
                    {
                        "name": "Bosch Center for Artificial Intelligence",
                        "type": "department"
                    }
                ]
            },
            {
                "name": "Gerhard Neumann",
                "affiliations": [
                    {
                        "name": "Karlsruhe Institute of Technology",
                        "type": "institution"
                    }
                ]
            },
            {
                "name": "{fabian Duffhauss"
            },
            {
                "name": "Anhvien Ngo"
            },
            {
                "name": "Hanna Ziesche}"
            },
            {
                "name": "@ Bosch"
            },
            {
                "name": "Com"
            }
        ],
        "abstract": "Sensor fusion can significantly improve the performance of many computer vision tasks. However, traditional fusion approaches are either not data-driven and cannot exploit prior knowledge nor find regularities in a given dataset or they are restricted to a single application. We overcome this shortcoming by presenting a novel deep hierarchical variational autoencoder called FusionVAE that can serve as a basis for many fusion tasks. Our approach is able to generate diverse image samples that are conditioned on multiple noisy, occluded, or only partially visible input images. We derive and optimize a variational lower bound for the conditional log-likelihood of FusionVAE. In order to assess the fusion capabilities of our model thoroughly, we created three novel datasets for image fusion based on popular computer vision datasets. In our experiments, we show that FusionVAE learns a representation of aggregated information that is relevant to fusion tasks. The results demonstrate that our approach outperforms traditional methods significantly. Furthermore, we present the advantages and disadvantages of different design choices."
    },
    "sections": {
        "Introduction": [
            {
                "title": "Introduction",
                "content": [
                    "Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving [7,30,67], for 6D pose estimation [16,17,61], and for robotic grasping [62,72]. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.",
                    "Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. 1, FusionVAE merges a varying number of input images for reconstructing the original target image using"
                ]
            }
        ],
        "RelatedWork": [
            {
                "title": "Introduction",
                "content": [
                    "Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving [7,30,67], for 6D pose estimation [16,17,61], and for robotic grasping [62,72]. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.",
                    "Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. 1, FusionVAE merges a varying number of input images for reconstructing the original target image using"
                ]
            }
        ],
        "Methodology": [
            {
                "title": "FusionVAE Inputs Target Predictions",
                "content": [
                    "Fig. 1: Overview of our FusionVAE approach. The network receives up to three partly occluded input images, fuses them together with prior knowledge, and predicts different hypothesis of how the target images could look like.",
                    "prior knowledge about the dataset. To the best of our knowledge, FusionVAE is the first approach that combines these two tasks. Therefore, we developed three challenging benchmarks based on well-known computer vision datasets to evaluate the performance of our approach. In addition, we perform comparisons to baselines by extending traditional approaches to perform the same tasks. FusionVAE outperforms all these traditional methods on all proposed benchmark tasks significantly. We show that FusionVAE can generate high-quality images given few input images with partial observability. We provide ablation studies to illustrate the impact of commonly used information aggregation operations and to prove the benefits of the employed posterior distribution. We can summarize the four main contributions of our paper as follows: i) We create three challenging image fusion tasks for generative models. ii) We develop a deep hierarchical VAE called FusionVAE that is able to perform image-based data fusion while employing prior knowledge of the used dataset. iii) We show that FusionVAE produces high-quality fused output images and outperforms traditional methods by a large margin. iv) We perform ablation studies showing the benefits of our design choices regarding both the posterior distribution and commonly used aggregation methods. and expressiveness and thus, when applied to image generation leads to oversmoothed results lacking fine-grained details. Over the last years much work has been invested into the effort of improving the generative performance of VAEs. One stream of work is based on introducing a hierarchy into the latent space of the VAE and scaling this hierarchy to greater and greater depth. First introduced in [54] many hierarchical VAEs are based on coupling the inference and generative processes by introducing a deterministic bottom-up path combined with a stochastic top-down process in the inference network and sharing the latter with the generative model. This setting has been extended by an additional deterministic top-down path and bidirectional inference in [44]. Recently, very deep hierarchical VAEs were realized in [8] by introducing residual bottlenecks with dedicated scaling, update skipping, and nearest neighbour up-sampling. Closest to our work is the recently proposed NVAE architecture [57], which relies on depth-wise convolution, residual posterior parametrization, and spectral regularization to enhance stability.",
                    "Other approaches propose increasing the expressiveness of VAEs by combining them with auto-regressive models like RNNs or PixelCNNs [6,11,13,50], conditioning contexts (CVAE) [53,60], normalizing flows [28], generative adversarial networks (GANs) [31,46], or variational generative adversarial networks (CVAE-GAN) [1]."
                ]
            },
            {
                "title": "Fusion of Multiple Images",
                "content": [
                    "Image fusion has long been dominated by classical computer vision. Only lately deep learning methods entered the domain with the CNN-based approach proposed by Liu et al. [38]. In a subsequent publication the authors extended their work to a multi-scale setting [37]. Shortly afterwards, Prabhakar et al. developed a fusion method based on a siamese network architecture, called DeepFuse [48] which was improved in subsequent work [33] by employing the DenseNet architecture [20]. Concurrently, Li et al. [35] proposed a fusion architecture based on VGG [52] and in order to scale to even greater depth another one [34] based on ResNet-50 [15]. The aforementioned methods use CNNs as feature extractors and as decoders, while the fusion operations themselves are restricted to classical methods like averaging or addition of feature maps or weighted source images. A fully CNN-based feature-map fusion mechanism was proposed in [24].",
                    "While all previous publications target only a specific fusion task (e.g. multifocus fusion, multi-resolution fusion, etc.) or were limited to specific domains (e.g. medical images), two very recent works propose novel multi-purpose fusion networks, which are applicable to many fusion tasks and image types [64,71]. Very recently also GAN-based methods entered the domain of image fusion, starting with the work by Ma et al. on infrared-visible fusion [41,43] and with [42,63] on multi-resolution image fusion. Most recent are two publications on GAN-based multi-focus image fusion [14,21]. While GAN-based approaches can generate high-fidelity images, it is known that they suffer from the mode collapse problem. VAE-based methods in contrast are known to generate more faithful data distribution [57]. Different from previous work, this paper proposes a VAE-based multi-purpose fusion framework."
                ]
            },
            {
                "title": "Image Completion",
                "content": [
                    "Similar to image fusion, also image completion has only recently become a playing field for deep learning methods. First approaches based on simple multilayer perceptrons (MLPs) [29] or CNNs [12] were targeted only to filling small holes in an image. However, with the introduction of GANs [10], the area quickly became dominated by GAN-based approaches, starting with the context encoders presented by Pathak et al. [47]. Many subsequent papers proposed extensions to this model in order to obtain fine-grained completions while preserving global coherence by introducing additional discriminators [22], searching for closest samples to the corrupted image in a latent embedding space conditioning on semantic labels [56], or designing additional specialized loss functions [36]. High resolution results were obtained recently by multi-scale approaches [66], iterative upsampling [70], and the application of contextual attention [55,65,68]. Another stream of current work focuses on multi-hypothesis image completion, leveraging probabilistic problem formulations [45,73]."
                ]
            },
            {
                "title": "Background",
                "content": [
                    "In this section, we outline the fundamentals of standard VAEs, conditional VAEs, and hierarchical VAEs upon which we build our approach. Another section is dedicated to aggregation methods for data fusion."
                ]
            },
            {
                "title": "Standard VAE",
                "content": [
                    "A variational autoencoder (VAE) [27] is a neural network consisting of a probabilistic encoder q(z|y) and a generative model p(y|z). The generator models a distribution over the input data y, conditioned on a latent variable z with prior distribution p θ (z). The encoder approximates the posterior distribution p(z|y) of the latent variables z given input data y and is trained along with the generative model by maximizing the evidence lower bound (ELBO) ELBO(y) = E q(z|y) [log p(y|z)] -KL(q(z|y)||p(z)), (1) where KL is the Kullback-Leibler divergence and log p(y) ≥ ELBO(y)."
                ]
            },
            {
                "title": "Conditional VAE",
                "content": [
                    "In VAEs, the generative model p(y|z) is unconditional. In contrast, conditional VAEs (CVAE) [53] consider a generative model for a conditional distribution p(y|x, z) where y is the target data, x is the conditional input variable, and z is a latent variable. The prior of the latent variable is p(z|x), while its approximate posterior distribution is given by q(z|x, y). The variational lower bound of the conditional log-likelihood can be written as follows log p(y|x) ≥ E q(z|x,y) [log p(y|x, z)] -KL(q(z|x, y)||p(z|x)).",
                    "(2)"
                ]
            },
            {
                "title": "Hierarchical VAE",
                "content": [
                    "In hierarchical VAEs [25,54,57], the latent variables z are divided into L disjoint groups z 1 , ..., z L in order to increase the expressiveness of both prior and approximate posterior which become",
                    "p(z l |z <l ) and q(z|y)",
                    "where z <l denotes the latent variables in all previous hierarchies. All the conditionals in the prior p(z l |z <l ) and in the approximate posterior q(z l |z <l , y) are modeled by factorial Gaussian distributions. Under this modelling choice, the ELBO from Eq. ( 1) turns into"
                ]
            },
            {
                "title": "Aggregation Methods",
                "content": [
                    "For fusing data within our approach, we consider different aggregation methods, such as mean aggregation, max aggregation, Bayesian aggregation [59] and pixelwise addition. All described aggregation methods fuse a set of feature tensors f 1 , ..., f K , obtained by encoding K input images {x i } K i=1 in a permutation invariant way [69]. In mean aggregation, multiple feature vectors are fused by taking the pixel-wise average f = 1 K K i=1 f i . For max aggregation, we take the pixel-wise maximum instead f = max i (f i ). Bayesian aggregation (BA) [59] considers an uncertainty estimate for the fused feature vectors. In order to obtain such an uncertainty estimate, the encoder has to predict means µ i and variances σ i of a factorized Gaussian distribution over the latent feature vectors",
                    "instead of f i directly. Here enc µ and enc σ represent the encoding process which generates means and variances respectively. The predicted distributions over latent feature vectors for multiple input images can be fused iteratively using the Bayes rule [2] (detailed derivation is given in Appendix B)",
                    "where 7) ⊘ and ⊙ denote element-wise division and multiplication respectively."
                ]
            },
            {
                "title": "Conditional Generative Models for Image Fusion",
                "content": [
                    "We propose a deep hierarchical conditional variational autoencoder, called Fusion-VAE (Fusion Variational Auto-Encoder), that is able to fuse information from multiple sources and to infer the missing information in the images from a prior learned from the dataset. To the best of our knowledge, it is the first model that combines the generative ability of a hierarchical VAE to learn the underlying distribution of complex datasets with the ability to fuse multiple input images."
                ]
            },
            {
                "title": "Problem Formulation",
                "content": [
                    "We consider image fusion problems that are concerned with generating the fused target image from multiple source images. Each source image contains partial information of the target image and the goal of the task is to recover the original target image given a finite set of source images. In particular, we denote the target image as y and the set of K source images as context x = {x 1 , x 2 , . . . , x K }, where each x i is one source image. Given training sample (x, y), we aim to maximize the conditional likelihood p(y|x)."
                ]
            },
            {
                "title": "FusionVAE",
                "content": [
                    "Our approach is designed to maximize the conditional likelihood p(y|x). However, optimizing this objective directly is intractable. Therefore, we derive a variational lower bound as follows (detailed derivation can be found in Appendix C)",
                    "where we split the latent variables z into L disjoint groups {z 1 , z 2 , . . . , z L }. β and α l are annealing parameters that control the warming-up of the KL terms as in [57]. Inspired by [54], β is increased linearly from 0 to 1 during the first few training epochs to start training the reconstruction before introducing the KL term, which is increased gradually. α l is a KL balancing coefficient [58] that is used during the warm-up period to encourage the equal use of all latent groups and to avoid posterior collapse. FusionVAE consists of three main networks and a latent space as illustrated in Fig. 2: 1) a context encoder network which models the conditional prior p ϕ (z l |x, z <l ), 2) a target encoder which models the approximate posterior q ϕ (z|y), 3) a latent space comprising the L latent groups, and 4) a generator network p θ (y|x, z) that aims to reconstruct the target image."
                ]
            },
            {
                "title": "Network Architecture",
                "content": [
                    "Fig. 2 illustrates the network architecture of our FusionVAE for training. It is built in a hierarchical way inspired by [57]. In each latent hierarchy l ∈ 1, . . . , L we have a set of feature maps f lx , f ly and latent distributions p l , q l . The first gray box contains the context encoder network that obtains a stack of source images x and employs residual cells [15] as in [57] to extract features f lx . The second gray box shows the target encoder network that encodes the target Fig. 2: Overview of the proposed network architecture. h is a trainable parameter vector, ⊕ denotes concatenation, ⊕ max aggregation, and ⊕ pixel-wise addition.",
                    "r r is a residual network like in [57]. The dotted lines between the residual networks indicate shared parameters.",
                    "image y into the feature map f ly using the same residual cells as the context encoder. The third gray box illustrates the latent space which contains the prior distributions p ϕ (z l |x, z <l ) (denoted p 1 , ..., p L in Fig. 2) and the approximate posterior distributions q ϕ (z l |y, z <l ) (denoted q 1 , ..., q L in Fig. 2). The fourth gray box contains the generator network which aims to create different output samples ŷ. It employs a trainable parameter vector h, concatenates the information from all hierarchies, and decodes them using residual cells.",
                    "In each latent hierarchy, we aggregate the context features f lx using pixelwise max aggregation. In all but the first hierarchy, we pixel-wisely add the corresponding feature map from the generator network to the aggregated context features and to the target image features f ly . Using 2D convolutional layers, we learn the prior distributions p l and the approximate posterior distributions q l . We propose to use the approximate posterior distributions q l as target distributions in order to learn good prior distributions p l . Therefore, q ϕ (z l |y, z <l ) is created from the target image features f ly as well as information from the generator network.",
                    "During training the generator network aims to create a prediction ŷ based on samples of the posterior distributions q l and a trainable parameter vector h.",
                    "For evaluation, we can omit the target image input y and sample from the prior distributions p l . In case no input image is given, we set p 1 to a standard normal distribution. Based on the samples and the trainable parameter vector h, our FusionVAE can generate new output images."
                ]
            }
        ],
        "Experiments": [
            {
                "title": "Experimental Setup",
                "content": [
                    "To evaluate our approach, we conduct a series of experiments on three different datasets using data augmentation. Furthermore, we adapt traditional architec-tures for solving the same tasks in order to compare our results. Finally, we perform an ablation study to show the effects of specific design choices."
                ]
            },
            {
                "title": "Datasets",
                "content": [
                    "For training and evaluating our approach, we create three novel fusion datasets based on MNIST [32], CelebA [39], and T-LESS [18] as described in the following.",
                    "FusionMNIST. Based on the dataset MNIST [32], we create an image fusion dataset called FusionMNIST. For each target image, it contains different noisy representations where only random parts of the target image are visible. The first three columns of Fig. 3 show different examples of FusionMNIST corresponding to the target images in the fourth column. To generate FusionMNIST, we applied zero padding to all MNIST images to obtain a resolution of 32 × 32. For creating a noisy representation, we generate a mask composed of the union of a varying number of ellipses with random size, shape and position. All parts of the given images outside the mask are blackened. Finally, we add Gaussian noise with a fixed variance and clip the pixel values afterwards to stay within [0, 1].",
                    "FusionCelebA. We generate a similar fusion dataset based on the aligned and cropped version of CelebA [39] which we call FusionCelebA. Fig. 4 depicts different example images in the first three columns which belong to the target image in the fourth column. To generate FusionCelebA, we center-crop the CelebA images to 148 × 148 before scaling them down to 64 × 64 as proposed by [31]. As in Fusion-MNIST, we create different representations by using masks based on random ellipses."
                ]
            },
            {
                "title": "FusionT-LESS.",
                "content": [
                    "A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS [18] which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector [5] and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation."
                ]
            },
            {
                "title": "Data Augmentation",
                "content": [
                    "During training we apply different augmentation methods on the datasets to avoid overfitting. For FusionMNIST, we apply the elliptical mask generation and the addition of Gaussian noise live during training so that we obtain an infinite number of different fusion tasks. For FusionT-LESS, almost the entire creation of occluded images is performed during training. We apply horizontal flips, rotations, scaling and movement of target and occluding images with random parameters before composing the different occluded representations. Solely the object cutting with the Canny edge detector is performed offline as a pre-processing step to keep the training time low. For FusionCelebA, we apply a horizontal flip of all images randomly in 50% of all occasions and also the elliptical mask generation is done live during training."
                ]
            },
            {
                "title": "Architectures for Comparison",
                "content": [
                    "To the best of our knowledge, FusionVAE is the first fusion network for multiple images with a generative ability to fill areas without input information based on prior knowledge about the dataset under consideration. For lack of a suitable other model from the literature which would allow a fair comparison on our multi-image fusion tasks, we compare our approach with standard architectures that we adapted to support our tasks.",
                    "The first architecture for comparison is a CVAE with residual layers as employed in [57]. We use a shared encoder for processing the input images and applied max aggregation before the latent space as we did in our FusionVAE. The second architecture for comparison is a fully convolutional network (FCN) with shared encoder and max aggregation before the decoder.",
                    "For both baseline architectures, we created a version with skip connections (+S) and a version without. When using skip connections, we applied max aggregation at each shortcut for merging the features from the encoder with the decoder's features. To allow for a fair comparison, we designed all architectures so that they have a similar number of trainable parameters."
                ]
            },
            {
                "title": "Training Procedure",
                "content": [
                    "We trained all networks in a supervised manner using the augmented target images y as described in Sec. 5.2. In order to teach the networks both to fuse information from a different number of input images and to learn prior knowledge about the dataset, we vary the number of input images x during the entire training. Specifically, we select a uniformly distributed random number between zero and three for each batch."
                ]
            },
            {
                "title": "Evaluation Metrics",
                "content": [
                    "For evaluation, we estimate the negative log-likelihood (NLL) in bits per dimension (BPD) using weighted importance sampling [4]. We use 100 samples for all experiments with FusionCelebA as well as FusionT-LESS and 1000 samples for FusionMNIST. Since we cannot estimate the NLL of the FCN, we used the minimum over all samples of the mean squared error (MSE min ) as second metric."
                ]
            },
            {
                "title": "Results",
                "content": [
                    "This section presents and discusses the quantitative and qualitative results of our research in comparison to the baseline methods mentioned in Sec. 5.3."
                ]
            },
            {
                "title": "Quantitative Results",
                "content": [
                    "Tabs. 1 to 3 show the NLL and the MSE min of all architectures on FusionMNIST, FusionCelebA, and FusionT-LESS respectively. The results are divided into the results based on zero to three input images and the average (avg) of it. We see that our FusionVAE outperforms all baseline methods on average. Regarding the NLL, our model surpasses the others additionally for 0 and 1 input images. For 2 and 3 images, CVAE+S reaches sometimes slightly better NLL values. However, our approach reaches the best MSE min values for each number of input images."
                ]
            },
            {
                "title": "Qualitative Results",
                "content": [
                    "The outstanding performance of our architecture in comparison to the others is also obvious when looking at the qualitative results in Figs. 3 to 5. For every row, these figures show the input, target, and up to three output predictions for all architectures. For the FCN, we depict just a single output prediction per row as all of them look almost identical.",
                    "In the first three rows when the network does not receive any input image, we see that our network provides very realistic images. This indicates that it is able to capture the underlying distribution of the used datasets very well and much better than the other architectures. Due to the difficulty of the FusionT-LESS dataset, none of the models is able to produce realistic images without any input. Still our model shows much better performance in generating object-like shapes. In case the models receive at least one input image (cf. rows 4 -12), all architectures are able to extract the available information from the given input images. In addition, all VAE approaches, ours included, are able to complete the given input data based on prior knowledge. It is clearly visible, however, that the predictions of our model are much more realistic than the ones of the standard CVAE approaches especially for the more difficult datasets like FusionCelebA and FusionT-LESS. Fig. 5: Predictions on FusionT-LESS for zero to three input images."
                ]
            },
            {
                "title": "Ablation Studies",
                "content": [
                    "We conducted ablation studies to show the effect of certain design choices, such as the selection of the approximate posterior and the aggregation method. All experiments are run on the FusionCelebA dataset. Tab. 4 compares the performance of our FusionVAE for two different approximate posterior distributions q. The approximate posterior we selected for our FusionVAE q(y), depends only on the given target image y. It performs slightly better on average compared to the same method using a posterior that is computed based on the input images x as well as the target image y. However, the latter approach is superior when fusing two or three input images.   Tab. 5 shows the performance of different aggregation methods which are applied to create the prior distributions p l of every latent group. In our Fusion-VAE, the prior is created by fusing the input image features f lx using max aggregation (MaxAgg) and adding them to the decoded features of the same latent group before applying a 2D convolution. We abbreviate that method with MaxAggAdd.",
                    "In addition to MaxAgg, we examined mean aggregation (MeanAgg) and Bayesian aggregation (BayAgg) [59] for comparison. For each aggregation principle, we tried two different versions: 1) aggregation of the input image features f lx adding the corresponding information from the decoder in a pixel-wise manner (denoted by suffix Add), and 2) directly aggregating all features, i.e. both input image features f lx and decoder features (denoted by suffix All).",
                    "For creating the prior p i when using BayAgg, we moved the 2D convolutions before the aggregation in order to create the parameters µ and σ of a latent Gaussian distribution. Unlike MaxAgg and MeanAgg, BayAgg directly outputs a new Gaussian distribution that does not need to be processed any further by a convolution.",
                    "We can see that all variations of mean and max aggregation are significantly better than Bayesian aggregation. Also their training procedures are less often impaired due to numeric instabilities. Interestingly, the NLL is very similar independent of whether the aggregation is performed on all features or not. However, the MSE min is much better for the aggregation with addition. Since the expressiveness of the metrics is limited, we provide additional visualizations of this ablation in Appendix D."
                ]
            }
        ],
        "Conclusion": [
            {
                "title": "Conclusion",
                "content": [
                    "We have presented a novel deep hierarchical variational autoencoder for generative image fusion called FusionVAE. Our approach fuses multiple corrupted input images together with prior knowledge obtained during training. We created three challenging image fusion benchmarks based on common computer vision datasets. Moreover, we implemented four standard methods that we modified to support our tasks. We showed that our FusionVAE outperforms all other methods significantly while having a similar number of trainable parameters. The predicted images of our approach look very realistic and incorporate given input information almost perfectly. During ablation studies, we revealed the benefits of our design choices regarding the applied aggregation method and the used posterior distribution. In future work, our research could be extended by enabling the fusion of different modalities e.g. by using multiple encoders. Additionally, an explicit uncertainty estimation could be implemented that helps to weigh the impact of input information according to its uncertainty."
                ]
            }
        ]
    }
}