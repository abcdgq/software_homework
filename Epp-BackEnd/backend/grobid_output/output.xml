<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-09-22">22 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Duffhauss</surname></persName>
							<idno type="ORCID">0000-0002-8910-3852</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ngo</forename><forename type="middle">Anh</forename><surname>Vien</surname></persName>
							<idno type="ORCID">0000-0001-9646-267X</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Ziesche</surname></persName>
							<idno type="ORCID">0000-0003-2042-3660</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
							<email>gerhard.neumann@kit.edu</email>
							<idno type="ORCID">0000-0002-5483-4225</idno>
							<affiliation key="aff2">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">{fabian</forename><surname>Duffhauss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anhvien</forename><surname>Ngo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Ziesche}</surname></persName>
						</author>
						<author>
							<persName><forename type="first">@</forename><surname>Bosch</surname></persName>
						</author>
						<author>
							<persName><surname>Com</surname></persName>
						</author>
						<title level="a" type="main">FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-22">22 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">1E6659617709632A94D0B1B570F2644F</idno>
					<idno type="arXiv">arXiv:2209.11277v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-05-14T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensor fusion can significantly improve the performance of many computer vision tasks. However, traditional fusion approaches are either not data-driven and cannot exploit prior knowledge nor find regularities in a given dataset or they are restricted to a single application. We overcome this shortcoming by presenting a novel deep hierarchical variational autoencoder called FusionVAE that can serve as a basis for many fusion tasks. Our approach is able to generate diverse image samples that are conditioned on multiple noisy, occluded, or only partially visible input images. We derive and optimize a variational lower bound for the conditional log-likelihood of FusionVAE. In order to assess the fusion capabilities of our model thoroughly, we created three novel datasets for image fusion based on popular computer vision datasets. In our experiments, we show that FusionVAE learns a representation of aggregated information that is relevant to fusion tasks. The results demonstrate that our approach outperforms traditional methods significantly. Furthermore, we present the advantages and disadvantages of different design choices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b65">67]</ref>, for 6D pose estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">61]</ref>, and for robotic grasping <ref type="bibr" target="#b60">[62,</ref><ref type="bibr">72]</ref>. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.</p><p>Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. <ref type="figure">1</ref>, FusionVAE merges a varying number of input images for reconstructing the original target image using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FusionVAE Inputs Target Predictions</head><p>Fig. <ref type="figure">1</ref>: Overview of our FusionVAE approach. The network receives up to three partly occluded input images, fuses them together with prior knowledge, and predicts different hypothesis of how the target images could look like.</p><p>prior knowledge about the dataset. To the best of our knowledge, FusionVAE is the first approach that combines these two tasks. Therefore, we developed three challenging benchmarks based on well-known computer vision datasets to evaluate the performance of our approach. In addition, we perform comparisons to baselines by extending traditional approaches to perform the same tasks. FusionVAE outperforms all these traditional methods on all proposed benchmark tasks significantly. We show that FusionVAE can generate high-quality images given few input images with partial observability. We provide ablation studies to illustrate the impact of commonly used information aggregation operations and to prove the benefits of the employed posterior distribution. We can summarize the four main contributions of our paper as follows: i) We create three challenging image fusion tasks for generative models. ii) We develop a deep hierarchical VAE called FusionVAE that is able to perform image-based data fusion while employing prior knowledge of the used dataset. iii) We show that FusionVAE produces high-quality fused output images and outperforms traditional methods by a large margin. iv) We perform ablation studies showing the benefits of our design choices regarding both the posterior distribution and commonly used aggregation methods. and expressiveness and thus, when applied to image generation leads to oversmoothed results lacking fine-grained details. Over the last years much work has been invested into the effort of improving the generative performance of VAEs. One stream of work is based on introducing a hierarchy into the latent space of the VAE and scaling this hierarchy to greater and greater depth. First introduced in <ref type="bibr" target="#b52">[54]</ref> many hierarchical VAEs are based on coupling the inference and generative processes by introducing a deterministic bottom-up path combined with a stochastic top-down process in the inference network and sharing the latter with the generative model. This setting has been extended by an additional deterministic top-down path and bidirectional inference in <ref type="bibr" target="#b42">[44]</ref>. Recently, very deep hierarchical VAEs were realized in <ref type="bibr" target="#b7">[8]</ref> by introducing residual bottlenecks with dedicated scaling, update skipping, and nearest neighbour up-sampling. Closest to our work is the recently proposed NVAE architecture <ref type="bibr" target="#b55">[57]</ref>, which relies on depth-wise convolution, residual posterior parametrization, and spectral regularization to enhance stability.</p><p>Other approaches propose increasing the expressiveness of VAEs by combining them with auto-regressive models like RNNs or PixelCNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">50]</ref>, conditioning contexts (CVAE) <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b58">60]</ref>, normalizing flows <ref type="bibr" target="#b27">[28]</ref>, generative adversarial networks (GANs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">46]</ref>, or variational generative adversarial networks (CVAE-GAN) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fusion of Multiple Images</head><p>Image fusion has long been dominated by classical computer vision. Only lately deep learning methods entered the domain with the CNN-based approach proposed by Liu et al. <ref type="bibr" target="#b36">[38]</ref>. In a subsequent publication the authors extended their work to a multi-scale setting <ref type="bibr" target="#b35">[37]</ref>. Shortly afterwards, Prabhakar et al. developed a fusion method based on a siamese network architecture, called DeepFuse <ref type="bibr" target="#b46">[48]</ref> which was improved in subsequent work <ref type="bibr" target="#b31">[33]</ref> by employing the DenseNet architecture <ref type="bibr" target="#b19">[20]</ref>. Concurrently, Li et al. <ref type="bibr" target="#b33">[35]</ref> proposed a fusion architecture based on VGG <ref type="bibr" target="#b50">[52]</ref> and in order to scale to even greater depth another one <ref type="bibr" target="#b32">[34]</ref> based on ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. The aforementioned methods use CNNs as feature extractors and as decoders, while the fusion operations themselves are restricted to classical methods like averaging or addition of feature maps or weighted source images. A fully CNN-based feature-map fusion mechanism was proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>While all previous publications target only a specific fusion task (e.g. multifocus fusion, multi-resolution fusion, etc.) or were limited to specific domains (e.g. medical images), two very recent works propose novel multi-purpose fusion networks, which are applicable to many fusion tasks and image types <ref type="bibr" target="#b62">[64,</ref><ref type="bibr">71]</ref>. Very recently also GAN-based methods entered the domain of image fusion, starting with the work by Ma et al. on infrared-visible fusion <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b41">43]</ref> and with <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b61">63]</ref> on multi-resolution image fusion. Most recent are two publications on GAN-based multi-focus image fusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. While GAN-based approaches can generate high-fidelity images, it is known that they suffer from the mode collapse problem. VAE-based methods in contrast are known to generate more faithful data distribution <ref type="bibr" target="#b55">[57]</ref>. Different from previous work, this paper proposes a VAE-based multi-purpose fusion framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Completion</head><p>Similar to image fusion, also image completion has only recently become a playing field for deep learning methods. First approaches based on simple multilayer perceptrons (MLPs) <ref type="bibr" target="#b28">[29]</ref> or CNNs <ref type="bibr" target="#b11">[12]</ref> were targeted only to filling small holes in an image. However, with the introduction of GANs <ref type="bibr" target="#b9">[10]</ref>, the area quickly became dominated by GAN-based approaches, starting with the context encoders presented by Pathak et al. <ref type="bibr" target="#b45">[47]</ref>. Many subsequent papers proposed extensions to this model in order to obtain fine-grained completions while preserving global coherence by introducing additional discriminators <ref type="bibr" target="#b21">[22]</ref>, searching for closest samples to the corrupted image in a latent embedding space conditioning on semantic labels <ref type="bibr" target="#b54">[56]</ref>, or designing additional specialized loss functions <ref type="bibr" target="#b34">[36]</ref>. High resolution results were obtained recently by multi-scale approaches <ref type="bibr" target="#b64">[66]</ref>, iterative upsampling [70], and the application of contextual attention <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b66">68]</ref>. Another stream of current work focuses on multi-hypothesis image completion, leveraging probabilistic problem formulations <ref type="bibr" target="#b43">[45,</ref><ref type="bibr">73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we outline the fundamentals of standard VAEs, conditional VAEs, and hierarchical VAEs upon which we build our approach. Another section is dedicated to aggregation methods for data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard VAE</head><p>A variational autoencoder (VAE) <ref type="bibr" target="#b26">[27]</ref> is a neural network consisting of a probabilistic encoder q(z|y) and a generative model p(y|z). The generator models a distribution over the input data y, conditioned on a latent variable z with prior distribution p θ (z). The encoder approximates the posterior distribution p(z|y) of the latent variables z given input data y and is trained along with the generative model by maximizing the evidence lower bound (ELBO) ELBO(y) = E q(z|y) [log p(y|z)] -KL(q(z|y)||p(z)), <ref type="bibr" target="#b0">(1)</ref> where KL is the Kullback-Leibler divergence and log p(y) ≥ ELBO(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional VAE</head><p>In VAEs, the generative model p(y|z) is unconditional. In contrast, conditional VAEs (CVAE) <ref type="bibr" target="#b51">[53]</ref> consider a generative model for a conditional distribution p(y|x, z) where y is the target data, x is the conditional input variable, and z is a latent variable. The prior of the latent variable is p(z|x), while its approximate posterior distribution is given by q(z|x, y). The variational lower bound of the conditional log-likelihood can be written as follows log p(y|x) ≥ E q(z|x,y) [log p(y|x, z)] -KL(q(z|x, y)||p(z|x)).</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical VAE</head><p>In hierarchical VAEs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b55">57]</ref>, the latent variables z are divided into L disjoint groups z 1 , ..., z L in order to increase the expressiveness of both prior and approximate posterior which become</p><formula xml:id="formula_0">p(z) = L l=1</formula><p>p(z l |z &lt;l ) and q(z|y)</p><formula xml:id="formula_1">= L l=1 q(z l |z &lt;l , y),<label>(3)</label></formula><p>where z &lt;l denotes the latent variables in all previous hierarchies. All the conditionals in the prior p(z l |z &lt;l ) and in the approximate posterior q(z l |z &lt;l , y) are modeled by factorial Gaussian distributions. Under this modelling choice, the ELBO from Eq. ( <ref type="formula">1</ref>) turns into</p><formula xml:id="formula_2">ELBO(y) = E q(z|y) [log p(y|z)] - L l=1 E q(z &lt;l |y) [KL(q(z l |z &lt;l , y)||p(z l |z &lt;l ))]. (<label>4</label></formula><formula xml:id="formula_3">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Aggregation Methods</head><p>For fusing data within our approach, we consider different aggregation methods, such as mean aggregation, max aggregation, Bayesian aggregation <ref type="bibr" target="#b57">[59]</ref> and pixelwise addition. All described aggregation methods fuse a set of feature tensors f 1 , ..., f K , obtained by encoding K input images {x i } K i=1 in a permutation invariant way <ref type="bibr">[69]</ref>. In mean aggregation, multiple feature vectors are fused by taking the pixel-wise average f = 1 K K i=1 f i . For max aggregation, we take the pixel-wise maximum instead f = max i (f i ). Bayesian aggregation (BA) <ref type="bibr" target="#b57">[59]</ref> considers an uncertainty estimate for the fused feature vectors. In order to obtain such an uncertainty estimate, the encoder has to predict means µ i and variances σ i of a factorized Gaussian distribution over the latent feature vectors</p><formula xml:id="formula_4">f i = N (µ i , diag(σ i )), with µ i = enc µ (x i , y) and σ i = enc σ (x i , y),<label>(5)</label></formula><p>instead of f i directly. Here enc µ and enc σ represent the encoding process which generates means and variances respectively. The predicted distributions over latent feature vectors for multiple input images can be fused iteratively using the Bayes rule <ref type="bibr" target="#b1">[2]</ref> (detailed derivation is given in Appendix B)</p><formula xml:id="formula_5">q i = σ 2 i-1 ⊘ (σ 2 i-1 + σ 2 i )<label>(6)</label></formula><p>where <ref type="formula">7</ref>) ⊘ and ⊙ denote element-wise division and multiplication respectively.</p><formula xml:id="formula_6">µ i = µ i-1 + q i ⊙ (µ i -µ i-1 ) and σ 2 i = σ 2 i-1 ⊙ (1 -q i ). (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conditional Generative Models for Image Fusion</head><p>We propose a deep hierarchical conditional variational autoencoder, called Fusion-VAE (Fusion Variational Auto-Encoder), that is able to fuse information from multiple sources and to infer the missing information in the images from a prior learned from the dataset. To the best of our knowledge, it is the first model that combines the generative ability of a hierarchical VAE to learn the underlying distribution of complex datasets with the ability to fuse multiple input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>We consider image fusion problems that are concerned with generating the fused target image from multiple source images. Each source image contains partial information of the target image and the goal of the task is to recover the original target image given a finite set of source images. In particular, we denote the target image as y and the set of K source images as context x = {x 1 , x 2 , . . . , x K }, where each x i is one source image. Given training sample (x, y), we aim to maximize the conditional likelihood p(y|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FusionVAE</head><p>Our approach is designed to maximize the conditional likelihood p(y|x). However, optimizing this objective directly is intractable. Therefore, we derive a variational lower bound as follows (detailed derivation can be found in Appendix C)</p><formula xml:id="formula_7">log p(y|x) ≥ E q ϕ (z|y) [log p θ (y|x, z)] -β L l=1 α l E q(z &lt;l |y) [KL(q ϕ (z l |y, z &lt;l )||p θ (z l |x, z &lt;l ))],<label>(8)</label></formula><p>where we split the latent variables z into L disjoint groups {z 1 , z 2 , . . . , z L }. β and α l are annealing parameters that control the warming-up of the KL terms as in <ref type="bibr" target="#b55">[57]</ref>. Inspired by <ref type="bibr" target="#b52">[54]</ref>, β is increased linearly from 0 to 1 during the first few training epochs to start training the reconstruction before introducing the KL term, which is increased gradually. α l is a KL balancing coefficient <ref type="bibr" target="#b56">[58]</ref> that is used during the warm-up period to encourage the equal use of all latent groups and to avoid posterior collapse. FusionVAE consists of three main networks and a latent space as illustrated in Fig. <ref type="figure">2:</ref> 1) a context encoder network which models the conditional prior p ϕ (z l |x, z &lt;l ), 2) a target encoder which models the approximate posterior q ϕ (z|y), 3) a latent space comprising the L latent groups, and 4) a generator network p θ (y|x, z) that aims to reconstruct the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Architecture</head><p>Fig. <ref type="figure">2</ref> illustrates the network architecture of our FusionVAE for training. It is built in a hierarchical way inspired by <ref type="bibr" target="#b55">[57]</ref>. In each latent hierarchy l ∈ 1, . . . , L we have a set of feature maps f lx , f ly and latent distributions p l , q l . The first gray box contains the context encoder network that obtains a stack of source images x and employs residual cells <ref type="bibr" target="#b14">[15]</ref> as in <ref type="bibr" target="#b55">[57]</ref> to extract features f lx . The second gray box shows the target encoder network that encodes the target Fig. <ref type="figure">2</ref>: Overview of the proposed network architecture. h is a trainable parameter vector, ⊕ denotes concatenation, ⊕ max aggregation, and ⊕ pixel-wise addition.</p><p>r r is a residual network like in <ref type="bibr" target="#b55">[57]</ref>. The dotted lines between the residual networks indicate shared parameters.</p><p>image y into the feature map f ly using the same residual cells as the context encoder. The third gray box illustrates the latent space which contains the prior distributions p ϕ (z l |x, z &lt;l ) (denoted p 1 , ..., p L in Fig. <ref type="figure">2</ref>) and the approximate posterior distributions q ϕ (z l |y, z &lt;l ) (denoted q 1 , ..., q L in Fig. <ref type="figure">2</ref>). The fourth gray box contains the generator network which aims to create different output samples ŷ. It employs a trainable parameter vector h, concatenates the information from all hierarchies, and decodes them using residual cells.</p><p>In each latent hierarchy, we aggregate the context features f lx using pixelwise max aggregation. In all but the first hierarchy, we pixel-wisely add the corresponding feature map from the generator network to the aggregated context features and to the target image features f ly . Using 2D convolutional layers, we learn the prior distributions p l and the approximate posterior distributions q l . We propose to use the approximate posterior distributions q l as target distributions in order to learn good prior distributions p l . Therefore, q ϕ (z l |y, z &lt;l ) is created from the target image features f ly as well as information from the generator network.</p><p>During training the generator network aims to create a prediction ŷ based on samples of the posterior distributions q l and a trainable parameter vector h.</p><p>For evaluation, we can omit the target image input y and sample from the prior distributions p l . In case no input image is given, we set p 1 to a standard normal distribution. Based on the samples and the trainable parameter vector h, our FusionVAE can generate new output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>To evaluate our approach, we conduct a series of experiments on three different datasets using data augmentation. Furthermore, we adapt traditional architec-tures for solving the same tasks in order to compare our results. Finally, we perform an ablation study to show the effects of specific design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For training and evaluating our approach, we create three novel fusion datasets based on MNIST [32], CelebA <ref type="bibr" target="#b37">[39]</ref>, and T-LESS <ref type="bibr" target="#b17">[18]</ref> as described in the following.</p><p>FusionMNIST. Based on the dataset MNIST [32], we create an image fusion dataset called FusionMNIST. For each target image, it contains different noisy representations where only random parts of the target image are visible. The first three columns of Fig. <ref type="figure">3</ref> show different examples of FusionMNIST corresponding to the target images in the fourth column. To generate FusionMNIST, we applied zero padding to all MNIST images to obtain a resolution of 32 × 32. For creating a noisy representation, we generate a mask composed of the union of a varying number of ellipses with random size, shape and position. All parts of the given images outside the mask are blackened. Finally, we add Gaussian noise with a fixed variance and clip the pixel values afterwards to stay within [0, 1].</p><p>FusionCelebA. We generate a similar fusion dataset based on the aligned and cropped version of CelebA <ref type="bibr" target="#b37">[39]</ref> which we call FusionCelebA. Fig. <ref type="figure">4</ref> depicts different example images in the first three columns which belong to the target image in the fourth column. To generate FusionCelebA, we center-crop the CelebA images to 148 × 148 before scaling them down to 64 × 64 as proposed by <ref type="bibr" target="#b30">[31]</ref>. As in Fusion-MNIST, we create different representations by using masks based on random ellipses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FusionT-LESS.</head><p>A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS <ref type="bibr" target="#b17">[18]</ref> which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector <ref type="bibr" target="#b4">[5]</ref> and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Augmentation</head><p>During training we apply different augmentation methods on the datasets to avoid overfitting. For FusionMNIST, we apply the elliptical mask generation and the addition of Gaussian noise live during training so that we obtain an infinite number of different fusion tasks. For FusionT-LESS, almost the entire creation of occluded images is performed during training. We apply horizontal flips, rotations, scaling and movement of target and occluding images with random parameters before composing the different occluded representations. Solely the object cutting with the Canny edge detector is performed offline as a pre-processing step to keep the training time low. For FusionCelebA, we apply a horizontal flip of all images randomly in 50% of all occasions and also the elliptical mask generation is done live during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Architectures for Comparison</head><p>To the best of our knowledge, FusionVAE is the first fusion network for multiple images with a generative ability to fill areas without input information based on prior knowledge about the dataset under consideration. For lack of a suitable other model from the literature which would allow a fair comparison on our multi-image fusion tasks, we compare our approach with standard architectures that we adapted to support our tasks.</p><p>The first architecture for comparison is a CVAE with residual layers as employed in <ref type="bibr" target="#b55">[57]</ref>. We use a shared encoder for processing the input images and applied max aggregation before the latent space as we did in our FusionVAE. The second architecture for comparison is a fully convolutional network (FCN) with shared encoder and max aggregation before the decoder.</p><p>For both baseline architectures, we created a version with skip connections (+S) and a version without. When using skip connections, we applied max aggregation at each shortcut for merging the features from the encoder with the decoder's features. To allow for a fair comparison, we designed all architectures so that they have a similar number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Procedure</head><p>We trained all networks in a supervised manner using the augmented target images y as described in Sec. 5.2. In order to teach the networks both to fuse information from a different number of input images and to learn prior knowledge about the dataset, we vary the number of input images x during the entire training. Specifically, we select a uniformly distributed random number between zero and three for each batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation Metrics</head><p>For evaluation, we estimate the negative log-likelihood (NLL) in bits per dimension (BPD) using weighted importance sampling <ref type="bibr" target="#b3">[4]</ref>. We use 100 samples for all experiments with FusionCelebA as well as FusionT-LESS and 1000 samples for FusionMNIST. Since we cannot estimate the NLL of the FCN, we used the minimum over all samples of the mean squared error (MSE min ) as second metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>This section presents and discusses the quantitative and qualitative results of our research in comparison to the baseline methods mentioned in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative Results</head><p>Tabs. 1 to 3 show the NLL and the MSE min of all architectures on FusionMNIST, FusionCelebA, and FusionT-LESS respectively. The results are divided into the results based on zero to three input images and the average (avg) of it. We see that our FusionVAE outperforms all baseline methods on average. Regarding the NLL, our model surpasses the others additionally for 0 and 1 input images. For 2 and 3 images, CVAE+S reaches sometimes slightly better NLL values. However, our approach reaches the best MSE min values for each number of input images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative Results</head><p>The outstanding performance of our architecture in comparison to the others is also obvious when looking at the qualitative results in Figs. <ref type="figure">3 to 5</ref>. For every row, these figures show the input, target, and up to three output predictions for all architectures. For the FCN, we depict just a single output prediction per row as all of them look almost identical.</p><p>In the first three rows when the network does not receive any input image, we see that our network provides very realistic images. This indicates that it is able to capture the underlying distribution of the used datasets very well and much better than the other architectures. Due to the difficulty of the FusionT-LESS dataset, none of the models is able to produce realistic images without any input. Still our model shows much better performance in generating object-like shapes. In case the models receive at least one input image (cf. rows 4 -12), all architectures are able to extract the available information from the given input images. In addition, all VAE approaches, ours included, are able to complete the given input data based on prior knowledge. It is clearly visible, however, that the predictions of our model are much more realistic than the ones of the standard CVAE approaches especially for the more difficult datasets like FusionCelebA and FusionT-LESS. Fig. <ref type="figure">5</ref>: Predictions on FusionT-LESS for zero to three input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>We conducted ablation studies to show the effect of certain design choices, such as the selection of the approximate posterior and the aggregation method. All experiments are run on the FusionCelebA dataset. Tab. 4 compares the performance of our FusionVAE for two different approximate posterior distributions q. The approximate posterior we selected for our FusionVAE q(y), depends only on the given target image y. It performs slightly better on average compared to the same method using a posterior that is computed based on the input images x as well as the target image y. However, the latter approach is superior when fusing two or three input images.   Tab. 5 shows the performance of different aggregation methods which are applied to create the prior distributions p l of every latent group. In our Fusion-VAE, the prior is created by fusing the input image features f lx using max aggregation (MaxAgg) and adding them to the decoded features of the same latent group before applying a 2D convolution. We abbreviate that method with MaxAggAdd.</p><p>In addition to MaxAgg, we examined mean aggregation (MeanAgg) and Bayesian aggregation (BayAgg) <ref type="bibr" target="#b57">[59]</ref> for comparison. For each aggregation principle, we tried two different versions: 1) aggregation of the input image features f lx adding the corresponding information from the decoder in a pixel-wise manner (denoted by suffix Add), and 2) directly aggregating all features, i.e. both input image features f lx and decoder features (denoted by suffix All).</p><p>For creating the prior p i when using BayAgg, we moved the 2D convolutions before the aggregation in order to create the parameters µ and σ of a latent Gaussian distribution. Unlike MaxAgg and MeanAgg, BayAgg directly outputs a new Gaussian distribution that does not need to be processed any further by a convolution.</p><p>We can see that all variations of mean and max aggregation are significantly better than Bayesian aggregation. Also their training procedures are less often impaired due to numeric instabilities. Interestingly, the NLL is very similar independent of whether the aggregation is performed on all features or not. However, the MSE min is much better for the aggregation with addition. Since the expressiveness of the metrics is limited, we provide additional visualizations of this ablation in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a novel deep hierarchical variational autoencoder for generative image fusion called FusionVAE. Our approach fuses multiple corrupted input images together with prior knowledge obtained during training. We created three challenging image fusion benchmarks based on common computer vision datasets. Moreover, we implemented four standard methods that we modified to support our tasks. We showed that our FusionVAE outperforms all other methods significantly while having a similar number of trainable parameters. The predicted images of our approach look very realistic and incorporate given input information almost perfectly. During ablation studies, we revealed the benefits of our design choices regarding the applied aggregation method and the used posterior distribution. In future work, our research could be extended by enabling the fusion of different modalities e.g. by using multiple encoders. Additionally, an explicit uncertainty estimation could be implemented that helps to weigh the impact of input information according to its uncertainty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details</head><p>For FusionMNIST and FusionT-LESS, we model the decoder's output by pixelwise independent Bernoulli distributions. For FusionCelebA, we use pixel-wise independent discretized logistic mixture distributions as proposed by Salimans et al. <ref type="bibr" target="#b49">[51]</ref>.</p><p>The residual cells of the encoder are composed of batch normalization layers <ref type="bibr" target="#b22">[23]</ref>, Swish activation functions <ref type="bibr" target="#b47">[49]</ref>, convolutional layers, and Squeeze-and-Excitation (SE) blocks <ref type="bibr" target="#b18">[19]</ref> as proposed in <ref type="bibr" target="#b55">[57]</ref>. In the decoder, we also follow <ref type="bibr" target="#b55">[57]</ref> and build the residual cells out of batch normalization layers, 1x1 convolutions, Swish activations, depthwise separable convolutions <ref type="bibr" target="#b8">[9]</ref>, and SE blocks. However, we omitted normalizing flow because in our experiments it showed to increase the training time without improving the prediction accuracy significantly.</p><p>For each dataset, we chose the size of the architecture individually to achieve acceptable accuracy while keeping the training time reasonable. Tab. 6 provides details about the used hyperparameters.  In general, the number of latent groups L should be chosen depending on the complexity of the task at hand. We made our decision based on the L of the NVAE <ref type="bibr" target="#b55">[57]</ref> but reduced it for computational reasons. For FusionCelebA and FusionT-LESS, we use 17 latent groups, for FusionMNIST only seven. Using more latent groups improves the results but increases the computational effort significantly.</p><p>For all experiments, we used GPUs of type NVIDIA Tesla V100 with 32GB of memory and trained with an AdaMax optimizer <ref type="bibr" target="#b25">[26]</ref>. We applied a cosine annealing schedule for the learning rate <ref type="bibr" target="#b38">[40]</ref> starting at 0.01 and ending at 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation of Bayesian Aggregation</head><p>We use two related encoders to learn a latent observation µ i = enc µ (x i , y) with its corresponding variance values σ i = enc σ (x i , y).</p><p>Assuming a factorized Gaussian prior distribution in the latent space p(z) = N (z|µ z,0 , diag(σ z,0 )), we can derive the factorized posterior distribution q ϕ (z|y) = N (z|µ z , diag(σ z ) in closed form using standard Gaussian conditioning <ref type="bibr" target="#b2">[3]</ref> following <ref type="bibr" target="#b57">[59]</ref> σ</p><formula xml:id="formula_8">2 z = (σ 2 z,0 ) ⊖ + (σ 2 i ) ⊖ ⊖ ,<label>(9)</label></formula><formula xml:id="formula_9">µ z = µ z,0 + σ 2 z,0 ⊙ (µ i -µ z,0 ) ⊘ σ 2 i (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where ⊖ denotes element-wise inversion, ⊙ denotes element-wise multiplication, and ⊘ denotes element-wise division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Derivation of FusionVAE's ELBO</head><p>We start with the following KL divergence between the approximate posterior and the real posterior, KL(q θ (z|y)||p θ (z|x, y)) ≥ 0.</p><p>Next, we apply the Bayes's theorem to obtain -q θ (z|y) log p(y|x, z)p(z|x) p(y|x)q θ (z|y) dz ≥ 0.</p><p>This leads to -E q θ (z|y) [log p(y|x, z)] -KL(q θ (z|y)||p(z|x))</p><p>+ q θ (z|y) log p(y|x)dz ≥ 0.</p><p>The term log p(y|x) can be moved out from the third integral component, and leaves the integral becoming 1. Finally, we obtain the ELBO of the conditional log-likelihood log p(y|x) ≥ E q θ (z|y) [log p(y|x, z)] + KL(q θ (z|y)||p(z|x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Studies</head><p>This is a supplement for the aggregation ablation study in Sec. 6.3. In Tab. 5, we saw that the average NLL of all experiments using mean and max aggregation methods are similar. Fig. <ref type="figure" target="#fig_2">6</ref> shows the corresponding qualitative results. However, even though the NLL is very similar, the results of the aggregation of all features (MaxAggAll and MeanAggAll) are much more blurry than the results of the aggregation with addition (MaxAggAdd and MeanAggAdd). This is in conformity with the MSE min results. It indicates that the NLL alone is not always the best metric to assess the visual closeness to real faces. When carefully examining the images of the addition aggregations, you could argue that the predictions with zero input images look slightly more realistic for max aggregation while for three input images, mean aggregation seems to be marginally better. This again confirms the validity of the MSE min results even though the NLL results are also in accordance for this comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Statistic Significance of the Results</head><p>All experiments for this publications were carefully designed and optimized so that the training procedures are stable and lead to reproducible results. However, the data processing pipelines introduce randomness which lead to non-deterministic training outcomes due to multi-GPU training. We therefore ran every experiment three times and reported the results of the best training in Sec. 6. In Tabs. 7 to 12 we provide the means and variances of the three training runs.    The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Reconstruction</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Prediction results of the different architectures on FusionMNIST for zero to three input images.</figDesc><graphic coords="11,152.68,374.44,311.31,210.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Prediction results of the different aggregation methods on FusionCelebA for zero to three input images.</figDesc><graphic coords="21,144.04,323.01,328.55,215.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figs. 7</head><label>7</label><figDesc>Figs. 7 to 9 visualize the reconstruction outputs for all our datasets and architectures. For these results, the target image is always given as input. The first three rows of each figure show the reconstruction, when additionally three noisy or occluded input images are fed into the network.The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well.</figDesc><graphic coords="24,152.68,368.88,311.31,105.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>Fig. 7: Reconstruction results of the different architectures on FusionMNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,152.68,136.28,311.27,210.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,152.68,413.81,311.27,210.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on FusionMNIST. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">10.99 5.81 5.78 5.79 7.25</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">5.80 3.74 2.54 1.78 3.56</cell></row><row><cell>CVAE</cell><cell cols="10">17.81 15.01 14.07 13.61 15.23 3.83 1.72 1.05 0.80 1.93</cell></row><row><cell>CVAE+S</cell><cell cols="10">18.43 14.57 13.18 12.30 14.77 3.62 1.75 1.19 0.97 1.95</cell></row><row><cell cols="11">FusionVAE 15.93 14.17 13.70 13.48 14.39 3.14 0.99 0.74 0.65 1.45</cell></row><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">13.77 14.82 13.10 11.24 13.23</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">12.56 8.96 6.06 4.09 7.92</cell></row><row><cell>CVAE</cell><cell cols="10">446.0 280.1 273.5 266.5 316.7 9.23 3.46 2.27 1.55 4.14</cell></row><row><cell>CVAE+S</cell><cell cols="10">525.0 270.1 233.5 203.5 308.3 11.08 5.49 3.66 2.57 5.71</cell></row><row><cell cols="11">FusionVAE 248.1 227.6 231.2 228.7 233.9 5.11 0.88 0.86 0.84 1.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on FusionCelebA. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">5.83 3.34 2.37 1.82 3.35</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">8.06 1.84 1.13 0.74 2.97</cell></row><row><cell>CVAE</cell><cell cols="10">25.24 23.73 22.70 23.13 23.71 5.57 1.54 0.77 0.37 2.08</cell></row><row><cell>CVAE+S</cell><cell cols="10">26.08 24.94 23.98 23.95 24.75 4.95 2.50 1.77 1.19 2.62</cell></row><row><cell cols="11">FusionVAE 24.18 23.07 22.23 22.88 23.10 4.11 0.59 0.32 0.19 1.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on FusionT-LESS. The best results are printed in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Posterior ablation on the FusionCelebA dataset. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 avg</cell></row><row><cell cols="10">MaxAggAdd 248.1 227.6 231.2 228.7 233.9 5.11 0.88 0.86 0.84 1.93</cell></row><row><cell cols="10">MeanAggAdd 270.9 223.3 216.4 214.4 231.3 5.41 1.00 0.79 0.70 1.98</cell></row><row><cell cols="10">BayAggAdd 970.7 294.0 291.4 291.5 462.6 6.03 5.17 5.10 5.12 5.36</cell></row><row><cell>MaxAggAll</cell><cell cols="9">249.6 236.0 223.9 212.7 230.6 6.15 2.82 1.84 1.30 3.03</cell></row><row><cell cols="10">MeanAggAll 252.7 235.2 222.2 213.5 230.9 6.19 2.39 1.52 1.13 2.81</cell></row><row><cell>BayAggAll</cell><cell cols="9">255.6 568.3 414.7 1376.9 653.3 5.10 4.24 1.96 1.39 3.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Aggregation ablation on the FusionCelebA dataset. The best results are printed in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>69. Zaheer, M., Kottur, S., Ravanbakhsh, S., Póczos, B., Salakhutdinov, R., Smola, A.J.: Deep sets. In: NeurIPS. vol. 30, pp. 3391-3401 (2017) 70. Zeng, Y., Lin, Z., Yang, J., Zhang, J., Shechtman, E., Lu, H.: High-resolution image inpainting with iterative confidence feedback and guided upsampling. In: ECCV.</figDesc><table><row><cell>pp. 1-17. Springer (2020)</cell></row><row><cell>71. Zhang, H., Xu, H., Xiao, Y., Guo, X., Ma, J.: Rethinking the image fusion: A fast</cell></row><row><cell>unified image fusion network based on proportional maintenance of gradient and</cell></row><row><cell>intensity. AAAI 34, 12797-12804 (04 2020)</cell></row><row><cell>72. Zhang, Q., Qu, D., Xu, F., Zou, F.: Robust robot grasp detection in multimodal</cell></row><row><cell>fusion. In: MATEC Web of Conferences. vol. 139, p. 00060. EDP Sciences (2017)</cell></row><row><cell>73. Zheng, C., Cham, T.J., Cai, J.: Pluralistic image completion. In: CVPR. pp. 1438-</cell></row><row><cell>1447 (2019)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Main hyperparameters of our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>± 0.10 15.11 ± 0.07 14.19 ± 0.08 13.71 ± 0.07 15.27 ± 0.02 CVAE+S 18.45 ± 0.02 14.64 ± 0.06 13.22 ± 0.03 12.32 ± 0.02 14.81 ± 0.03 FusionVAE 15.91 ± 0.03 14.13 ± 0.07 13.64 ± 0.09 13.41 ± 0.10 14.34 ± 0.07 Mean and standard deviation of the FusionMNIST NLL results in 10 -2 BPD. The best results are printed in bold.</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>17.67 0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell cols="5">10.84 ± 0.37 5.96 ± 0.11 6.02 ± 0.18 6.13 ± 0.25 7.38 ± 0.11</cell></row><row><cell>FCN+S</cell><cell cols="5">6.21 ± 0.65 3.79 ± 0.04 2.64 ± 0.07 1.88 ± 0.08 3.73 ± 0.22</cell></row><row><cell>CVAE</cell><cell cols="5">3.87 ± 0.03 1.76 ± 0.03 1.09 ± 0.03 0.83 ± 0.02 1.97 ± 0.03</cell></row><row><cell>CVAE+S</cell><cell cols="5">3.53 ± 0.06 1.77 ± 0.01 1.23 ± 0.04 1.02 ± 0.04 1.96 ± 0.01</cell></row><row><cell cols="6">FusionVAE 3.14 ± 0.01 1.04 ± 0.06 0.77 ± 0.04 0.67 ± 0.03 1.47 ± 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Mean and standard deviation of the FusionMNIST MSE min results in 10 -2 . The best results are printed in bold. ± 9.42 289.1 ± 6.53 278.9 ± 4.51 270.3 ± 3.66 324.0 ± 5.17 CVAE+S 487.4 ± 27.45 355.4 ± 60.39 280.7 ± 35.12 230.9 ± 22.59 338.8 ± 22.99 FusionVAE 251.0 ± 2.06 222.3 ± 3.71 226.8 ± 3.16 224.0 ± 3.36 231.0 ± 2.05</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>456.9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Mean and standard deviation of the FusionCelebA NLL results in 10 -2 BPD. The best results are printed in bold. ± 0.87 20.00 ± 3.77 17.87 ± 3.42 15.56 ± 3.08 16.62 ± 2.48 FCN+S 11.94 ± 0.52 18.58 ± 7.02 14.90 ± 6.59 11.19 ± 5.39 14.15 ± 4.65 CVAE 8.70 ± 0.42 6.25 ± 2.16 4.33 ± 1.77 3.02 ± 1.37 5.58 ± 1.21 CVAE+S 9.87 ± 1.10 9.60 ± 3.34 7.30 ± 3.07 5.57 ± 2.64 8.09 ± 2.19 FusionVAE 5.82 ± 0.52 1.10 ± 0.16 0.93 ± 0.06 0.84 ± 0.03 2.18 ± 0.17</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell>13.07</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Mean and standard deviation of the FusionCelebA MSE min results in 10 -2 . The best results are printed in bold. ± 0.04 24.00 ± 0.25 22.98 ± 0.24 23.34 ± 0.19 23.90 ± 0.17 CVAE+S 26.12 ± 0.23 25.29 ± 0.27 24.27 ± 0.25 24.15 ± 0.20 24.97 ± 0.16 FusionVAE 24.32 ± 0.10 23.09 ± 0.02 22.25 ± 0.02 22.90 ± 0.02 23.15 ± 0.04</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>25.27</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Mean and standard deviation of the FusionT-LESS NLL results in 10 -2 BPD. The best results are printed in bold. ± 0.04 3.32 ± 0.10 2.50 ± 0.09 1.96 ± 0.10 3.43 ± 0.06 FCN+S 8.83 ± 1.05 1.95 ± 0.14 1.28 ± 0.15 0.86 ± 0.14 3.26 ± 0.37 CVAE 5.49 ± 0.11 1.73 ± 0.22 0.95 ± 0.18 0.44 ± 0.07 2.18 ± 0.13 CVAE+S 4.87 ± 0.06 2.98 ± 0.29 2.06 ± 0.19 1.27 ± 0.09 2.81 ± 0.12 FusionVAE 4.15 ± 0.03 0.62 ± 0.03 0.33 ± 0.03 0.20 ± 0.02 1.34 ± 0.02</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell>5.88</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Mean and standard deviation of the FusionT-LESS MSE min results in 10 -2 . The best results are printed in bold.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CVAE-GAN: fine-grained image generation through asymmetric training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2745" to="2754" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent Kalman networks: Factorized inference in high-dimensional deep feature spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gebhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="544" to="552" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Proc. Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PixelVAE: A latent variable model for natural images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FuseGAN: Learning to fuse multi-focus image via conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1982" to="1996" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FFB6D: A full flow bidirectional fusion network for 6D pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3003" to="3013" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PVN3D: A deep point-wise 3D keypoints voting network for 6DoF pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Š</forename><surname>Obdržálek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative adversarial network with adaptive constraints for multi-focus image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="15119" to="15129" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page" from="448" to="456" />
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised deep image fusion with structure tensor representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3845" to="3858" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SetVAE: Learning hierarchical composition for generative modeling of set-structured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15059" to="15068" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask-specific inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conf. Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
		<respStmt>
			<orgName>IROS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="m">The MNIST database of handwritten digits</title>
		<meeting><address><addrLine>LeCun, Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">2016. 1998</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DenseFuse: A fusion approach to infrared and visible images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2614" to="2623" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with ResNet and zero-phase component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Durrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">103039</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Infrared and visible image fusion using a deep learning framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2705" to="2710" />
		</imprint>
		<respStmt>
			<orgName>ICPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Generative face completion</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A medical image fusion method based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="191" to="207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via detail preserving adversarial learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DDcGAN: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4980" to="4995" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FusionGAN: A generative adversarial network for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BIVA: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6551" to="6562" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bayesian image reconstruction using deep generative models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04567[cs.CV</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dual contradistinctive generative autoencoder</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="823" to="832" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">DeepFuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Srikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4714" to="4722" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vinci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Amin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09948[cs.CV</idno>
		<title level="m">PixelVAE++: Improved PixelVAE with discrete prior</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3483" to="3491" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3738" to="3746" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<title level="m">Contextualbased image inpainting: Infer, match, and translate</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">SPG-Net: Segmentation prediction and guidance network for image inpainting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19667" to="19679" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">DVAE++: Discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5035" to="5044" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bayesian context aggregation for neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Flürenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grossberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="835" to="851" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">DenseFusion: 6D object pose estimation by iterative dense fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GraspFusionNet: a two-stage multi-parameter grasp detection network based on RGB-XYZ fusion in dense clutter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FusionDN: A unified densely connected network for image fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12484" to="12491" />
			<date type="published" when="2020-04">04 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Shift-Net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">D-CVF: Generating joint camera and lidar features using cross-view spatial feature fusion for 3D object detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="720" to="736" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
