<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
<<<<<<< HEAD
				<title level="a" type="main">FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-09-22">22 Sep 2022</date>
=======
				<title level="a" type="main">Location-Sensitive Visual Recognition with Cross-IOU Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
>>>>>>> main
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
<<<<<<< HEAD
							<persName><forename type="first">Fabian</forename><surname>Duffhauss</surname></persName>
							<idno type="ORCID">0000-0002-8910-3852</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of TÃ¼bingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ngo</forename><forename type="middle">Anh</forename><surname>Vien</surname></persName>
							<idno type="ORCID">0000-0001-9646-267X</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Ziesche</surname></persName>
							<idno type="ORCID">0000-0003-2042-3660</idno>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
							<email>gerhard.neumann@kit.edu</email>
							<idno type="ORCID">0000-0002-5483-4225</idno>
							<affiliation key="aff2">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">{fabian</forename><surname>Duffhauss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anhvien</forename><surname>Ngo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Ziesche}</surname></persName>
						</author>
						<author>
							<persName><forename type="first">@</forename><surname>Bosch</surname></persName>
						</author>
						<author>
							<persName><surname>Com</surname></persName>
						</author>
						<title level="a" type="main">FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-22">22 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">1E6659617709632A94D0B1B570F2644F</idno>
					<idno type="arXiv">arXiv:2209.11277v1[cs.CV]</idno>
=======
							<persName><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
							<email>kaiwenduan@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<email>hgqi@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Location-Sensitive Visual Recognition with Cross-IOU Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">482E8D09AE05387813ED7D8252B10729</idno>
>>>>>>> main
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
<<<<<<< HEAD
				<application version="0.8.1" ident="GROBID" when="2025-05-14T06:32+0000">
=======
				<application version="0.8.1" ident="GROBID" when="2025-05-18T07:34+0000">
>>>>>>> main
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<<<<<<< HEAD
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensor fusion can significantly improve the performance of many computer vision tasks. However, traditional fusion approaches are either not data-driven and cannot exploit prior knowledge nor find regularities in a given dataset or they are restricted to a single application. We overcome this shortcoming by presenting a novel deep hierarchical variational autoencoder called FusionVAE that can serve as a basis for many fusion tasks. Our approach is able to generate diverse image samples that are conditioned on multiple noisy, occluded, or only partially visible input images. We derive and optimize a variational lower bound for the conditional log-likelihood of FusionVAE. In order to assess the fusion capabilities of our model thoroughly, we created three novel datasets for image fusion based on popular computer vision datasets. In our experiments, we show that FusionVAE learns a representation of aggregated information that is relevant to fusion tasks. The results demonstrate that our approach outperforms traditional methods significantly. Furthermore, we present the advantages and disadvantages of different design choices.</p></div>
=======
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection, instance segmentation, and pose estimation are popular visual recognition tasks which require localizing the object by internal or boundary landmarks. This paper summarizes these tasks as locationsensitive visual recognition and proposes a unified solution named location-sensitive network (LSNet). Based on a deep neural network as the backbone, LSNet predicts an anchor point and a set of landmarks which together define the shape of the target object. The key to optimizing the LSNet lies in the ability of fitting various scales, for which we design a novel loss function named cross-IOU loss that computes the cross-IOU of each anchor-landmark pair to approximate the global IOU between the prediction and ground-truth. The flexibly located and accurately predicted landmarks also enable LSNet to incorporate richer contextual information for visual recognition. Evaluated on the MS-COCO dataset, LSNet set the new state-of-theart accuracy for anchor-free object detection (a 53.5% box AP) and instance segmentation (a 40.2% mask AP), and shows promising performance in detecting multi-scale human poses. Code is available at <ref type="url" target="https://github.com/Duankaiwen/LSNet">https://github. com/Duankaiwen/LSNet</ref>.</p></div>
>>>>>>> main
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<<<<<<< HEAD
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sensor fusion is a popular technique in computer vision as it allows to combine the advantages from multiple information sources. It is especially gainful in scenarios where a single sensor is not able to capture all necessary data to perform a task satisfactorily. Over the last years, we have seen many examples, where the accuracy of computer vision tasks was significantly improved by sensor fusion, e.g. in environmental perception for autonomous driving <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b65">67]</ref>, for 6D pose estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">61]</ref>, and for robotic grasping <ref type="bibr" target="#b60">[62,</ref><ref type="bibr">72]</ref>. However, traditional fusion methods usually focus more on the beneficial merging of multiple modalities and less on teaching the model to obtain profound prior knowledge about the used dataset.</p><p>Our work tries to fill in this research gap by proposing a deep hierarchical variational autoencoder called FusionVAE that is able to perform both tasks: fusing information from multiple sources and supplementing it with prior knowledge about the data gained while training. As shown in Fig. <ref type="figure">1</ref>, FusionVAE merges a varying number of input images for reconstructing the original target image using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FusionVAE Inputs Target Predictions</head><p>Fig. <ref type="figure">1</ref>: Overview of our FusionVAE approach. The network receives up to three partly occluded input images, fuses them together with prior knowledge, and predicts different hypothesis of how the target images could look like.</p><p>prior knowledge about the dataset. To the best of our knowledge, FusionVAE is the first approach that combines these two tasks. Therefore, we developed three challenging benchmarks based on well-known computer vision datasets to evaluate the performance of our approach. In addition, we perform comparisons to baselines by extending traditional approaches to perform the same tasks. FusionVAE outperforms all these traditional methods on all proposed benchmark tasks significantly. We show that FusionVAE can generate high-quality images given few input images with partial observability. We provide ablation studies to illustrate the impact of commonly used information aggregation operations and to prove the benefits of the employed posterior distribution. We can summarize the four main contributions of our paper as follows: i) We create three challenging image fusion tasks for generative models. ii) We develop a deep hierarchical VAE called FusionVAE that is able to perform image-based data fusion while employing prior knowledge of the used dataset. iii) We show that FusionVAE produces high-quality fused output images and outperforms traditional methods by a large margin. iv) We perform ablation studies showing the benefits of our design choices regarding both the posterior distribution and commonly used aggregation methods. and expressiveness and thus, when applied to image generation leads to oversmoothed results lacking fine-grained details. Over the last years much work has been invested into the effort of improving the generative performance of VAEs. One stream of work is based on introducing a hierarchy into the latent space of the VAE and scaling this hierarchy to greater and greater depth. First introduced in <ref type="bibr" target="#b52">[54]</ref> many hierarchical VAEs are based on coupling the inference and generative processes by introducing a deterministic bottom-up path combined with a stochastic top-down process in the inference network and sharing the latter with the generative model. This setting has been extended by an additional deterministic top-down path and bidirectional inference in <ref type="bibr" target="#b42">[44]</ref>. Recently, very deep hierarchical VAEs were realized in <ref type="bibr" target="#b7">[8]</ref> by introducing residual bottlenecks with dedicated scaling, update skipping, and nearest neighbour up-sampling. Closest to our work is the recently proposed NVAE architecture <ref type="bibr" target="#b55">[57]</ref>, which relies on depth-wise convolution, residual posterior parametrization, and spectral regularization to enhance stability.</p><p>Other approaches propose increasing the expressiveness of VAEs by combining them with auto-regressive models like RNNs or PixelCNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">50]</ref>, conditioning contexts (CVAE) <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b58">60]</ref>, normalizing flows <ref type="bibr" target="#b27">[28]</ref>, generative adversarial networks (GANs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">46]</ref>, or variational generative adversarial networks (CVAE-GAN) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fusion of Multiple Images</head><p>Image fusion has long been dominated by classical computer vision. Only lately deep learning methods entered the domain with the CNN-based approach proposed by Liu et al. <ref type="bibr" target="#b36">[38]</ref>. In a subsequent publication the authors extended their work to a multi-scale setting <ref type="bibr" target="#b35">[37]</ref>. Shortly afterwards, Prabhakar et al. developed a fusion method based on a siamese network architecture, called DeepFuse <ref type="bibr" target="#b46">[48]</ref> which was improved in subsequent work <ref type="bibr" target="#b31">[33]</ref> by employing the DenseNet architecture <ref type="bibr" target="#b19">[20]</ref>. Concurrently, Li et al. <ref type="bibr" target="#b33">[35]</ref> proposed a fusion architecture based on VGG <ref type="bibr" target="#b50">[52]</ref> and in order to scale to even greater depth another one <ref type="bibr" target="#b32">[34]</ref> based on ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. The aforementioned methods use CNNs as feature extractors and as decoders, while the fusion operations themselves are restricted to classical methods like averaging or addition of feature maps or weighted source images. A fully CNN-based feature-map fusion mechanism was proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>While all previous publications target only a specific fusion task (e.g. multifocus fusion, multi-resolution fusion, etc.) or were limited to specific domains (e.g. medical images), two very recent works propose novel multi-purpose fusion networks, which are applicable to many fusion tasks and image types <ref type="bibr" target="#b62">[64,</ref><ref type="bibr">71]</ref>. Very recently also GAN-based methods entered the domain of image fusion, starting with the work by Ma et al. on infrared-visible fusion <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b41">43]</ref> and with <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b61">63]</ref> on multi-resolution image fusion. Most recent are two publications on GAN-based multi-focus image fusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. While GAN-based approaches can generate high-fidelity images, it is known that they suffer from the mode collapse problem. VAE-based methods in contrast are known to generate more faithful data distribution <ref type="bibr" target="#b55">[57]</ref>. Different from previous work, this paper proposes a VAE-based multi-purpose fusion framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Completion</head><p>Similar to image fusion, also image completion has only recently become a playing field for deep learning methods. First approaches based on simple multilayer perceptrons (MLPs) <ref type="bibr" target="#b28">[29]</ref> or CNNs <ref type="bibr" target="#b11">[12]</ref> were targeted only to filling small holes in an image. However, with the introduction of GANs <ref type="bibr" target="#b9">[10]</ref>, the area quickly became dominated by GAN-based approaches, starting with the context encoders presented by Pathak et al. <ref type="bibr" target="#b45">[47]</ref>. Many subsequent papers proposed extensions to this model in order to obtain fine-grained completions while preserving global coherence by introducing additional discriminators <ref type="bibr" target="#b21">[22]</ref>, searching for closest samples to the corrupted image in a latent embedding space conditioning on semantic labels <ref type="bibr" target="#b54">[56]</ref>, or designing additional specialized loss functions <ref type="bibr" target="#b34">[36]</ref>. High resolution results were obtained recently by multi-scale approaches <ref type="bibr" target="#b64">[66]</ref>, iterative upsampling [70], and the application of contextual attention <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b66">68]</ref>. Another stream of current work focuses on multi-hypothesis image completion, leveraging probabilistic problem formulations <ref type="bibr" target="#b43">[45,</ref><ref type="bibr">73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we outline the fundamentals of standard VAEs, conditional VAEs, and hierarchical VAEs upon which we build our approach. Another section is dedicated to aggregation methods for data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard VAE</head><p>A variational autoencoder (VAE) <ref type="bibr" target="#b26">[27]</ref> is a neural network consisting of a probabilistic encoder q(z|y) and a generative model p(y|z). The generator models a distribution over the input data y, conditioned on a latent variable z with prior distribution p Î¸ (z). The encoder approximates the posterior distribution p(z|y) of the latent variables z given input data y and is trained along with the generative model by maximizing the evidence lower bound (ELBO) ELBO(y) = E q(z|y) [log p(y|z)] -KL(q(z|y)||p(z)), <ref type="bibr" target="#b0">(1)</ref> where KL is the Kullback-Leibler divergence and log p(y) â¥ ELBO(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional VAE</head><p>In VAEs, the generative model p(y|z) is unconditional. In contrast, conditional VAEs (CVAE) <ref type="bibr" target="#b51">[53]</ref> consider a generative model for a conditional distribution p(y|x, z) where y is the target data, x is the conditional input variable, and z is a latent variable. The prior of the latent variable is p(z|x), while its approximate posterior distribution is given by q(z|x, y). The variational lower bound of the conditional log-likelihood can be written as follows log p(y|x) â¥ E q(z|x,y) [log p(y|x, z)] -KL(q(z|x, y)||p(z|x)).</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical VAE</head><p>In hierarchical VAEs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b55">57]</ref>, the latent variables z are divided into L disjoint groups z 1 , ..., z L in order to increase the expressiveness of both prior and approximate posterior which become</p><formula xml:id="formula_0">p(z) = L l=1</formula><p>p(z l |z &lt;l ) and q(z|y)</p><formula xml:id="formula_1">= L l=1 q(z l |z &lt;l , y),<label>(3)</label></formula><p>where z &lt;l denotes the latent variables in all previous hierarchies. All the conditionals in the prior p(z l |z &lt;l ) and in the approximate posterior q(z l |z &lt;l , y) are modeled by factorial Gaussian distributions. Under this modelling choice, the ELBO from Eq. ( <ref type="formula">1</ref>) turns into</p><formula xml:id="formula_2">ELBO(y) = E q(z|y) [log p(y|z)] - L l=1 E q(z &lt;l |y) [KL(q(z l |z &lt;l , y)||p(z l |z &lt;l ))]. (<label>4</label></formula><formula xml:id="formula_3">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Aggregation Methods</head><p>For fusing data within our approach, we consider different aggregation methods, such as mean aggregation, max aggregation, Bayesian aggregation <ref type="bibr" target="#b57">[59]</ref> and pixelwise addition. All described aggregation methods fuse a set of feature tensors f 1 , ..., f K , obtained by encoding K input images {x i } K i=1 in a permutation invariant way <ref type="bibr">[69]</ref>. In mean aggregation, multiple feature vectors are fused by taking the pixel-wise average f = 1 K K i=1 f i . For max aggregation, we take the pixel-wise maximum instead f = max i (f i ). Bayesian aggregation (BA) <ref type="bibr" target="#b57">[59]</ref> considers an uncertainty estimate for the fused feature vectors. In order to obtain such an uncertainty estimate, the encoder has to predict means Âµ i and variances Ï i of a factorized Gaussian distribution over the latent feature vectors</p><formula xml:id="formula_4">f i = N (Âµ i , diag(Ï i )), with Âµ i = enc Âµ (x i , y) and Ï i = enc Ï (x i , y),<label>(5)</label></formula><p>instead of f i directly. Here enc Âµ and enc Ï represent the encoding process which generates means and variances respectively. The predicted distributions over latent feature vectors for multiple input images can be fused iteratively using the Bayes rule <ref type="bibr" target="#b1">[2]</ref> (detailed derivation is given in Appendix B)</p><formula xml:id="formula_5">q i = Ï 2 i-1 â (Ï 2 i-1 + Ï 2 i )<label>(6)</label></formula><p>where <ref type="formula">7</ref>) â and â denote element-wise division and multiplication respectively.</p><formula xml:id="formula_6">Âµ i = Âµ i-1 + q i â (Âµ i -Âµ i-1 ) and Ï 2 i = Ï 2 i-1 â (1 -q i ). (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conditional Generative Models for Image Fusion</head><p>We propose a deep hierarchical conditional variational autoencoder, called Fusion-VAE (Fusion Variational Auto-Encoder), that is able to fuse information from multiple sources and to infer the missing information in the images from a prior learned from the dataset. To the best of our knowledge, it is the first model that combines the generative ability of a hierarchical VAE to learn the underlying distribution of complex datasets with the ability to fuse multiple input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>We consider image fusion problems that are concerned with generating the fused target image from multiple source images. Each source image contains partial information of the target image and the goal of the task is to recover the original target image given a finite set of source images. In particular, we denote the target image as y and the set of K source images as context x = {x 1 , x 2 , . . . , x K }, where each x i is one source image. Given training sample (x, y), we aim to maximize the conditional likelihood p(y|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FusionVAE</head><p>Our approach is designed to maximize the conditional likelihood p(y|x). However, optimizing this objective directly is intractable. Therefore, we derive a variational lower bound as follows (detailed derivation can be found in Appendix C)</p><formula xml:id="formula_7">log p(y|x) â¥ E q Ï (z|y) [log p Î¸ (y|x, z)] -Î² L l=1 Î± l E q(z &lt;l |y) [KL(q Ï (z l |y, z &lt;l )||p Î¸ (z l |x, z &lt;l ))],<label>(8)</label></formula><p>where we split the latent variables z into L disjoint groups {z 1 , z 2 , . . . , z L }. Î² and Î± l are annealing parameters that control the warming-up of the KL terms as in <ref type="bibr" target="#b55">[57]</ref>. Inspired by <ref type="bibr" target="#b52">[54]</ref>, Î² is increased linearly from 0 to 1 during the first few training epochs to start training the reconstruction before introducing the KL term, which is increased gradually. Î± l is a KL balancing coefficient <ref type="bibr" target="#b56">[58]</ref> that is used during the warm-up period to encourage the equal use of all latent groups and to avoid posterior collapse. FusionVAE consists of three main networks and a latent space as illustrated in Fig. <ref type="figure">2:</ref> 1) a context encoder network which models the conditional prior p Ï (z l |x, z &lt;l ), 2) a target encoder which models the approximate posterior q Ï (z|y), 3) a latent space comprising the L latent groups, and 4) a generator network p Î¸ (y|x, z) that aims to reconstruct the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Architecture</head><p>Fig. <ref type="figure">2</ref> illustrates the network architecture of our FusionVAE for training. It is built in a hierarchical way inspired by <ref type="bibr" target="#b55">[57]</ref>. In each latent hierarchy l â 1, . . . , L we have a set of feature maps f lx , f ly and latent distributions p l , q l . The first gray box contains the context encoder network that obtains a stack of source images x and employs residual cells <ref type="bibr" target="#b14">[15]</ref> as in <ref type="bibr" target="#b55">[57]</ref> to extract features f lx . The second gray box shows the target encoder network that encodes the target Fig. <ref type="figure">2</ref>: Overview of the proposed network architecture. h is a trainable parameter vector, â denotes concatenation, â max aggregation, and â pixel-wise addition.</p><p>r r is a residual network like in <ref type="bibr" target="#b55">[57]</ref>. The dotted lines between the residual networks indicate shared parameters.</p><p>image y into the feature map f ly using the same residual cells as the context encoder. The third gray box illustrates the latent space which contains the prior distributions p Ï (z l |x, z &lt;l ) (denoted p 1 , ..., p L in Fig. <ref type="figure">2</ref>) and the approximate posterior distributions q Ï (z l |y, z &lt;l ) (denoted q 1 , ..., q L in Fig. <ref type="figure">2</ref>). The fourth gray box contains the generator network which aims to create different output samples Å·. It employs a trainable parameter vector h, concatenates the information from all hierarchies, and decodes them using residual cells.</p><p>In each latent hierarchy, we aggregate the context features f lx using pixelwise max aggregation. In all but the first hierarchy, we pixel-wisely add the corresponding feature map from the generator network to the aggregated context features and to the target image features f ly . Using 2D convolutional layers, we learn the prior distributions p l and the approximate posterior distributions q l . We propose to use the approximate posterior distributions q l as target distributions in order to learn good prior distributions p l . Therefore, q Ï (z l |y, z &lt;l ) is created from the target image features f ly as well as information from the generator network.</p><p>During training the generator network aims to create a prediction Å· based on samples of the posterior distributions q l and a trainable parameter vector h.</p><p>For evaluation, we can omit the target image input y and sample from the prior distributions p l . In case no input image is given, we set p 1 to a standard normal distribution. Based on the samples and the trainable parameter vector h, our FusionVAE can generate new output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>To evaluate our approach, we conduct a series of experiments on three different datasets using data augmentation. Furthermore, we adapt traditional architec-tures for solving the same tasks in order to compare our results. Finally, we perform an ablation study to show the effects of specific design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For training and evaluating our approach, we create three novel fusion datasets based on MNIST [32], CelebA <ref type="bibr" target="#b37">[39]</ref>, and T-LESS <ref type="bibr" target="#b17">[18]</ref> as described in the following.</p><p>FusionMNIST. Based on the dataset MNIST [32], we create an image fusion dataset called FusionMNIST. For each target image, it contains different noisy representations where only random parts of the target image are visible. The first three columns of Fig. <ref type="figure">3</ref> show different examples of FusionMNIST corresponding to the target images in the fourth column. To generate FusionMNIST, we applied zero padding to all MNIST images to obtain a resolution of 32 Ã 32. For creating a noisy representation, we generate a mask composed of the union of a varying number of ellipses with random size, shape and position. All parts of the given images outside the mask are blackened. Finally, we add Gaussian noise with a fixed variance and clip the pixel values afterwards to stay within [0, 1].</p><p>FusionCelebA. We generate a similar fusion dataset based on the aligned and cropped version of CelebA <ref type="bibr" target="#b37">[39]</ref> which we call FusionCelebA. Fig. <ref type="figure">4</ref> depicts different example images in the first three columns which belong to the target image in the fourth column. To generate FusionCelebA, we center-crop the CelebA images to 148 Ã 148 before scaling them down to 64 Ã 64 as proposed by <ref type="bibr" target="#b30">[31]</ref>. As in Fusion-MNIST, we create different representations by using masks based on random ellipses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FusionT-LESS.</head><p>A promising area of application for our FusionVAE is robot vision. Scenes in robotics settings can be very difficult to understand due to texture-less or reflective objects and occlusions. To examine the performance of our FusionVAE in this area, we create an object dataset with challenging occlusions based on T-LESS <ref type="bibr" target="#b17">[18]</ref> which we call FusionT-LESS. To generate FusionT-LESS, we use the real training images of T-LESS and take all images of classes 19 -24 as basis for the target images. This selection contains all objects with power sockets and therefore images with many similarities. Every tenth image is removed from the training set and used for evaluation. In order to create challenging occlusions, we cut all objects from images of other classes using a Canny edge detector <ref type="bibr" target="#b4">[5]</ref> and overlay each target image with a random number between five and eight cropped objects. We select all images from classes 1, 2, 5 -7, 11 -14, and 25 -27 as occluding objects for training and classes 3, 4, 8 -10, 15 -18, and 28 -30 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Augmentation</head><p>During training we apply different augmentation methods on the datasets to avoid overfitting. For FusionMNIST, we apply the elliptical mask generation and the addition of Gaussian noise live during training so that we obtain an infinite number of different fusion tasks. For FusionT-LESS, almost the entire creation of occluded images is performed during training. We apply horizontal flips, rotations, scaling and movement of target and occluding images with random parameters before composing the different occluded representations. Solely the object cutting with the Canny edge detector is performed offline as a pre-processing step to keep the training time low. For FusionCelebA, we apply a horizontal flip of all images randomly in 50% of all occasions and also the elliptical mask generation is done live during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Architectures for Comparison</head><p>To the best of our knowledge, FusionVAE is the first fusion network for multiple images with a generative ability to fill areas without input information based on prior knowledge about the dataset under consideration. For lack of a suitable other model from the literature which would allow a fair comparison on our multi-image fusion tasks, we compare our approach with standard architectures that we adapted to support our tasks.</p><p>The first architecture for comparison is a CVAE with residual layers as employed in <ref type="bibr" target="#b55">[57]</ref>. We use a shared encoder for processing the input images and applied max aggregation before the latent space as we did in our FusionVAE. The second architecture for comparison is a fully convolutional network (FCN) with shared encoder and max aggregation before the decoder.</p><p>For both baseline architectures, we created a version with skip connections (+S) and a version without. When using skip connections, we applied max aggregation at each shortcut for merging the features from the encoder with the decoder's features. To allow for a fair comparison, we designed all architectures so that they have a similar number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Procedure</head><p>We trained all networks in a supervised manner using the augmented target images y as described in Sec. 5.2. In order to teach the networks both to fuse information from a different number of input images and to learn prior knowledge about the dataset, we vary the number of input images x during the entire training. Specifically, we select a uniformly distributed random number between zero and three for each batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation Metrics</head><p>For evaluation, we estimate the negative log-likelihood (NLL) in bits per dimension (BPD) using weighted importance sampling <ref type="bibr" target="#b3">[4]</ref>. We use 100 samples for all experiments with FusionCelebA as well as FusionT-LESS and 1000 samples for FusionMNIST. Since we cannot estimate the NLL of the FCN, we used the minimum over all samples of the mean squared error (MSE min ) as second metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>This section presents and discusses the quantitative and qualitative results of our research in comparison to the baseline methods mentioned in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative Results</head><p>Tabs. 1 to 3 show the NLL and the MSE min of all architectures on FusionMNIST, FusionCelebA, and FusionT-LESS respectively. The results are divided into the results based on zero to three input images and the average (avg) of it. We see that our FusionVAE outperforms all baseline methods on average. Regarding the NLL, our model surpasses the others additionally for 0 and 1 input images. For 2 and 3 images, CVAE+S reaches sometimes slightly better NLL values. However, our approach reaches the best MSE min values for each number of input images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative Results</head><p>The outstanding performance of our architecture in comparison to the others is also obvious when looking at the qualitative results in Figs. <ref type="figure">3 to 5</ref>. For every row, these figures show the input, target, and up to three output predictions for all architectures. For the FCN, we depict just a single output prediction per row as all of them look almost identical.</p><p>In the first three rows when the network does not receive any input image, we see that our network provides very realistic images. This indicates that it is able to capture the underlying distribution of the used datasets very well and much better than the other architectures. Due to the difficulty of the FusionT-LESS dataset, none of the models is able to produce realistic images without any input. Still our model shows much better performance in generating object-like shapes. In case the models receive at least one input image (cf. rows 4 -12), all architectures are able to extract the available information from the given input images. In addition, all VAE approaches, ours included, are able to complete the given input data based on prior knowledge. It is clearly visible, however, that the predictions of our model are much more realistic than the ones of the standard CVAE approaches especially for the more difficult datasets like FusionCelebA and FusionT-LESS. Fig. <ref type="figure">5</ref>: Predictions on FusionT-LESS for zero to three input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>We conducted ablation studies to show the effect of certain design choices, such as the selection of the approximate posterior and the aggregation method. All experiments are run on the FusionCelebA dataset. Tab. 4 compares the performance of our FusionVAE for two different approximate posterior distributions q. The approximate posterior we selected for our FusionVAE q(y), depends only on the given target image y. It performs slightly better on average compared to the same method using a posterior that is computed based on the input images x as well as the target image y. However, the latter approach is superior when fusing two or three input images.   Tab. 5 shows the performance of different aggregation methods which are applied to create the prior distributions p l of every latent group. In our Fusion-VAE, the prior is created by fusing the input image features f lx using max aggregation (MaxAgg) and adding them to the decoded features of the same latent group before applying a 2D convolution. We abbreviate that method with MaxAggAdd.</p><p>In addition to MaxAgg, we examined mean aggregation (MeanAgg) and Bayesian aggregation (BayAgg) <ref type="bibr" target="#b57">[59]</ref> for comparison. For each aggregation principle, we tried two different versions: 1) aggregation of the input image features f lx adding the corresponding information from the decoder in a pixel-wise manner (denoted by suffix Add), and 2) directly aggregating all features, i.e. both input image features f lx and decoder features (denoted by suffix All).</p><p>For creating the prior p i when using BayAgg, we moved the 2D convolutions before the aggregation in order to create the parameters Âµ and Ï of a latent Gaussian distribution. Unlike MaxAgg and MeanAgg, BayAgg directly outputs a new Gaussian distribution that does not need to be processed any further by a convolution.</p><p>We can see that all variations of mean and max aggregation are significantly better than Bayesian aggregation. Also their training procedures are less often impaired due to numeric instabilities. Interestingly, the NLL is very similar independent of whether the aggregation is performed on all features or not. However, the MSE min is much better for the aggregation with addition. Since the expressiveness of the metrics is limited, we provide additional visualizations of this ablation in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a novel deep hierarchical variational autoencoder for generative image fusion called FusionVAE. Our approach fuses multiple corrupted input images together with prior knowledge obtained during training. We created three challenging image fusion benchmarks based on common computer vision datasets. Moreover, we implemented four standard methods that we modified to support our tasks. We showed that our FusionVAE outperforms all other methods significantly while having a similar number of trainable parameters. The predicted images of our approach look very realistic and incorporate given input information almost perfectly. During ablation studies, we revealed the benefits of our design choices regarding the applied aggregation method and the used posterior distribution. In future work, our research could be extended by enabling the fusion of different modalities e.g. by using multiple encoders. Additionally, an explicit uncertainty estimation could be implemented that helps to weigh the impact of input information according to its uncertainty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details</head><p>For FusionMNIST and FusionT-LESS, we model the decoder's output by pixelwise independent Bernoulli distributions. For FusionCelebA, we use pixel-wise independent discretized logistic mixture distributions as proposed by Salimans et al. <ref type="bibr" target="#b49">[51]</ref>.</p><p>The residual cells of the encoder are composed of batch normalization layers <ref type="bibr" target="#b22">[23]</ref>, Swish activation functions <ref type="bibr" target="#b47">[49]</ref>, convolutional layers, and Squeeze-and-Excitation (SE) blocks <ref type="bibr" target="#b18">[19]</ref> as proposed in <ref type="bibr" target="#b55">[57]</ref>. In the decoder, we also follow <ref type="bibr" target="#b55">[57]</ref> and build the residual cells out of batch normalization layers, 1x1 convolutions, Swish activations, depthwise separable convolutions <ref type="bibr" target="#b8">[9]</ref>, and SE blocks. However, we omitted normalizing flow because in our experiments it showed to increase the training time without improving the prediction accuracy significantly.</p><p>For each dataset, we chose the size of the architecture individually to achieve acceptable accuracy while keeping the training time reasonable. Tab. 6 provides details about the used hyperparameters.  In general, the number of latent groups L should be chosen depending on the complexity of the task at hand. We made our decision based on the L of the NVAE <ref type="bibr" target="#b55">[57]</ref> but reduced it for computational reasons. For FusionCelebA and FusionT-LESS, we use 17 latent groups, for FusionMNIST only seven. Using more latent groups improves the results but increases the computational effort significantly.</p><p>For all experiments, we used GPUs of type NVIDIA Tesla V100 with 32GB of memory and trained with an AdaMax optimizer <ref type="bibr" target="#b25">[26]</ref>. We applied a cosine annealing schedule for the learning rate <ref type="bibr" target="#b38">[40]</ref> starting at 0.01 and ending at 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation of Bayesian Aggregation</head><p>We use two related encoders to learn a latent observation Âµ i = enc Âµ (x i , y) with its corresponding variance values Ï i = enc Ï (x i , y).</p><p>Assuming a factorized Gaussian prior distribution in the latent space p(z) = N (z|Âµ z,0 , diag(Ï z,0 )), we can derive the factorized posterior distribution q Ï (z|y) = N (z|Âµ z , diag(Ï z ) in closed form using standard Gaussian conditioning <ref type="bibr" target="#b2">[3]</ref> following <ref type="bibr" target="#b57">[59]</ref> Ï</p><formula xml:id="formula_8">2 z = (Ï 2 z,0 ) â + (Ï 2 i ) â â ,<label>(9)</label></formula><formula xml:id="formula_9">Âµ z = Âµ z,0 + Ï 2 z,0 â (Âµ i -Âµ z,0 ) â Ï 2 i (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where â denotes element-wise inversion, â denotes element-wise multiplication, and â denotes element-wise division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Derivation of FusionVAE's ELBO</head><p>We start with the following KL divergence between the approximate posterior and the real posterior, KL(q Î¸ (z|y)||p Î¸ (z|x, y)) â¥ 0.</p><p>Next, we apply the Bayes's theorem to obtain -q Î¸ (z|y) log p(y|x, z)p(z|x) p(y|x)q Î¸ (z|y) dz â¥ 0.</p><p>This leads to -E q Î¸ (z|y) [log p(y|x, z)] -KL(q Î¸ (z|y)||p(z|x))</p><p>+ q Î¸ (z|y) log p(y|x)dz â¥ 0.</p><p>The term log p(y|x) can be moved out from the third integral component, and leaves the integral becoming 1. Finally, we obtain the ELBO of the conditional log-likelihood log p(y|x) â¥ E q Î¸ (z|y) [log p(y|x, z)] + KL(q Î¸ (z|y)||p(z|x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Studies</head><p>This is a supplement for the aggregation ablation study in Sec. 6.3. In Tab. 5, we saw that the average NLL of all experiments using mean and max aggregation methods are similar. Fig. <ref type="figure" target="#fig_2">6</ref> shows the corresponding qualitative results. However, even though the NLL is very similar, the results of the aggregation of all features (MaxAggAll and MeanAggAll) are much more blurry than the results of the aggregation with addition (MaxAggAdd and MeanAggAdd). This is in conformity with the MSE min results. It indicates that the NLL alone is not always the best metric to assess the visual closeness to real faces. When carefully examining the images of the addition aggregations, you could argue that the predictions with zero input images look slightly more realistic for max aggregation while for three input images, mean aggregation seems to be marginally better. This again confirms the validity of the MSE min results even though the NLL results are also in accordance for this comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Statistic Significance of the Results</head><p>All experiments for this publications were carefully designed and optimized so that the training procedures are stable and lead to reproducible results. However, the data processing pipelines introduce randomness which lead to non-deterministic training outcomes due to multi-GPU training. We therefore ran every experiment three times and reported the results of the best training in Sec. 6. In Tabs. 7 to 12 we provide the means and variances of the three training runs.    The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Reconstruction</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Prediction results of the different architectures on FusionMNIST for zero to three input images.</figDesc><graphic coords="11,152.68,374.44,311.31,210.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Prediction results of the different aggregation methods on FusionCelebA for zero to three input images.</figDesc><graphic coords="21,144.04,323.01,328.55,215.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figs. 7</head><label>7</label><figDesc>Figs. 7 to 9 visualize the reconstruction outputs for all our datasets and architectures. For these results, the target image is always given as input. The first three rows of each figure show the reconstruction, when additionally three noisy or occluded input images are fed into the network.The images show that our FusionVAE reconstructs the target images almost perfectly for all three datasets. On FusionMNIST, only the FCN does not manage to reconstruct the target images but shows blurry versions of them. We also see the same behavior for FusionCelebA and FusionT-LESS which underlines the importance of skip connections for this type of network. On FusionCelebA, we see that CVAE+S suffers from numeric instabilities causing colorful artifacts in some images. Omitting the skip connections here avoids that issue. On FusionT-LESS, all baseline methods create more or less blurry versions of the target image when just the target image is given. When inputting the occluded images in addition to the target image, the reconstruction is much better which shows that these networks have over-fitted to the task of removing occluded objects so that they cannot deal well with non-occluded images. In contrast, FusionVAE has the ability to reconstruct non-occluded input images very well.</figDesc><graphic coords="24,152.68,368.88,311.31,105.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>Fig. 7: Reconstruction results of the different architectures on FusionMNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,152.68,136.28,311.27,210.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,152.68,413.81,311.27,210.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on FusionMNIST. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">10.99 5.81 5.78 5.79 7.25</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">5.80 3.74 2.54 1.78 3.56</cell></row><row><cell>CVAE</cell><cell cols="10">17.81 15.01 14.07 13.61 15.23 3.83 1.72 1.05 0.80 1.93</cell></row><row><cell>CVAE+S</cell><cell cols="10">18.43 14.57 13.18 12.30 14.77 3.62 1.75 1.19 0.97 1.95</cell></row><row><cell cols="11">FusionVAE 15.93 14.17 13.70 13.48 14.39 3.14 0.99 0.74 0.65 1.45</cell></row><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">13.77 14.82 13.10 11.24 13.23</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">12.56 8.96 6.06 4.09 7.92</cell></row><row><cell>CVAE</cell><cell cols="10">446.0 280.1 273.5 266.5 316.7 9.23 3.46 2.27 1.55 4.14</cell></row><row><cell>CVAE+S</cell><cell cols="10">525.0 270.1 233.5 203.5 308.3 11.08 5.49 3.66 2.57 5.71</cell></row><row><cell cols="11">FusionVAE 248.1 227.6 231.2 228.7 233.9 5.11 0.88 0.86 0.84 1.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on FusionCelebA. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">5.83 3.34 2.37 1.82 3.35</cell></row><row><cell>FCN+S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">8.06 1.84 1.13 0.74 2.97</cell></row><row><cell>CVAE</cell><cell cols="10">25.24 23.73 22.70 23.13 23.71 5.57 1.54 0.77 0.37 2.08</cell></row><row><cell>CVAE+S</cell><cell cols="10">26.08 24.94 23.98 23.95 24.75 4.95 2.50 1.77 1.19 2.62</cell></row><row><cell cols="11">FusionVAE 24.18 23.07 22.23 22.88 23.10 4.11 0.59 0.32 0.19 1.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on FusionT-LESS. The best results are printed in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Posterior ablation on the FusionCelebA dataset. The best results are printed in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NLL in 10 -2 BPD</cell><cell></cell><cell></cell><cell cols="3">MSEmin in 10 -2</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 avg</cell></row><row><cell cols="10">MaxAggAdd 248.1 227.6 231.2 228.7 233.9 5.11 0.88 0.86 0.84 1.93</cell></row><row><cell cols="10">MeanAggAdd 270.9 223.3 216.4 214.4 231.3 5.41 1.00 0.79 0.70 1.98</cell></row><row><cell cols="10">BayAggAdd 970.7 294.0 291.4 291.5 462.6 6.03 5.17 5.10 5.12 5.36</cell></row><row><cell>MaxAggAll</cell><cell cols="9">249.6 236.0 223.9 212.7 230.6 6.15 2.82 1.84 1.30 3.03</cell></row><row><cell cols="10">MeanAggAll 252.7 235.2 222.2 213.5 230.9 6.19 2.39 1.52 1.13 2.81</cell></row><row><cell>BayAggAll</cell><cell cols="9">255.6 568.3 414.7 1376.9 653.3 5.10 4.24 1.96 1.39 3.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Aggregation ablation on the FusionCelebA dataset. The best results are printed in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>69. Zaheer, M., Kottur, S., Ravanbakhsh, S., PÃ³czos, B., Salakhutdinov, R., Smola, A.J.: Deep sets. In: NeurIPS. vol. 30, pp. 3391-3401 (2017) 70. Zeng, Y., Lin, Z., Yang, J., Zhang, J., Shechtman, E., Lu, H.: High-resolution image inpainting with iterative confidence feedback and guided upsampling. In: ECCV.</figDesc><table><row><cell>pp. 1-17. Springer (2020)</cell></row><row><cell>71. Zhang, H., Xu, H., Xiao, Y., Guo, X., Ma, J.: Rethinking the image fusion: A fast</cell></row><row><cell>unified image fusion network based on proportional maintenance of gradient and</cell></row><row><cell>intensity. AAAI 34, 12797-12804 (04 2020)</cell></row><row><cell>72. Zhang, Q., Qu, D., Xu, F., Zou, F.: Robust robot grasp detection in multimodal</cell></row><row><cell>fusion. In: MATEC Web of Conferences. vol. 139, p. 00060. EDP Sciences (2017)</cell></row><row><cell>73. Zheng, C., Cham, T.J., Cai, J.: Pluralistic image completion. In: CVPR. pp. 1438-</cell></row><row><cell>1447 (2019)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Main hyperparameters of our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Â± 0.10 15.11 Â± 0.07 14.19 Â± 0.08 13.71 Â± 0.07 15.27 Â± 0.02 CVAE+S 18.45 Â± 0.02 14.64 Â± 0.06 13.22 Â± 0.03 12.32 Â± 0.02 14.81 Â± 0.03 FusionVAE 15.91 Â± 0.03 14.13 Â± 0.07 13.64 Â± 0.09 13.41 Â± 0.10 14.34 Â± 0.07 Mean and standard deviation of the FusionMNIST NLL results in 10 -2 BPD. The best results are printed in bold.</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>17.67 0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell cols="5">10.84 Â± 0.37 5.96 Â± 0.11 6.02 Â± 0.18 6.13 Â± 0.25 7.38 Â± 0.11</cell></row><row><cell>FCN+S</cell><cell cols="5">6.21 Â± 0.65 3.79 Â± 0.04 2.64 Â± 0.07 1.88 Â± 0.08 3.73 Â± 0.22</cell></row><row><cell>CVAE</cell><cell cols="5">3.87 Â± 0.03 1.76 Â± 0.03 1.09 Â± 0.03 0.83 Â± 0.02 1.97 Â± 0.03</cell></row><row><cell>CVAE+S</cell><cell cols="5">3.53 Â± 0.06 1.77 Â± 0.01 1.23 Â± 0.04 1.02 Â± 0.04 1.96 Â± 0.01</cell></row><row><cell cols="6">FusionVAE 3.14 Â± 0.01 1.04 Â± 0.06 0.77 Â± 0.04 0.67 Â± 0.03 1.47 Â± 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Mean and standard deviation of the FusionMNIST MSE min results in 10 -2 . The best results are printed in bold. Â± 9.42 289.1 Â± 6.53 278.9 Â± 4.51 270.3 Â± 3.66 324.0 Â± 5.17 CVAE+S 487.4 Â± 27.45 355.4 Â± 60.39 280.7 Â± 35.12 230.9 Â± 22.59 338.8 Â± 22.99 FusionVAE 251.0 Â± 2.06 222.3 Â± 3.71 226.8 Â± 3.16 224.0 Â± 3.36 231.0 Â± 2.05</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>456.9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Mean and standard deviation of the FusionCelebA NLL results in 10 -2 BPD. The best results are printed in bold. Â± 0.87 20.00 Â± 3.77 17.87 Â± 3.42 15.56 Â± 3.08 16.62 Â± 2.48 FCN+S 11.94 Â± 0.52 18.58 Â± 7.02 14.90 Â± 6.59 11.19 Â± 5.39 14.15 Â± 4.65 CVAE 8.70 Â± 0.42 6.25 Â± 2.16 4.33 Â± 1.77 3.02 Â± 1.37 5.58 Â± 1.21 CVAE+S 9.87 Â± 1.10 9.60 Â± 3.34 7.30 Â± 3.07 5.57 Â± 2.64 8.09 Â± 2.19 FusionVAE 5.82 Â± 0.52 1.10 Â± 0.16 0.93 Â± 0.06 0.84 Â± 0.03 2.18 Â± 0.17</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell>13.07</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Mean and standard deviation of the FusionCelebA MSE min results in 10 -2 . The best results are printed in bold. Â± 0.04 24.00 Â± 0.25 22.98 Â± 0.24 23.34 Â± 0.19 23.90 Â± 0.17 CVAE+S 26.12 Â± 0.23 25.29 Â± 0.27 24.27 Â± 0.25 24.15 Â± 0.20 24.97 Â± 0.16 FusionVAE 24.32 Â± 0.10 23.09 Â± 0.02 22.25 Â± 0.02 22.90 Â± 0.02 23.15 Â± 0.04</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>CVAE</cell><cell>25.27</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Mean and standard deviation of the FusionT-LESS NLL results in 10 -2 BPD. The best results are printed in bold. Â± 0.04 3.32 Â± 0.10 2.50 Â± 0.09 1.96 Â± 0.10 3.43 Â± 0.06 FCN+S 8.83 Â± 1.05 1.95 Â± 0.14 1.28 Â± 0.15 0.86 Â± 0.14 3.26 Â± 0.37 CVAE 5.49 Â± 0.11 1.73 Â± 0.22 0.95 Â± 0.18 0.44 Â± 0.07 2.18 Â± 0.13 CVAE+S 4.87 Â± 0.06 2.98 Â± 0.29 2.06 Â± 0.19 1.27 Â± 0.09 2.81 Â± 0.12 FusionVAE 4.15 Â± 0.03 0.62 Â± 0.03 0.33 Â± 0.03 0.20 Â± 0.02 1.34 Â± 0.02</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>avg</cell></row><row><cell>FCN</cell><cell>5.88</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Mean and standard deviation of the FusionT-LESS MSE min results in 10 -2 . The best results are printed in bold.</figDesc><table /></figure>
		</body>
		<back>
=======
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition is a fundamental task in computer vision. Beyond image classification <ref type="bibr" target="#b15">[16]</ref> that depicts an image using a single semantic label, there exist other recognition tasks that not only predict the class of the object but also localize it using fine-scaled information. In this paper, we consider three popular examples including object detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>, instance segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>, and human pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref>. We notice that, despite the fact that the rapid progress of deep learning <ref type="bibr" target="#b31">[32]</ref> has introduced</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection Instance Segmentation Pose Estimation</head><p>Figure <ref type="figure">1</ref>: The location-sensitive visual recognition tasks, including object detection, instance segmentation, and human pose estimation, can be formulated into localizing an anchor point (in red) and a set of landmarks (in green). Our work aims to offer a unified framework for these tasks. powerful deep networks as the backbone <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b19">20]</ref>, the designs of the head module for detection <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b5">6]</ref>, segmentation <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7]</ref>, and pose estimation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b65">66]</ref> have fallen into individual subfields. This is mainly due to the difference in the prediction target, i.e., a bounding box for detection, a pixel-level mask for segmentation, and a set of keypoints for pose estimation, respectively.</p><p>Going one step further, we merge the aforementioned three tasks into one, named the location-sensitive visual recognition (LSVR). On the basis of the definition, we propose a location-sensitive network (LSNet) as a unified formulation to deal with them all. The LSNet is built upon any network backbone, e.g., those designed for image classification. The key is to relate an object to an anchor point and a set of landmarks that accurately localize the object. In particular, the landmarks should correspond to the four extreme points for object detection, sufficiently dense boundary pixels for instance segmentation, and the keypoints for human pose estimation. Note that the anchor point as well arXiv:2104.04899v1 [cs.CV] 11 Apr 2021 as landmarks can be also used for extracting discriminative features of the object and thus assisting recognition. Figure <ref type="figure">1</ref> illustrates the overall idea.</p><p>The major difficulty of optimizing the LSNet lies in the requirement of fitting objects of different scales and different properties, which existing methods including the smooth-1 loss and the IOU loss suffer cannot satisfy. This motivates us to design a novel loss function named the cross-IOU loss. It assumes that the landmarks are uniformly distributed around the anchor point and thus approximates the IOU (between the prediction and ground-truth) using the coordinate of the offset vectors. The cross-IOU loss is easily implemented in a few lines of codes. Compared to other loss functions, it achieves a better trade-off between the global and local properties and transplants easier to multi-scale feature maps without specific parameter tuning.</p><p>We perform all three tasks on the MS-COCO dataset <ref type="bibr" target="#b35">[36]</ref>. LSNet, equipped with the cross-IOU loss, achieves competitive recognition accuracy. We further equip the LSNet with a pyramid of deformable convolution that extracts discriminative visual cues around the landmarks. As a result, LSNet reports a 53.5% box AP and a 40.2% mask AP, both of which surpass all existing anchorfree methods. For human pose estimation, LSNet reports competitive results without using the heatmaps, offering a new possibility to the community. Moreover, LSNet shows a promising ability in detecting human poses in various scales, some of which were not annotated in the dataset.</p><p>On top of these results, we claim two-fold contributions of this work. First, we present the formulation of locationsensitive visual recognition that inspires the community to consider the common property of these tasks. Second, we propose the LSNet as a unified framework in which the key technical contribution is the cross-IOU loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep neural networks have been widely applied for visual recognition tasks. Among them, image classification <ref type="bibr" target="#b15">[16]</ref> is the fundamental task that facilitates the design of powerful network backbones <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b19">20]</ref>. Beyond image-level description, there exist fine-scaled tasks, including object detection, instance segmentation, and pose estimation, which focus on depicting different aspects of the object. For example, the bounding boxes locate objects simply and efficiently but lack the details, while masks and keypoints reflect the shape and pose of the objects but usually need the bounding boxes to locate object firstly. According to the different properties of different tasks, many representative methods have been developed.</p><p>The object detection methods can be roughly categorized into anchor-based and anchor-free. The anchor-based methods detect objects by placing a pre-defined set of anchor boxes, predicting the class and score for each anchor, and finally regressing the preserved boxes tightly around the objects. The representative methods include Fast R-CNN <ref type="bibr" target="#b20">[21]</ref>, Faster RCNN <ref type="bibr" target="#b48">[49]</ref>, R-FCN <ref type="bibr" target="#b13">[14]</ref>, SSD <ref type="bibr" target="#b37">[38]</ref>, Reti-naNet <ref type="bibr" target="#b34">[35]</ref>, Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref>, etc. On another line, the anchor-free methods usually represent an object as a combination of geometry. Among them, CornerNet <ref type="bibr" target="#b29">[30]</ref> and DeNet <ref type="bibr" target="#b54">[55]</ref> generated a bounding box by predicting a pair of corner keypoints, and CenterNet (keypoint triplets) <ref type="bibr" target="#b16">[17]</ref> and CPNDet <ref type="bibr" target="#b17">[18]</ref> applied semantic information within the objects to filter out the incorrect corner pairs. FCOS <ref type="bibr" target="#b51">[52]</ref>, RepPoints <ref type="bibr" target="#b60">[61]</ref>, FoveaBox <ref type="bibr" target="#b28">[29]</ref>, SAPD <ref type="bibr" target="#b67">[68]</ref>, CenterNet (objects as points) <ref type="bibr" target="#b65">[66]</ref>, YOLO <ref type="bibr" target="#b46">[47]</ref>, etc., defined a bounding box by placing a single point (called anchor point) within the object and predicting its distances to the object boundary.</p><p>For instance segmentation, there are mainly two kinds of methods, namely, the pixel-based and the contour-based methods. The pixel-based methods consider the segmentation problem as predicting the class of each single pixel. One of the representative work is Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, which first predicted the bounding box to help locating the objects, and then used pixel-wise classification to determine the object mask. The contour-based methods instead represent an object by the contour. They often start with a set points that are roughly located around the object boundary, and the points gradually get closer to the object boundary under iteration. The early representative methods are the snake series <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref> and the recent efforts include using deep neural network <ref type="bibr" target="#b21">[22]</ref> and the idea of anchor-free to improve the features, such as DeepSnake <ref type="bibr" target="#b43">[44]</ref> and PolarMask <ref type="bibr" target="#b58">[59]</ref>.</p><p>There are two mainstreams for human pose estimation, namely, the bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref> and topdown methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b36">37]</ref>. The bottom-up methods first detect the human parts and then locates the keypoints in each object, while the top-down first locates all the keypoints in the human body and then composes the individual parts into a person. The keypoints are often sparsely distributed in an image and thus are difficult to be accurately located. A practical solution is to detect the keypoints in a high-resolution feature map, called heatmap <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b9">10]</ref>. However, applying the heatmap makes the optimization hard and introduces a complex post-processing operation. CenterNet <ref type="bibr" target="#b65">[66]</ref> proposes a neat and simple method, which only predicts a center heatmap and the keypoints are obtained by regressing the vector from the center within objects to the keypoints.</p><p>This paper particularly focuses on the anchor-free methods for visual recognition. These methods originated from object detection and have drawn a lot of attention recently. They do not rely on the pre-defined anchor boxes to locate objects but by points and distance. Therefor, the anchor-free methods enjoy the ability to extend into any directions. This offers the researchers a possibility to unify the visual recognition tasks. Recent trends have spent effort in extending the anchor-free methods in object detection into other tasks, e.g., PolarMask <ref type="bibr" target="#b58">[59]</ref> tries to extend the anchor-free into instance segmentation, while Center-Net <ref type="bibr" target="#b65">[66]</ref> applies it into the pose estimation. Compared with our framework, both of them have limitations, which we will give a detailed discussion in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Location-Sensitive Visual Recognition</head><p>Visual recognition tasks start with an image, X. Image classification aims to assign a class label c for the entire image, yet there are more challenging tasks for finescaled recognition. These tasks often focus on the instances (i.e., individual objects) in the image and depict the object properties from different aspects. Typical examples include object detection that uses a rectangular box that tightly cover the object, instance segmentation that finds out each pixel that belongs to the object, and human pose estimation that localizes the landmarks of the object (i.e., human keypoints). We use b â R 4 , s â [0, 1] W ÃH , and k â R KÃ2 to indicate the target of these tasks, where W and H are the image width and height, and K is the number of keypoints.</p><p>An important motivation of our work is that, although these tasks differ from each other in the form of description, they share the common requirement that the model should be sensitive to the location of the anchor and/or landmarks. Throughout the remaining part of this paper, we refer to these tasks as location-sensitive visual recognition and design a unified framework for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LSNet: A Unified Framework</head><p>The proposed location-sensitive network (LSNet) starts with a backbone (e.g., the ResNet <ref type="bibr" target="#b24">[25]</ref>, ResNeXt <ref type="bibr" target="#b59">[60]</ref>, etc.) that extracts features from the input image. We denote the process using x = f (X). Next, an anchor point and a few landmarks are predicted on top of x, denoted as p = g(x).</p><p>Here we define p â R (N +1)Ã2 where N is the number of landmarks, p 0 â R<ref type="foot" target="#foot_1">foot_1</ref> is the anchor point, and</p><formula xml:id="formula_0">p n â R 2 is a landmark for n = 1, 2, . . . , N .</formula><p>As a unified framework, the key is to relate the prediction targets (i.e., b â R 4 , s â [0, 1] W ÃH , and k â R KÃ2 as aforementioned) to p. For object detection, this is done by finding an extreme point (a pixel that belongs to the object and is tangent to the bounding box) on each edge of the bounding box<ref type="foot" target="#foot_0">foot_0</ref> , i.e., N = 4. For instance segmenta-tion, we locate a fixed number (e.g., 36 in the experiments, N = 36) of landmarks along the contour and thus use the formed polygon to approximate the shape of the object 2 . For human pose estimation, we follow the definition of the dataset to learn a fixed number of keypoints, e.g., in the MS-COCO dataset, N = 17.</p><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the pipeline of LSNet. It belongs to the category of anchor-free methods, i.e., there is no need to pre-define a set of anchor boxes for localizing the object. LSNet is partitioned into two stages, where the first stage predicts an anchor point from the FPN head and relates it with a set of landmarks, and the second stage composes the landmarks into an object with the desired geometry (e.g., a bounding box). To facilitate accurate localization, we use the ATSS assigner <ref type="bibr" target="#b63">[64]</ref> to assign more anchor points for each object and extract features with deformable convolution (DCN) upon the predicted landmarks. The entire model is an end-to-end trainable learnable function.</p><p>LSNet receives two sources of supervision for localization and classification, elaborated in Sections 3.3 and 3.4, respectively. The localization loss is added to both stages, where the major contribution is a unified loss that fits the properties of different tasks, and the classification loss is added to the second stage upon the DCN features.</p><p>LSNet extends the border of anchor-free methods for location-sensitive visual recognition. We briefly review two counterparts. (i) CenterNet predicted horizontal or vertical offsets beyond the anchor point for object detection. This limits its ability in finding the extreme points and extracting discriminative features, yet it cannot perform instance segmentation. (ii) PolarMask used a polar coordinate system for instance segmentation, making it difficult to process the situation that a ray intersects the object multiple times at some direction. In comparison, LSNet easily handles the challenging scenarios and report superior performance (see the experimental part, section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross IOU Loss</head><p>The unified framework raises new challenges to the supervision of localization, because the function needs to consider both the global and local properties of the object. To clarify, we notice that the evaluation of object detection and instance segmentation judges if an object is correctly recognized by the global IOU between the prediction and the ground-truth, while pose estimation measures the accuracy by each individual keypoint.</p><p>To this end, we design the cross-IOU loss as the unified supervision.</p><p>The loss is defined upon the predicted and ground-truth objects, and we use p , where p = (p x , p y ), to denote the corresponding ground-truth of the anchor point and landmarks. In the left part, C3-C5 denote the feature maps in the backbone that are connected to the feature pyramid <ref type="bibr" target="#b33">[34]</ref>, and P3-P7 denote the FPN layers used for final prediction. LSNet is partitioned into two stages. In the first stage, we predict a set of offset vectors that relate the anchor point to the extreme points under the supervision of the cross-IOU loss. In the second stage, the predicted vectors are used as the offsets of the deformable convolution (DCN) <ref type="bibr" target="#b14">[15]</ref> to extract complementary visual features around the extreme points. These features are used for refining the localization and predicting the object class.</p><p>We compute the offset from the anchor point to the landmarks in a cross-coordinate system, i.e., q n = (p n,x -p 0,x ) -, (p n,x -p 0,x ) + , (p n,y -p 0,y ) -, (p n,y -p 0,y )</p><formula xml:id="formula_1">+</formula><p>for n = 1, 2, . . . , N , where (a)</p><p>-and (a)</p><p>+ denotes max{-a, 0} and max{a, 0}, respectively. Finally, we write the cross-IOU loss as:</p><formula xml:id="formula_2">cIOU(q n , q n ) = |min {q n -q n }| 1 |max {q n -q n }| 1 ,<label>(1)</label></formula><p>where |â¢| 1 indicates the 1 -norm. In other words, the cross-IOU function rewards the components (q n and q n ) of similar length (in which case the prediction and the groundtruth maximally overlap) and penalizes the components on different directions. Based on the Eqn (1), we define the cross-IOU loss as</p><formula xml:id="formula_3">L cIOU = 1 -1 n N n=1 cIOU(q n , q n ).</formula><p>Obviously, when p n = p n for all n, we have L cIOU = 0 as expected <ref type="foot" target="#foot_2">3</ref> .</p><p>The cross-IOU loss brings a direct benefit that it fits different scales of features without the need of specific parameter tuning. This alleviates the difficulty of integrating multi-scale information, e.g., using the feature pyramid <ref type="bibr" target="#b33">[34]</ref>. In comparison, the smooth-1 loss <ref type="bibr" target="#b20">[21]</ref> is sensitive to the scale of vector (e.g., the loss value tends to be large when the feature resolution is large) and neglects the relationship between the components that are from the same vector. Moreover, by approximating the IOU using individual components, the cross-IOU loss is flexibly transplanted to instance segmentation and human pose estimation, unlike the original IOU loss <ref type="bibr" target="#b62">[63]</ref> that is difficult to compute on polygons (for segmentation) and undefined for discrete keypoints (for pose estimation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPN layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pyramid DCN</head><p>To enhance discriminative information for recognition, we use deformable convolution (DCN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b69">70]</ref> to extract features from the landmarks. The standard DCN has 9 off-sets, while the number of offsets is 4, 36, and 17 for detection, segmentation, and pose estimation, respectively. In the latter two cases, to avoid redundant features extracted from close areas, we sample 9 landmarks uniformly from the candidates. We further build the feature extraction module upon the feature pyramid <ref type="bibr" target="#b33">[34]</ref>. The offsets are adjusted to different stages by accordingly rescaling the vectors.</p><p>We name the proposed method Pyramid-DCN, and illustrate it in Figure <ref type="figure" target="#fig_1">3</ref>. As shown in experiments, both feature extraction from the landmarks and using the pyramid structure improve recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>We evaluate our framework on the MS-COCO 2017 dataset <ref type="bibr" target="#b35">[36]</ref>, which is a popular, large-scale object detection, segmentation, and human pose dataset. For object detection and segmentation, it contains over 118K training images, 5K validation images and 20K test-dev images covering 80 object categories. For human pose, the person instance is labeled with 17 keypoints, containing over 57K training images with 150K person instances, 5K validation images and 20K test-dev images.</p><p>The average precision (AP) metric is applied to characterize the performance of our method as well as other competitors. There are subtle differences in the definition of AP for different tasks. For object detection, AP is calculated the average precision under different bounding box IOU thresholds(from 0.5 to 0.95), while the bounding box IOU is replaced with the mask IOU in instance segmentation. In the human pose task, AP is calculated based on the object keypoint similarity (OKS), which reflects the distance between the predicted keypoints and the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use ResNet <ref type="bibr" target="#b24">[25]</ref>, ResNeXt <ref type="bibr" target="#b59">[60]</ref> and Res2Net <ref type="bibr" target="#b19">[20]</ref> with the weights pre-trained on ImageNet <ref type="bibr" target="#b15">[16]</ref> as our backbones, respectively. The feature pyramid network (FPN) <ref type="bibr" target="#b33">[34]</ref> is applied to deal with objects with different scales. For object detection, we set four vectors for each object to learn to find the four extreme points (top, left, bottom, right). We refer to ExtremeNet <ref type="bibr" target="#b66">[67]</ref> to obtain extreme point annotations from the object mask <ref type="foot" target="#foot_3">4</ref> . For instance segmentation and human pose estimation, we set 36 vectors for each instance to regress the location of contour points and 17 vectors to regress the 17 keypoints.</p><p>Training and Inference. We train our framework on eight NVIDIA Tesla-V100 GPUs with two images on each GPU. The initial learning rate is set as 0.01, the weight decay as 0.0001 and momentum as 0.9. In the ablation study, we use a ResNet-50 <ref type="bibr" target="#b24">[25]</ref> pre-trained on ImageNet <ref type="bibr" target="#b15">[16]</ref> as the backbone, and fine-tune the model for 12 epochs using a single-scale of [800, 1333] and augment the training images with random horizontal flipping. The learning rate decays by a factor of 10 at after the 8th and 11th epochs, respectively. We also use stronger backbones and longer training epochs (24 epochs for object detection, 30 epochs for instance segmentation and 60 epochs for human pose with the learning rate decayed by a factor of 10 after the 16th, 22nd, and 50th epochs, respectively) and multi-scale input images (from [400, 1333] to [960, 1333]) to further improve the recognition accuracy. In the first stage, we only select the anchor point closest to the center of the object as a positive sample. In the second stage, we use the ATSS <ref type="bibr" target="#b63">[64]</ref> assigner to assign the anchor points for each object. The overall loss function is</p><formula xml:id="formula_4">L = L cls + Î² â¢ L vector1 + Î³ â¢ L vector2 ,<label>(2)</label></formula><p>where L cls and L vector denote the Focal loss <ref type="bibr" target="#b34">[35]</ref> and our cross-IOU loss, respectively. We set the balancing coefficients, Î² and Î³, to be 1.0 and 2.0 in the experiments. During the inference, both the single-scale testing and multi-scale testing strategy are applied. We use the scale of [800, 1333] for single-scale testing. For the multi-scale testing, we refer to ATSS <ref type="bibr" target="#b63">[64]</ref> to set the image scales. We also use the nonmaximum suppression (NMS) strategy with a threshold of 0.6 to remove the redundant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection</head><p>Comparisons to SOTA. We evaluate the detection accuracy of LSNet on the MS-COCO test-dev set, the results are shown in Table <ref type="table" target="#tab_0">1</ref>. As Table <ref type="table" target="#tab_0">1</ref> shows, our method is an anchor-free detector, with a backbone of ResNet-50, LSNet achieves a box AP of 44.8% with 12.7 FPS, which has been competitive with other detectors that equipped with deeper backbones. When equipped with stronger backbones, LSNet performs even better. This benefits from our proposed cross-IOU loss. It helps the LSNet to locate the landmarks with high accuracy, the rich global information contained in the landmarks further promote the cross-IOU loss to regress the landmarks more accurately. With the additional corner point verification (CPV) <ref type="bibr" target="#b8">[9]</ref> and multi-scale testing <ref type="bibr" target="#b63">[64]</ref>, LSNet achieves a box AP of 53.5%, which outperforms all the anchor-free detectors as we know.</p><p>Cross-IOU Loss for Vector Regression. To evaluate the performance of cross-IOU loss, we design four contrast experiments on the MS COCO <ref type="bibr" target="#b35">[36]</ref> validation set, which are (i) the GIOU loss <ref type="bibr" target="#b49">[50]</ref> (a variant of the IOU loss) for rectangle bounding box regression, (ii) the smooth-1 loss for rect- LSNet surpasses all competitors in the anchor-free group. The abbreviations are: 'R' -ResNet <ref type="bibr" target="#b24">[25]</ref>, 'X' -ResNeXt <ref type="bibr" target="#b59">[60]</ref>, 'HG' -Hourglass network <ref type="bibr" target="#b39">[40]</ref>, 'R2' -Res2Net <ref type="bibr" target="#b19">[20]</ref>, 'CPV' -corner point verification <ref type="bibr" target="#b8">[9]</ref>, 'MS train ' -multi-scale training, ' â ' -multi-scale testing <ref type="bibr" target="#b63">[64]</ref>.  angle bounding box regression, (iii) the smooth-1 loss for extreme bounding box regression, and (iv) the cross-IOU loss for extreme bounding box regression, respectively. All the experiments are done in the first stage in our framework (shown in Figure <ref type="figure" target="#fig_0">2</ref>) with ResNet-50 <ref type="bibr" target="#b24">[25]</ref> as the backbone, and we train the model for each experiment for 12 epochs. Table <ref type="table" target="#tab_2">2</ref> summarizes the results. We can see that the smooth- Table <ref type="table">3</ref>: The detection accuracy (%) of using different features. PA, PD, and PP denote using the anchor point features along, anchor point features with the single-scale extreme point features, and with the pyramid extreme point features, respectively.</p><p>boxes, respectively. This reveals that it is more difficult to regress an angled vector than a straight vector. By contrast, the cross-IOU loss performs much better than the smooth- </p><p>Figure <ref type="figure">4</ref>: The detection classification loss and average IOU with respect to the number of elapsed epochs. 'DCN feat', 'point feat' and 'extreme feat' denote the features adaptively learned by the DCN kernel, the features at the anchor points and extreme points, respectively. 'Smooth', 'GIOU' and 'Ours' denote the smooth-1 loss, the GIOU loss <ref type="bibr" target="#b49">[50]</ref>, and the cross-IOU loss, respectively. tion of the cross-IOU loss and landmark feature extraction significantly boosts recognition accuracy.</p><p>Landmark Features Improve Precision. The landmarks (in particular, the extreme points in the detection task) are often related to discriminative appearance features, which may benefit visual recognition. To confirm this, we investigate different settings by using the anchor point features alone and integrating the anchor point features with either DCN or extreme point features, respectively. We still report the detection accuracy with all other settings remaining the same as in the previous experiments (studying the cross IOU loss). The results are shown in Figure <ref type="figure">4</ref>. For the smooth-1 loss and the GIOU loss, we regress the rectangle bounding boxes, and the DCN features are extracted by the adaptively learned DCN kernel; for the cross-IOU loss, we use two sets of vectors both of which regress the extreme bounding boxes -the first set is trained to predict the extreme points and extract the extreme features, and we use the extreme features along with the anchor point features to train the second set from scratch. As shown in Figure <ref type="figure">4</ref>, both the extreme and DCN features boost the classification accuracy. Recall that the prior experiments suggested the usefulness of the extreme features are useful for localization, combining the current results, we verify that the features around the landmarks are discriminative and thus benefit visual recognition.</p><p>Pyramid DCN Improves Precision. We further equip the LSNet with the pyramid DCN to extract the multi-scale features around the landmarks. Table <ref type="table">3</ref> shows our method achieves an AP of 36.2% with the features extracted by the Pyramid DCN, which outperforms the AP with single-scale features by a margin of 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Instance Segmentation</head><p>Comparisons to SOTA. We show the instance segmentation inference results evaluated on the MS-COCO test-dev set <ref type="bibr" target="#b35">[36]</ref> on Table <ref type="table" target="#tab_4">4</ref>. LSNet achieves a mask AP of 38.0% and 40.2% using the single-scale and multi-scale testing protocols, respectively, surpassing all published contourbased methods to the best of our knowledge, and the accuracy is even competitive among the pixel-based approaches.</p><p>Comparisons with PolarMask <ref type="bibr" target="#b58">[59]</ref>. It is interesting to further compare our method with PolarMask, the previous best contour-based approach for instance segmentation. The major difference is that PolarMask assumed the entire object boundary to be seen by the anchor point, but this may not be the case especially for some complicated objects. Once the ray along some direction intersects with the border more than once, the method considered only one and thus in-  curred accuracy loss (a typical example is the 'motorcycle' contour in Figure <ref type="figure">5</ref>). In our approach, this issue is solved by ranking the landmarks more flexibly, being compatible to complicated shapes. The Number of Landmarks. LSNet represents each instance using a polygon. Using a larger number of landmarks improves the upper-bound of accuracy, but can also incur heavy computational costs and cause the landmark prediction module difficult to be optimized. To choose a proper number of landmarks, we refer to the ground-truth masks of the MS-COCO validation set and quantize each mask into a polygon that best describes it. We find that using 18, 36, and 72 landmarks achieves APs of 89.0%, 97.4%, and 99.2%, respectively, and we consider N = 36 to be a nice tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Human Pose Estimation</head><p>Comparisons to SOTA. Unlike most of the human pose estimation methods that predict the keypoints using the heatmaps, LSNet predicts the keypoints using regression only. In the experiment, we use the object bounding boxes ('obj-box') and keypoint-boxes ('kps-box') to assign training samples, respectively. We will give a detailed discussion of the difference between the two methods in the Appendix B. On the MS-COCO test-dev set, LSNet reports an AP of 55.7% w/ obj-box and 59.0% w/ kps-box, respectively, which outperform CenterNet-reg <ref type="bibr" target="#b65">[66]</ref> with the Hourglass-104 backbone. However, LSNet does not perform as well as the heatmap-based methods, and we analyze the reason as follows.</p><p>Error Analysis. We can observe that the LSNet struggles particularly in the high OKS regimes, e.g., compared to Pose-AE <ref type="bibr" target="#b38">[39]</ref>, the deficit of AP 50 (for LSNet w/ obj-box) is 3.3% while that of AP 75 grows to 9.0%. Note that using keypoint regression is not as accurate as using the heatmaps for refinement, and thus LSNet is less sensitive in the pixellevel prediction. However, the AP metric of pose estimation is largely impacted by this factor. To show this, we artificially add an average deviation of 1, 2, and 3 pixels to the prediction results of CenterNet-jd <ref type="bibr" target="#b65">[66]</ref> (with a backbone of Hourglass-104). The AP on the MS-COCO validation set is significantly reduced from 64.0% (corresponding to the test AP of 63.0% in Table <ref type="table">5</ref>) to 61.1%, 53.4%, and 44.0%, respectively.</p><p>On the other hand, we use the heatmaps produced by CenterNet-jd (Hourglass-104) to refine the prediction of LSNet w/ obj-box. As a result, the AP on the MS-COCO validation set is improved from 56.5% (corresponding to the test AP of 55.7% in Table <ref type="table">5</ref>) to 60.7%. This suggests that LSNet still needs further manipulation of high-resolution features towards higher pixel-level accuracy. The Benefit of LSNet. Despite the relatively weak pixellevel localization, LSNet (w/ obj-box) enjoys the ability of perceiving multi-scale human instances, many of which are not annotated in the dataset. Some examples are shown in the right side of the Table <ref type="table">5</ref>. Since the ground-truth is not available to evaluate the impact, we refer to the heatmaps of CenterNet-jd (Hourglass-104) to deliberately remove these 'false positives'. Consequently, AP is further improved from 60.7% to 63.0%, comparable with the heatmap-based methods, though the improvement seems less meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative results for LSNet</head><p>We show some visualized results of LSNet in Figure <ref type="figure">5</ref>, inlcuding object detection, instance segmentation and human pose estimation. Please refer to the appendix for more qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper unifies three location-sensitive visual recognition tasks (object detection, instance segmentation, and human pose estimation) using the location-sensitive network (LSNet). The key module that supports the framework is a novel cross-IOU loss that is friendly to receiving supervision from multiple scales. Equipped with a pyramid DCN, LSNet achieves the state-of-the-art performance on anchor-free detection and segmentation. This work suggests that using keypoints to define and localize objects is a promising direction, and we hope to extend our approach to achieve a stronger ability of generalization. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: An illustration of the overall framework of LSNet, using object detection as an example, i.e., N = 4. In the left part, C3-C5 denote the feature maps in the backbone that are connected to the feature pyramid<ref type="bibr" target="#b33">[34]</ref>, and P3-P7 denote the FPN layers used for final prediction. LSNet is partitioned into two stages. In the first stage, we predict a set of offset vectors that relate the anchor point to the extreme points under the supervision of the cross-IOU loss. In the second stage, the predicted vectors are used as the offsets of the deformable convolution (DCN)<ref type="bibr" target="#b14">[15]</ref> to extract complementary visual features around the extreme points. These features are used for refining the localization and predicting the object class.</figDesc><graphic coords="4,311.94,337.92,97.81,151.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the Pyramid DCN structure. The DCN kernel is allowed to be rescaled to the adjacent FPN layers to extract richer features. The blue boxes and arrows denote the positions of convolution determined by the predicted offset vectors (the blue arrows).</figDesc><graphic coords="4,332.21,391.53,57.88,89.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 5 :Figure 5 :</head><label>55</label><figDesc>Figure 5: Some location-sensitive visual recognition results on the MS-COCO validation set. As discussed in Section 4.4, the contour of the 'motorcycles' in the figure cannot be well represented by the polar coordinate system in PolarMask.</figDesc><graphic coords="8,443.84,356.28,95.98,59.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Left: LSNet uses the object bounding boxes to assign training samples. Right: LSNet uses the keypoint-boxes to assign training samples. Although LSNet with keypoint-boxes enjoys a higher AP score, its ability of perceiving multi-scale human instances is weakened.</figDesc><graphic coords="10,95.95,510.93,197.63,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Epoch MStrain FPS</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>Anchor-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Libra R-CNN [41]</cell><cell>X-101-64x4d</cell><cell>12</cell><cell>8.5</cell><cell>43.0</cell><cell>64.0</cell><cell>47.0</cell><cell>25.3</cell><cell>45.6</cell><cell>54.6</cell></row><row><cell>AB+FSAF [69]  â </cell><cell>X-101-64x4d</cell><cell>18</cell><cell>-</cell><cell>44.6</cell><cell>65.2</cell><cell>48.6</cell><cell>29.7</cell><cell>47.1</cell><cell>54.6</cell></row><row><cell>FreeAnchor [65]  â </cell><cell>X-101-32x8d</cell><cell>24</cell><cell>-</cell><cell>47.3</cell><cell>66.3</cell><cell>51.5</cell><cell>30.6</cell><cell>50.4</cell><cell>59.0</cell></row><row><cell>GFLV1 [65]</cell><cell>X-101-32x8d</cell><cell>24</cell><cell cols="2">10.7 48.2</cell><cell>67.4</cell><cell>52.6</cell><cell>29.2</cell><cell>51.7</cell><cell>60.2</cell></row><row><cell>ATSS [64]  â </cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>-</cell><cell>50.7</cell><cell>68.9</cell><cell>56.3</cell><cell>33.2</cell><cell>52.9</cell><cell>62.4</cell></row><row><cell>PAA [28]  â </cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>-</cell><cell>51.4</cell><cell>69.7</cell><cell>57.0</cell><cell>34.0</cell><cell>53.8</cell><cell>64.0</cell></row><row><cell>GFLV2 [33] â </cell><cell>R2-101-DCN</cell><cell>24</cell><cell>-</cell><cell>53.3</cell><cell>70.9</cell><cell>59.2</cell><cell>35.7</cell><cell>56.1</cell><cell>65.6</cell></row><row><cell>YOLOv4-P7 [56] â </cell><cell>CSP-P7</cell><cell>450</cell><cell>-</cell><cell>56.0</cell><cell>73.3</cell><cell>61.2</cell><cell>38.9</cell><cell>60.0</cell><cell>68.6</cell></row><row><cell>Anchor-free:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ExtremeNet [67]  â </cell><cell>HG-104</cell><cell>200</cell><cell>-</cell><cell>43.2</cell><cell>59.8</cell><cell>46.4</cell><cell>24.1</cell><cell>46.0</cell><cell>57.1</cell></row><row><cell>RepPointsV1 [61]  â </cell><cell>R-101-DCN</cell><cell>24</cell><cell>-</cell><cell>46.5</cell><cell>67.4</cell><cell>50.9</cell><cell>30.3</cell><cell>49.7</cell><cell>57.1</cell></row><row><cell>SAPD [68]</cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>4.5</cell><cell>47.4</cell><cell>67.4</cell><cell>51.1</cell><cell>28.1</cell><cell>50.3</cell><cell>61.5</cell></row><row><cell>CornerNet [30]  â </cell><cell>HG-104</cell><cell>200</cell><cell>-</cell><cell>42.1</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell>56.7</cell></row><row><cell>DETR [6]</cell><cell>R-101</cell><cell>500</cell><cell>10</cell><cell>44.9</cell><cell>64.7</cell><cell>47.7</cell><cell>23.7</cell><cell>49.5</cell><cell>62.3</cell></row><row><cell>CenterNet [17]  â </cell><cell>HG-104</cell><cell>190</cell><cell>-</cell><cell>47.0</cell><cell>64.5</cell><cell>50.7</cell><cell>28.9</cell><cell>49.9</cell><cell>58.9</cell></row><row><cell>CPNDet [18]  â </cell><cell>HG-104</cell><cell>100</cell><cell>-</cell><cell>49.2</cell><cell>67.4</cell><cell>53.7</cell><cell>31.0</cell><cell>51.9</cell><cell>62.4</cell></row><row><cell>BorderDet [46]  â </cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>-</cell><cell>50.3</cell><cell>68.9</cell><cell>55.2</cell><cell>32.8</cell><cell>52.8</cell><cell>62.3</cell></row><row><cell>FCOS-BiFPN [53]</cell><cell>X-101-32x8-DCN</cell><cell>24</cell><cell>n/a</cell><cell>50.4</cell><cell>68.9</cell><cell>55.0</cell><cell>33.2</cell><cell>53.0</cell><cell>62.7</cell></row><row><cell>RepPointsV2 [9]  â </cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>-</cell><cell>52.1</cell><cell>70.1</cell><cell>57.5</cell><cell>34.5</cell><cell>54.6</cell><cell>63.6</cell></row><row><cell>LSNet</cell><cell>R-50</cell><cell>24</cell><cell cols="2">12.7 44.8</cell><cell>64.1</cell><cell>48.8</cell><cell>26.6</cell><cell>47.7</cell><cell>55.7</cell></row><row><cell>LSNet</cell><cell>X-101-64x4d</cell><cell>24</cell><cell>7.2</cell><cell>48.2</cell><cell>67.6</cell><cell>52.6</cell><cell>29.6</cell><cell>51.3</cell><cell>60.5</cell></row><row><cell>LSNet</cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>5.9</cell><cell>49.6</cell><cell>69.0</cell><cell>54.1</cell><cell>30.3</cell><cell>52.8</cell><cell>62.8</cell></row><row><cell>LSNet-CPV</cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>5.1</cell><cell>50.4</cell><cell>69.4</cell><cell>54.5</cell><cell>31.0</cell><cell>53.3</cell><cell>64.0</cell></row><row><cell>LSNet-CPV</cell><cell>R2-101-DCN</cell><cell>24</cell><cell>6.3</cell><cell>51.1</cell><cell>70.3</cell><cell>55.2</cell><cell>31.2</cell><cell>54.3</cell><cell>65.0</cell></row><row><cell>LSNet-CPV  â </cell><cell>R2-101-DCN</cell><cell>24</cell><cell>-</cell><cell>53.5</cell><cell>71.1</cell><cell>59.2</cell><cell>35.2</cell><cell>56.4</cell><cell>65.8</cell></row></table><note><p>A comparison between LSNet and the sate-of-the-art methods in object detection on the MS-COCO test-dev set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The bounding box AP (%) with different experimental settings. The cross-IOU loss achieves a high score even when regressing the extreme bounding box which is more difficult. Note that the GIOU loss cannot regress a non-rectangle bounding box.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 loss reports an AP of 33.8% and 31.5% when regressing the rectangle bounding boxes and the the extreme bounding</figDesc><table><row><cell>Loss</cell><cell>PA PE PP AP AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell>34.3 54.9 36.5 19.6 37.6 44.7</cell></row><row><cell>Cross-IOU</cell><cell>35.5 54.8 38.3 20.0 39.2 45.3</cell></row><row><cell></cell><cell>36.2 55.4 38.9 19.8 39.8 46.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of LSNet to the sate-of-the-art methods in instance segmentation task on the COCO test-dev set. Our LSNet achieves the state-of-the-art accuracy for contour-based instance segmentation. 'R': ResNet<ref type="bibr" target="#b24">[25]</ref>, 'X': ResNeXt<ref type="bibr" target="#b59">[60]</ref>, 'HG': Hourglass<ref type="bibr" target="#b39">[40]</ref>, 'R2':Res2Net<ref type="bibr" target="#b19">[20]</ref>, ' â ': multi-scale testing<ref type="bibr" target="#b63">[64]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Backbone</cell><cell>Epoch</cell><cell>AP</cell><cell>AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pixel-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">YOLACT [2]</cell><cell></cell><cell></cell><cell>R-101</cell><cell>48</cell><cell>31.2</cell><cell>50.6</cell><cell>32.8</cell><cell>12.1</cell><cell>33.3</cell><cell>47.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">TensorMask [8]</cell><cell></cell><cell></cell><cell>R-101</cell><cell>72</cell><cell>37.1</cell><cell>59.3</cell><cell>39.4</cell><cell>17.1</cell><cell>39.1</cell><cell>51.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Mask R-CNN [24]</cell><cell></cell><cell>X-101-32x4d</cell><cell>12</cell><cell>37.1</cell><cell>60.0</cell><cell>39.4</cell><cell>16.9</cell><cell>39.9</cell><cell>53.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HTC [7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-101-64x4d</cell><cell>20</cell><cell>41.2</cell><cell>63.9</cell><cell>44.7</cell><cell>22.8</cell><cell>43.9</cell><cell>54.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">DetectoRS [45]  â </cell><cell></cell><cell>X-101-64x4d</cell><cell>40</cell><cell>48.5</cell><cell>72.0</cell><cell>53.3</cell><cell>31.6</cell><cell>50.9</cell><cell>61.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Contour-based:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">ExtremeNet [67]</cell><cell></cell><cell>HG-104</cell><cell>100</cell><cell>18.9</cell><cell>44.5</cell><cell>13.7</cell><cell>10.4</cell><cell>20.4</cell><cell>28.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">DeepSnake [44]</cell><cell></cell><cell>DLA-34 [62]</cell><cell>120</cell><cell>30.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">PolarMask [59]</cell><cell></cell><cell></cell><cell>X-101-64x4d-DCN</cell><cell>24</cell><cell>36.2</cell><cell>59.4</cell><cell>37.7</cell><cell>17.8</cell><cell>37.7</cell><cell>51.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-101-64x4d-DCN</cell><cell>30</cell><cell>37.6</cell><cell>64.0</cell><cell>38.3</cell><cell>22.1</cell><cell>39.9</cell><cell>49.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R2-101-DCN</cell><cell>30</cell><cell>38.0</cell><cell>64.6</cell><cell>39.0</cell><cell>22.4</cell><cell>40.6</cell><cell>49.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LSNet  â </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-101-64x4d-DCN</cell><cell>30</cell><cell>39.7</cell><cell>65.5</cell><cell>41.3</cell><cell>25.5</cell><cell>41.3</cell><cell>50.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LSNet  â </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R2-101-DCN</cell><cell>30</cell><cell>40.2</cell><cell>66.2</cell><cell>42.1</cell><cell>25.8</cell><cell>42.2</cell><cell>51.0</cell></row><row><cell>Detection Clss Loss</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0</cell><cell>0</cell><cell>2</cell><cell>4 Training Epoch 6 8 point feat (Smooth) point feat (GIoU) point with DCN feat (GIoU) 10 point with extreme feat (Ours)</cell><cell>12</cell><cell>Detection Accuracy IoU</cell><cell>0.800 0.775 0.600 0.625 0.650 0.675 0.700 0.725 0.750</cell><cell>0</cell><cell>2</cell><cell>4 Training Epoch 6 8 point feat (Smooth) point feat (GIoU) point with DCN feat (GIoU) 10 point with extreme feat (Ours)</cell><cell>12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As a disclaimer, there may exist multiple or even continuous extreme points on each edge. We assume the method to find any one of them, by which it confirms the prediction of the bounding box.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In case that the mask is not simply-connected in topology, we follow PolarMask<ref type="bibr" target="#b58">[59]</ref> to deal with each part separately and ignore the holes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The current form of L cIOU can cause the gradients over pn,x and pn,y to be 0 when the corresponding dimensions of prediction and groundtruth are of different signs. We design a softened prediction mechanism to solve the issue. Please refer to the Appendix A for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>As a side comment, when annotating the bounding boxes on a dataset, we recommend annotating an object by clicking the four extreme points (top-most, left-most, bottom-most, right-most) of the object. According to<ref type="bibr" target="#b41">[42]</ref>, this way is roughly four times faster than directly annotating the bounding boxes. In addition, the extreme point itself contains the object information as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4"><p>loss and even produces competitive results with the IOU loss for the rectangle bounding box. Although the GIOU loss at present still performs better than the cross-IOU loss, the cross-IOU loss allows the framework regressing the location of the landmarks, thus we could extract the discriminative information around the landmarks to enhance recognition. We will show in the next section that the combina-</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. The Softened Prediction Mechanism for Cross-IOU Loss</head><p>As mentioned in Section 3.3, the form of L cIOU (Equation 1) can cause the gradients over p n,x and p n,y to be 0 when the corresponding dimensions of prediction and ground-truth are of different signs. To solve this problem, we predict four components for each offset vector, as shown in Figure <ref type="figure">6</ref>. Then q n can be rewritten as: q n = [q n,t , q n,l , q n,b , q n,r ], where q n,l , q n,r , q n,t , q n,b are all greater than 0. On the other hand, when transforming q n into the cross-coordinate system, as shown in Figure <ref type="figure">6</ref>, we assign the minimum sides (q n,l and q n,b ) the non-zero value, which are Î± times the corresponding maximum sides (q n,t and q n,r ), where 0 &lt; Î± &lt; 1. In all our experiments, we set Î± = 0.2.</p><p>During inference, we transform the predicted offset vectors from the cross-coordinate system into the rectangular coordinate system by taking the maximum value in the horizontal and vertical direction, respectively, i.e., q n = [max{q n,t , q n,b }, max{q n,l , q n,r }]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Assign Samples by Keypoint-boxes to Improve Human Pose Estimation</head><p>In Section 4.5, we mainly discuss the characteristics of using the object bounding boxes ('obj-box') to assign training samples, which lets LSNet enjoy the ability of perceiving multi-scale human instances especially for small human instances, many of which are not annotated in the dataset. In this section, we mainly discuss the characteristics of using the keypoint-boxes ('kps-box', bounding box generated by the topmost, leftmost, bottommost and rightmost keypoints of an objects) to assign training samples. Compared with the former, the later will no longer treat the human instances that only have the object bounding box annotations but lack of pose annotations as positive samples. This makes the network pay more attention to learn the human instances that have the pose annotations, which helps to improve the AP score. As shown in Table <ref type="table">5</ref>, LSNet using keypoint-boxes reports an AP of 59.0%, an improvement of 3.3% over 55.7%, achieved by LSNet using object bounding boxes.</p><p>However, we find that, with the 'improved' AP score, the ability of the algorithm at perceiving multi-scale human instances is weakened. As shown in Figure <ref type="figure">7</ref>, the modified algorithm mostly fails to detect the small person instances. This proves that the annotations of the dataset is biased.</p></div>			</div>
>>>>>>> main
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CVAE-GAN: fine-grained image generation through asymmetric training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2745" to="2754" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent Kalman networks: Factorized inference in high-dimensional deep feature spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gebhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="544" to="552" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
=======
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3686" to="3693" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2017">2017</date>
=======
<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Realtime instance segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9157" to="9166" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-view 3D object detection network for autonomous driving</title>
=======
<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
>>>>>>> main
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
=======
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Du</surname></persName>
>>>>>>> main
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
=======
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
=======
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
>>>>>>> main
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
=======
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="455" to="472" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Proc. Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PixelVAE: A latent variable model for natural images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
=======
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">FuseGAN: Learning to fuse multi-focus image via conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1982" to="1996" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FFB6D: A full flow bidirectional fusion network for 6D pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3003" to="3013" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PVN3D: A deep point-wise 3D keypoints voting network for 6DoF pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11632" to="11641" />
=======
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2008">2019. 1, 2, 8</date>
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>HodaÅ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å </forename><surname>ObdrÅ¾Ã¡lek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
=======
<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
=======
<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
>>>>>>> main
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative adversarial network with adaptive constraints for multi-focus image fusion</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="15119" to="15129" />
			<date type="published" when="2020">2020</date>
=======
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page" from="448" to="456" />
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised deep image fusion with structure tensor representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3845" to="3858" />
			<date type="published" when="2020">2020</date>
=======
<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2061" to="2069" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SetVAE: Learning hierarchical composition for generative modeling of set-structured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
=======
<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reppoints v2: Verification meets regression for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
>>>>>>> main
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15059" to="15068" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
=======
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016">2016</date>
=======
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognitio</title>
		<meeting>the IEEE conference on computer vision and pattern recognitio</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5386" to="5395" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask-specific inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>KÃ¶hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conf. Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
=======
<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On active contour models and balloons</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image understanding</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1991">1991</date>
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>IROS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>SÃ¸nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
=======
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
>>>>>>> main
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="m">The MNIST database of handwritten digits</title>
		<meeting><address><addrLine>LeCun, Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">2016. 1998</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DenseFuse: A fusion approach to infrared and visible images</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2614" to="2623" />
			<date type="published" when="2018">2018</date>
=======
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Infrared and visible image fusion with ResNet and zero-phase component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Durrani</surname></persName>
=======
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">103039</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Infrared and visible image fusion using a deep learning framework</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2705" to="2710" />
		</imprint>
		<respStmt>
			<orgName>ICPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Generative face completion</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A medical image fusion method based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
=======
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="191" to="207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
=======
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via detail preserving adversarial learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
=======
<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
=======
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
>>>>>>> main
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="2020">2020</date>
=======
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005">2009. 1, 2, 5</date>
			<biblScope unit="page" from="248" to="255" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">DDcGAN: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4980" to="4995" />
			<date type="published" when="2020">2020</date>
=======
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FusionGAN: A generative adversarial network for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2019">2019</date>
=======
<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Corner proposal network for anchor-free, two-stage object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BIVA: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>MaalÃ¸e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>LiÃ©vin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
=======
<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6551" to="6562" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bayesian image reconstruction using deep generative models</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04567[cs.CV</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
=======
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dual contradistinctive generative autoencoder</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
=======
<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
>>>>>>> main
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="823" to="832" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
=======
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007">2019. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust snake implementation; a dual active contour</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="68" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008">2017. 1, 2, 7, 8</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007">2016. 1, 2, 3, 5, 6, 7</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="34" to="50" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">DeepFuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Srikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4714" to="4722" />
=======
<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2006">2018. 1, 2, 6</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cornernetlite: Efficient keypoint based object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2007">2014. 1, 2, 5, 7</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving convolutional networks with self-calibrated convolutions</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10096" to="10105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2017. 2, 8</date>
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2016. 1, 2, 6, 7</date>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4930" to="4939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep snake for real-time instance segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 2, 7</date>
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="549" to="564" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
=======
<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vinci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Amin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09948[cs.CV</idno>
		<title level="m">PixelVAE++: Improved PixelVAE with discrete prior</title>
		<imprint>
			<date type="published" when="2019">2019</date>
=======
<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
=======
<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
=======
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
=======
<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3483" to="3491" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>SÃ¸nderby</surname></persName>
=======
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>MaalÃ¸e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>SÃ¸nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3738" to="3746" />
			<date type="published" when="2016">2016</date>
=======
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
=======
<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fcos: A simple and strong anchor-free object detector</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<title level="m">Contextualbased image inpainting: Infer, match, and translate</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
=======
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020">2020</date>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">SPG-Net: Segmentation prediction and guidance network for image inpainting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
=======
<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Denet: Scalable realtime object detection with directed sparse sampling</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
=======
			<persName><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
>>>>>>> main
		</author>
		<imprint>
<<<<<<< HEAD
			<date type="published" when="2018">2018</date>
			<publisher>BMVC</publisher>
=======
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="428" to="436" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
=======
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
>>>>>>> main
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
	</analytic>
	<monogr>
<<<<<<< HEAD
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19667" to="19679" />
			<date type="published" when="2020">2020</date>
=======
		<title level="m">Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
<<<<<<< HEAD
		<title level="m" type="main">DVAE++: Discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5035" to="5044" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bayesian context aggregation for neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>FlÃ¼renbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grossberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
=======
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
>>>>>>> main
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="835" to="851" />
=======
<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4724" to="4732" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">DenseFusion: 6D object pose estimation by iterative dense fusion</title>
=======
<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
>>>>>>> main
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>MartÃ­n-MartÃ­n</surname></persName>
		</author>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3343" to="3352" />
=======
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 7</date>
			<biblScope unit="page" from="12193" to="12202" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GraspFusionNet: a two-stage multi-parameter grasp detection network based on RGB-XYZ fusion in dense clutter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020</date>
=======
<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007">2017. 1, 2, 3, 5, 6, 7</date>
			<biblScope unit="page" from="1492" to="1500" />
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
=======
<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
>>>>>>> main
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FusionDN: A unified densely connected network for image fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12484" to="12491" />
			<date type="published" when="2020-04">04 2020</date>
=======
<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
<<<<<<< HEAD
		<title level="a" type="main">Shift-Net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
=======
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 3, 5, 6, 7</date>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
>>>>>>> main
		</imprint>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
=======
<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
>>>>>>> main
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2009">2019. 1, 2, 3, 8, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
<<<<<<< HEAD
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
=======
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
>>>>>>> main
		</author>
		<imprint>
<<<<<<< HEAD
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6721" to="6729" />
=======
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">D-CVF: Generating joint camera and lidar features using cross-view spatial feature fusion for 3D object detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="720" to="736" />
=======
<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12448</idno>
		<title level="m">Soft anchorpoint object detection</title>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Feature selective anchorfree module for single-shot object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="840" to="849" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<<<<<<< HEAD
<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5505" to="5514" />
=======
<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9308" to="9316" />
>>>>>>> main
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
