<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOFT: Softmax-free Transformer with Linear Complexity</title>
				<funder ref="#_gz8yuk7 #_GR9xmK7">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Education, China</orgName>
				</funder>
				<funder ref="#_GXf8ZYA">
					<orgName type="full">Shanghai Municipal Science and Technology Major Projects</orgName>
				</funder>
				<funder ref="#_arDJAmE">
					<orgName type="full">Mindspore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-30">30 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinghan</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junge</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab https://fudan-zvg.github.io/SOFT</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiguo</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab https://fudan-zvg.github.io/SOFT</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<email>lizhangfd@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Li Zhang</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SOFT: Softmax-free Transformer with Linear Complexity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-30">30 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">569EC1AA8A797EFCF1EE1C7CE19DD143</idno>
					<idno type="arXiv">arXiv:2110.11945v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-05-30T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the selfattention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently the step change brought by Transformers <ref type="bibr" target="#b33">[34]</ref> in natural language processing (NLP) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> seems to have arrived in vision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b47">47]</ref>. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> and its variants have shown to be able to outperform CNNs on various vision tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity O(n 2 ) with the token sequence length n in both computation (time) and memory (space) since an n × n sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks. Figure <ref type="figure">1</ref>: Top1-Accuracy on ImageNet <ref type="bibr" target="#b8">[9]</ref> validation set with respect to parameters and the memory usage corresponding to the token sequence length in practice compared to other methods. (a) Comparison with CNN models: RegNet <ref type="bibr" target="#b26">[27]</ref>, ResNet <ref type="bibr" target="#b13">[14]</ref> and Transformer models: PVT <ref type="bibr" target="#b35">[36]</ref>, DeiT <ref type="bibr" target="#b31">[32]</ref>,</p><p>ViT <ref type="bibr" target="#b10">[11]</ref>, T2T-ViT <ref type="bibr" target="#b42">[42]</ref>, Twins-SVT <ref type="bibr" target="#b5">[6]</ref> and SAN10 <ref type="bibr" target="#b46">[46]</ref>; (b) Comparison with Transformer <ref type="bibr" target="#b33">[34]</ref>, Linformer <ref type="bibr" target="#b34">[35]</ref>, Nyströformer <ref type="bibr" target="#b39">[40]</ref> and Performer <ref type="bibr" target="#b4">[5]</ref>. The memory usage is measured with a batch size of 1 on a 16GB Tesla V100.</p><p>A natural solution is to reduce the complexity of self-attention computation via approximation. Indeed, there have been a number of attempts in NLP <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. For example, <ref type="bibr" target="#b34">[35]</ref> takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention. <ref type="bibr" target="#b18">[19]</ref> instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, <ref type="bibr" target="#b39">[40]</ref> uses Nyström matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2).</p><p>In this work, we identify that the limitations of existing efficient Transformers are caused by the use of softmax self-attention, and for the first time propose a softmax-free Transformer. More specifically, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors <ref type="bibr" target="#b33">[34]</ref>. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity O(n) in both space and time. Specifically, SOFT uses Gaussian kernel to define the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix.</p><p>We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure <ref type="figure">1b</ref>), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on ImageNet <ref type="bibr" target="#b8">[9]</ref> classification in the accuracy/complexity trade-off (Figure <ref type="figure">1a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Vision Transformers There is a surge of research interests recently in exploiting Transformers for visual recognition tasks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">45]</ref>, inspired by their remarkable success in NLP <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Core to these NLP and vision transformers is the same self-attention mechanism <ref type="bibr" target="#b33">[34]</ref> that computes a self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity with the sequence length in both space and time, which thus limits the scalability of Transformers in dealing with long sequences. This limitation is more serious in vision than NLP: To process an image with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control the computational cost. Given higher resolution images, the patch size also needs to be enlarged proportionally sacrificing the spatial resolution. This limits the capability of Transformers, e.g., learning fine-grained feature representation as required in many visual recognition tasks.</p><p>Linear Transformers Recently, there have been a number of linear/efficient variants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref> of Transformers in NLP. For example, <ref type="bibr" target="#b34">[35]</ref> learns to shrink the length of Key and Value based on a low-rank assumption. <ref type="bibr" target="#b18">[19]</ref> adopts a hashing strategy to selective the most similar pairs and only compute attention among them. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> utilize different kernel functions for approximating softmax-based self-attention matrix. <ref type="bibr" target="#b24">[25]</ref> applies random feature mapping on the sequences to approach the original softmax function. <ref type="bibr" target="#b16">[17]</ref> decreases the time and memory consumption of the attention matrix by replacing the softmax function with its linear-complexity recurrent alternative. When applied to visual recognition tasks, however, we show that these models have considerable performance degradation compared to the standard Transformers <ref type="bibr" target="#b33">[34]</ref> (see Sec. 4.2).</p><p>The most related work to SOFT is <ref type="bibr" target="#b39">[40]</ref> which uses the Nyström matrix decomposition to avoid computing the full attention matrix. However, this method suffers from several theoretical defects:</p><p>(1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention matrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply applied to all the ingredient matrices in <ref type="bibr" target="#b39">[40]</ref>. Such an approximation is not guaranteed theoretically.</p><p>(2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix inverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the above methods, in this paper we propose a softmax-free self-attention mechanism that facilitates matrix decomposition for complexity minimization with theoretical guarantees.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Softmax-free self-attention formulation</head><p>A schematic illustration of our model is given in Figure <ref type="figure">2</ref>. Let's first look at our attention module design. Given a sequence of n tokens X ∈ R n×d with each token represented by a d-dimensional feature vector, self-attention <ref type="bibr" target="#b33">[34]</ref> aims to discover the correlations of all token pairs exhaustively.</p><p>Formally, X is first linearly projected into three d e -dimensional spaces (query, key, and values) as:</p><formula xml:id="formula_0">Q = XW q ∈ R n×de , K = XW k ∈ R n×de , V = XW v ∈ R n×de ,<label>(1)</label></formula><p>where W q , W k , W v ∈ R d×de are learnable matrices. Self-attention can be expressed in a generic formulation as:</p><formula xml:id="formula_1">y i,: = n j=1 α(Q i,: , K j,: ) V j,: ,<label>(2)</label></formula><p>where is the Hadamard product, and i, j ∈ {1, • • • , n} index the tokens. The key self-attention function α : R de × R de → R is composed of a nonlinear function β : R → R and a relation function γ : R de × R de → R. A dominant instantiation of α is the scaled dot-product based softmax self-attention <ref type="bibr" target="#b33">[34]</ref>, defined as</p><formula xml:id="formula_2">β(•) = softmax(•), γ(Q i,: , K j,: ) = 1 √ d e • Q i,: K j,: .<label>(3)</label></formula><p>Whilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed earlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we introduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel as:</p><formula xml:id="formula_3">β (•) = exp(•), γ (Q i,: , K j,: ) = - 1 2 √ d e • Q i,: -K j,:<label>2 2 .</label></formula><p>(4)</p><p>Figure <ref type="figure">2</ref>: Schematic illustration of the proposed softmax-free self-attention (SOFT) method. P.E.: Position embedding. Dash lines: linear projection. dh: the hidden dim of each attention head. • denotes the matrix dot product.</p><p>To preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices W q and W k in Eq (1) identical (i.e., Q = K). Our self-attention matrix is then written as:</p><formula xml:id="formula_4">S i,j = exp - 1 2 √ d e • Q i,: -K j,:<label>2 2 .</label></formula><p>(</p><formula xml:id="formula_5">)<label>5</label></formula><p>For notation simplicity, we define the matrix formulation as: S = exp (Q K).</p><p>Remarks Our self-attention matrix S has three important properties: (1) It is symmetric;</p><p>(2) All the elements lie in a unit range of [0, 1]; (3) All diagonal elements hold the largest value 1 (selfreinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to 0.</p><p>As Gaussian kernel is a positive definite kernel <ref type="bibr" target="#b11">[12]</ref>, S is deemed a Gram matrix. However, we find that when using our kernel-based self-attention matrix S without linearization, the training of a transformer fails to converge. This might explain why softmax dot-product based self-attention <ref type="bibr" target="#b33">[34]</ref> is so popular in vanilla transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-rank regularization via matrix decomposition with linear complexity</head><p>To solve the convergence and quadratic complexity problems, we leverage matrix decomposition as a unified solution with low-rank regularization. In particular, we consider Nyström <ref type="bibr" target="#b38">[39]</ref>, which is originally a low-rank matrix approximation algorithm. This enables our model's complexity to be reduced significantly without computing the full self-attention matrix S.</p><p>We make this choice because our S is positive semi-definite (i.e., a Gram matrix) without follow-up normalization which are all necessary conditions for Nyström. In contrast, <ref type="bibr" target="#b39">[40]</ref> totally ignores these requirements, leading to theoretical flaw in its approximation.</p><p>To define the Nyström method formally, let us express S = exp (Q K) as a block matrix:</p><formula xml:id="formula_6">S = A B B C ∈ R n×n ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">A ∈ R m×m , B ∈ R m×(n-m) , C ∈ R (n-m)×(n-m) with m n.</formula><p>Through Nyström decomposition (see derivative details in Appendix A.1), an approximation can be represented as:</p><formula xml:id="formula_8">Ŝ = A B A † [A B] = P A † P, where P = [A B] ,<label>(7)</label></formula><p>and A † is the Moore-Penrose (a generalized) inverse of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling</head><p>In the standard Nyström formulation, A and B are sub-matrices of S obtained by randomly sampled m tokens, denoted as Q. We call the sampled Q as bottleneck tokens. However, Algorithm 1: SOFT: Softmax-free attention</p><formula xml:id="formula_9">Input: Q ∈ R n×de , sampling function f s Sampling Q ← f s (Q) ; A ← exp( Q Q), P ← exp( Q Q); Ŝ ← P NR(A)P ; Output: Ŝ Algorithm 2: NR: Newton-Raphson iteration Input: A ∈ R m×m , and T ∈ Z + α = 2/ A 2 1 .Initialize A 0 ← αA; for k from 1 to T do A k ← 2A k-1 -A k-1 AA k-1 end Output: A T</formula><p>we find empirically that random sampling is considerably sensitive to the choice of m. We hence explore two additional options by leveraging the structural prior of visual data: (1) Using one convolutional layer with kernel size k and stride k to learn Q, and (2) Using average pooling with kernel size k and stride k to generate Q. For both, we need to reshape Q to the form of R H×W ×de . Each slide of convolution or pooling produces a token. We set k according to the length of Q such that m tokens can be obtained. Our experiments show that a convolution layer performs better in accuracy. We therefore use a convolution layer by default.</p><p>As K is identical to Q, we have K = Q. Given these m tokens, we then compute A and P as:</p><formula xml:id="formula_10">A = exp( Q K), P = exp( Q K).<label>(8)</label></formula><p>We finally obtain the regularized self-attention matrix Ŝ of SOFT as:</p><formula xml:id="formula_11">Ŝ = exp Q K exp Q K † exp Q K ,<label>(9)</label></formula><p>leading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the attention score between any two tokens, we first correlate each of them with sampled tokens using our self-attention function (Eq (5)); With this correlation representation we then compute their similarity under the modulation of the generalized inverse of Q's correlation matrix. Similar as standard Nyström, our design associates the input tokens w.r.t. a small space spanned by sampled tokens, giving a proper estimation of the original attention relationships subject to a low-rank constraint. The correctness of this method is proved in Appendix A.1.</p><p>Moore-Penrose inverse An accurate and commonly used way to calculate the Moore-Penrose inverse is to use Singular Value Decomposition (SVD). Given A ∈ R m×m and its SVD form A = U ΣV where U, V are m × m unitary matrices and Σ is a m × m diagonal matrix, the Moore-Penrose inverse of A is A † = V Σ † U . Nevertheless, SVD is not friendly to the training process on GPU hence harming the model training efficiency. To solve this issue, we adopt the Newton-Raphson method. It is an iterative algorithm with the (k + 1)-th iteration formulated given the previous iteration as:</p><formula xml:id="formula_12">A k+1 = 2A k -A k AA k , and A 0 = αA. (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>We now prove that A k finally converges to Moore-Penrose inverse of A m×m , if α is sufficiently small <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_14">Theorem 1 When α is sufficiently small, A k+1 = 2A k -A k AA k , A k converges to A † .</formula><p>Though α = 2/ A 2 1 which ensures good convergence behavior in Algorithm 2 (see more details in Appendix A.2.1), in practice, we find that using an alternative form gives more stable training and faster convergence. Specifically, in</p><formula xml:id="formula_15">I -A 2β n A 2 1</formula><p>1 ≤ 1 where β equals to 0.5, we find the smallest n i that holds this inequality. Then, we initialize α as α =</p><formula xml:id="formula_16">2β n i A 2 1 .</formula><p>The following proposition comes with the proof of Theorem 1:</p><formula xml:id="formula_17">Proposition 1 AA k A -A and A k -A † decreases to 0 monotonously, if α is sufficiently small.</formula><p>The detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is sufficiently accurate for matrix decomposition, subject to that our SOFT attention is regularized. </p><formula xml:id="formula_18">(n × m) + O(m × m) + O(m × n) + O(n × d e ) = O((2m + d e )n + m 2 ).</formula><p>As we keep m (m n) a fixed constant in our model, both time and space complexity are O(n), making SOFT a linear self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instantiations</head><p>Figure <ref type="figure">2</ref> shows how our proposed softmax-free self-attention block (SOFT block) can be implemented in a neural network. We replace the self-attention block with our SOFT block in the traditional Transformer, that is, we stack a SOFT block with a feed forward residual block <ref type="bibr" target="#b10">[11]</ref> to form a softmax-free Transformer layer (SOFT layer).</p><p>Focusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyramidal Transformer architecture <ref type="bibr" target="#b35">[36]</ref> to form our final model SOFT. Further, several improvements are introduced in patch embedding (i.e., tokenization). Specifically, unlike <ref type="bibr" target="#b35">[36]</ref> that uses a combination of non-overlapping convolution and layer normalization <ref type="bibr" target="#b0">[1]</ref>, we adopt a stack of overlapping convolutions, batch normalization <ref type="bibr" target="#b14">[15]</ref> and ReLU non-linearity. Concretely, the STEM is implemented by 3 units of 3x3 Conv→BN→ReLU, with the stride of 2, 1, 2 respectively. Then, one such unit is applied to each of three following down-sampling operations with stride of 2 in the multi-stage architecture.</p><p>The architecture hyper-parameters of SOFT are: d: the input channel dimension of SOFT layer. d e : the embedding dimension of tokens in SOFT block. In practice, we set d e = d. h: the head number of SOFT block. d h : the channel dimension of each head and d h = d e /h. n: the input token sequence length of a SOFT block. m: the bottleneck token sequence length of SOFT block. sp: the sampling ratio of token sequence length sampling, which is the ratio between input token sequence length and the bottleneck token sequence length. e: the expansion ratio of the 2-layer feed forward block. In SOFT, for all the stages we set d h = 32, e = 4 and m = 49, sp varies in each stage according to the input token sequence length. Table <ref type="table" target="#tab_1">2</ref> details the family of our SOFT configurations with varying capacities (depth and width).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Dataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset <ref type="bibr" target="#b8">[9]</ref> with 1.28M training images and 50K validation images from 1,000 classes. Following the common practice, we train a model on the training set and evaluate on the validation set. Metrics: For model performance, the top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the model size and floating point operations (i.e., FLOPs). Implementation details: We use the code base <ref type="bibr" target="#b37">[38]</ref> with the default setting to train and test all the models. Specifically, we use weight decay of 0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an AdamW optimizer and decreasing learning rate with the cosine annealing schedule. During training, random flipping, mixup <ref type="bibr" target="#b44">[44]</ref> and cutmix <ref type="bibr" target="#b43">[43]</ref> are adopted for data augmentation. Label smoothing <ref type="bibr" target="#b28">[29]</ref> is used for loss calculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We also implement our method using the Mindspore <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with existing linear Transformers</head><p>We compare our method with three existing linear Transformer models: Linformer <ref type="bibr" target="#b34">[35]</ref>, Performer <ref type="bibr" target="#b4">[5]</ref>, Nyströmformer <ref type="bibr" target="#b39">[40]</ref> in terms of model complexity and accuracy.</p><p>Two experimental settings are adopted. Under the first setting, for all methods we use the same Tiny (Table <ref type="table" target="#tab_1">2</ref>) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT with each baseline's own attention block with the rest of the architecture unchanged. Note that the spatial reduction module of <ref type="bibr" target="#b35">[36]</ref> is a special case of Linformer <ref type="bibr" target="#b34">[35]</ref>. We set the reduction ratio to be identical to ours. With the same uniform sampling idea, we replace the 1D window averaging of Nyströmformer <ref type="bibr" target="#b39">[40]</ref> (for NLP tasks) with 2D average pooling (for images). The downsampling ratio remains identical to ours. It is also worth mentioning that there is no official code released for Reformer <ref type="bibr" target="#b18">[19]</ref> and the local Sensitive Hash (LSH) module has strict requirements on the length of input tokens. We thus do not include this method in our comparison.</p><p>From Table <ref type="table" target="#tab_0">1</ref> we can make the following observations: (i) Linear Transformer methods substantially reduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer on the Tiny architecture; (ii) Our approach SOFT achieves the best classification accuracy among all the linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers and our training speed is slightly slower than Nystromformer and both are slower than Performer and Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson iteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse. In summary, due to the on-par inference speed we consider the training cost increase is a price worth paying for our superior accuracy.</p><p>Under the second setting, we focus on the memory efficiency of SOFT against the baselines. Here we follow the ViT <ref type="bibr" target="#b10">[11]</ref> network structure, stacking 12 attention layers with hidden dimension d = 384, heads h = 12, bottleneck token sequence length m = 49. Different attention blocks from the three linearized Transformer variants, Linformer <ref type="bibr" target="#b34">[35]</ref>, Performer <ref type="bibr" target="#b4">[5]</ref>, and Nyströmformer <ref type="bibr" target="#b39">[40]</ref> are studied.</p><p>For each Transformer variant, we adjust its token sequence length n in a linear increment. Specifically, we use a token sequence length of 784 × p where p = 1, 2, 3, 4, 5, 6, 7, 8 and set batch size 1 to verify whether the memory consumption increases "quadratically" or "linearly". Figure <ref type="figure">1b</ref> shows all compared transformer variants including our SOFT indeed have a linear memory usage complexity. This is in contrast with the standard Transformer which cannot cope with long token sequences with a quadratic complexity. Table <ref type="table">3</ref>: Evaluation results on ILSVRC-2012 ImageNet-1K <ref type="bibr" target="#b8">[9]</ref> validation set. We report the results using the input size of 224x224 pixels center cropped from resized images with 256x256 pixels. M.S.Out. stands for whether the model is designed for multi-scale output. †: Corrected FLOPs by taking into account the cost of attention matrix multiplication overlooked in the origin paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art CNNs and ViTs</head><p>We compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K validation set. FLOPs are calculated at batch size 1. From Figure <ref type="figure">1a</ref> and Table <ref type="table">3</ref>, the following observations are made: (i) Overall, ViT and its variants yield better classification accuracy over CNNs.</p><p>(ii) We achieve the best performance among the recent pure vision Transformer based methods including ViT <ref type="bibr" target="#b10">[11]</ref> and DeiT <ref type="bibr" target="#b31">[32]</ref>, as well as the state-of-the-art CNN RegNet <ref type="bibr" target="#b26">[27]</ref>. (iii) Our SOFT outperforms the most similar (in architecture configuration) Transformer counterparts PVT <ref type="bibr" target="#b35">[36]</ref> at all variants. Since the attention module is the main difference, this validates directly the effectiveness of our model. (iv) We can also beat the latest ViT variants Twins <ref type="bibr" target="#b5">[6]</ref> which is designed to address the efficiency limitation of ViT. We have done so with less parameters and fewer float point computation.</p><p>To gain some insights into how attention is learned using our SOFT and the alternatives, Figure <ref type="figure" target="#fig_1">3</ref> shows the attention masks of various compared models. For each model, we show the output from the first two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local and long distance relations among pixels. It is interesting to note that, although SOFT is trained on an object categorization dataset in ImageNet <ref type="bibr" target="#b8">[9]</ref>, it seems to be able to learn both semantic concepts shared across instances in the same category and instance specific features. For instance, in the bottom-right example of a bird class, one attention head focuses on the black bird only, while the other attend to both birds in the image. More examples are shown in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>Pyramidal architecture: Unlike the earlier non-pyramidal vision Transformers (e.g., ViT <ref type="bibr" target="#b10">[11]</ref>), most recent pyramidal (multi-scale) Transformers (e.g., PVT <ref type="bibr" target="#b35">[36]</ref>) use convolution layers to reduce the spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT with a pyramidal architecture (our default SOFT-Small), SOFT w/o a pyramidal architecture and DeiT-S <ref type="bibr" target="#b31">[32]</ref> (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer to get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs. Table <ref type="table" target="#tab_4">5a</ref> shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal design, and our non-pyramidal counterpart is even slightly better than DeiT-S <ref type="bibr" target="#b31">[32]</ref> whilst enjoying linear complexity. Bottleneck token sequence length: In this study, we examine how the bottleneck token sequence length m, sampled from n tokens, influences the model's performance. We change the bottleneck token sequence length in all stages to 36, 49, 64, 81. Table <ref type="table" target="#tab_3">4a</ref> shows that longer bottleneck token would increase the memory cost and the computational overhead. m = 49 seems to give the best trade-off between the performance and computational overhead. The memory usage is measured with the batch size of 128.</p><p>Token sampling: The sampling function in SOFT can assume different forms. Convolution: The sequence Q ∈ R n×de is first reshaped to a feature map R H×W ×de . r × r convolution kernel with stride of r is applied for downsampling, where r = √ sp. The output channel size is also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average pooling: using a r × r kernel and r stride, where r = √ sp. Random sampling: m tokens are randomly picked from n tokens. Biased sampling: We pick m tokens with a biased policy. Here, the first m tokens are picked. Table <ref type="table" target="#tab_3">4b</ref> shows that average pooling yields the best performance while with less computational overhead comparing to convolution. Biased sampling can miss the most salient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen samples. This result thus justifies the choice of using average pooling in SOFT. Newton-Raphson's convergence: We study how many iterations the Newton-Raphson method needs to converge when computing the Moore-Penrose inverse A † . We use AA k A -A p / A p with p = 2 (see Proposition 1) as the convergence metric to quantify the difference between A k and A † . Figure <ref type="figure">4</ref> shows that our approximation converges within 20 iterations across all stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottleneck Memory FLOPs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Additional experiments on NLP tasks</head><p>In this section, we evaluate our method against other linear counterparts on four tasks of the Long Range Arena (LRA) <ref type="bibr" target="#b29">[30]</ref> benchmark e.g., Listops <ref type="bibr" target="#b23">[24]</ref>, byte-level IMDb reviews text classification <ref type="bibr" target="#b21">[22]</ref>, byte-level document retrieval <ref type="bibr" target="#b25">[26]</ref>, and image classification on sequences of pixels <ref type="bibr" target="#b19">[20]</ref>.</p><p>Implementations. We use the Pytorch version of LRA <ref type="bibr" target="#b29">[30]</ref> benchmark, provided by <ref type="bibr" target="#b39">[40]</ref>. For the evaluation protocol, we strictly follow <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>. We omit the Pathfinder(1K) task as we cannot replicate the result of Nyströmformer <ref type="bibr" target="#b39">[40]</ref>. For our SOFT, we simply use the average pooling with window size 4, stride 4 to sample the bottlenecks. We follow the configurations of <ref type="bibr" target="#b39">[40]</ref>, with 2 layers, 64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table <ref type="table" target="#tab_6">6</ref> shows that our SOFT outperforms both the standard and alternative efficient Transformers on three out of four tasks, as well as the average result.   <ref type="bibr" target="#b29">[30]</ref>, based on its official configuration. Our SOFT surpasses previous efficient methods on three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Transformer's complexity in space and time. Unlike existing linear Transformers that aim to approximate the conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention which eliminates the need for softmax normalization. This design enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments show that SOFT yields superior trade-off in accuracy and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Nyström method</p><p>Nyström method <ref type="bibr" target="#b38">[39]</ref> aims to calculate a low-rank approximation for a Gram matrix. For Transformers, the self-attention matrix can be viewed as a Gram matrix S with a Gaussian kernel k applied to the query Q, with each element S ij expressed as:</p><formula xml:id="formula_19">S ij = k Q i,: , Q j,: = exp(- Q i,: -Q j,:<label>2</label></formula><formula xml:id="formula_20">2 2 √ d ),<label>(11)</label></formula><p>k(x, y) means operating Gaussian kernel k to (x, y), which can be written in the feature space as:</p><formula xml:id="formula_21">k(x, y) = n i=1 λ i φ i (x)φ i (y),<label>(12)</label></formula><p>n is the dimension of a feature space, λ i denotes the eigenvalue and φ i denotes the eigenfunction of kernel k. According to the eigenfunction's definition, we can get:</p><formula xml:id="formula_22">k(y, x)φ i (x)p(x)dx = λ i φ i (y),<label>(13)</label></formula><p>where p(x) is the probability distribution of x. And {φ i (x)} are p-orthogonal:</p><formula xml:id="formula_23">φ i (x)φ j (x)p(x)dx = δ ij . (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>δ ij is 0 when i = j, 1 when i = j. To get an approximation of the eigenfunctions, we sample {x 1 , x 2 , • • • , x q } from p(x), then:</p><formula xml:id="formula_25">1 q q t=1 k(y, x t )φ i (x t ) ≈ λ i φ i (y),<label>(15) 1 q</label></formula><formula xml:id="formula_26">q t=1 φ i (x t )φ j (x t ) ≈ δ ij .<label>(16)</label></formula><p>This inspires us to approximate the Gram matrix S. Let S (m) be a submatrix of S, consisting of m × m elements from S. Gram matrix is a symmetric positive semi-definite matrix, so it has a spectral decomposition:</p><formula xml:id="formula_27">S (m) U (m) = U (m) Λ (m) ,<label>(17)</label></formula><p>where U (m) is column orthogonal and Λ (m) is a diagonal matrix with the diagonal elements as the eigenvalues of S (m) . Substituting the y to x j and applying the approximation above to S, we can get:</p><formula xml:id="formula_28">φ i (x j ) ≈ √ mU (m) j,i , λ i ≈ λ (m) i m ,<label>(18)</label></formula><formula xml:id="formula_29">φ i (y) ≈ √ m λ (m) i m t=1 k(y, x t )φ i (x t ),<label>(19)</label></formula><formula xml:id="formula_30">λ i is eigenvalue of S and λ (m) i</formula><p>is the eigenvalue of S (m) . Denote S as the rank-m approximation of S and Ũ , Λ as the approximation for spectral decomposition of S. Now we can get an approximation of S with rank m:</p><formula xml:id="formula_31">S = Ũ Λ Ũ T = m t=1 λt (n) ũ(n) t (ũ (n) t ) T .<label>(20)</label></formula><p>Similarly, we have:</p><formula xml:id="formula_32">φ i (x j ) ≈ √ nU j,i (n), λ i ≈ λi (n) n .<label>(21)</label></formula><p>Thus</p><formula xml:id="formula_33">λi (n) ≈ nλ (m) i m ,<label>(22)</label></formula><formula xml:id="formula_34">ũ(n) t ≈ m n 1 λ (m) t S n,m u (m) t .<label>(23)</label></formula><p>Then we get an approximation of S: S ≈ S n,m S † m,m S m,n . S has a block representation below:</p><formula xml:id="formula_35">S = S m,m S m,n-m S n-m,m S n-m,n-m .<label>(24)</label></formula><p>A.2 Newton method A.2.1 Proof of theorem 1 Proof A.1 A is a symmetric positive semi-definite matrix and</p><formula xml:id="formula_36">A ij ≤ 1, ∀1 ≤ i, j ≤ n, A ii = 1, 1 ≤ i ≤ n in our case.</formula><p>A 0 is chosen to be αA, so the A k can be written as A k = C k A = AD k for some matrix C k , D k , leading to the fact that</p><formula xml:id="formula_37">A † AA k = A k , A k AA † = A k . (<label>25</label></formula><formula xml:id="formula_38">)</formula><p>This is because A k+1 = A k (2I n -AA k ) = (2I n -A k A)A k and A 0 = αA. We make a difference between A † and A k+1 :</p><formula xml:id="formula_39">A † -A k+1 = A † -2A k + A k AA k = A † -A k AA † -A † AA k + A k AA k = (A † -A k )A(A † -A k ).<label>(26)</label></formula><p>We norm both sides of the equation above:</p><formula xml:id="formula_40">A † -A k+1 = (A † -A k )A(A † -A k ) ≤ A † -A k A(A † -A k ) .<label>(27)</label></formula><p>And we left multiply A on the both sides of (26), then norm the equation:</p><formula xml:id="formula_41">AA † -AA k+1 = A(A † -A k )A(A † -A k ) ≤ AA † -AA k 2 . (<label>28</label></formula><formula xml:id="formula_42">)</formula><p>We choose α sufficiently small so that the initial value satisfy AA † -AA 0 &lt; 1. We set α = 2</p><formula xml:id="formula_43">A 2 1</formula><p>to ensure it is small enough <ref type="bibr" target="#b2">[3]</ref>. Then the AA † -AA k → 0, when k → ∞. The inequality <ref type="bibr" target="#b26">(27)</ref> implies that A k → A † , k → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Proof of proposition 1</head><p>Proof A.2 Note that when we multiply A on both sides of (26), the equation turns to be:</p><formula xml:id="formula_44">A -AA k+1 A = A(A † -A k )A(A † -A k )A = (AA † -AA k )(A -AA k A). (<label>29</label></formula><formula xml:id="formula_45">)</formula><p>Similarly norm both sides of (29), considering that AA † -AA k → 0 and AA † -AA k &lt; 1 always holds, A-AA k A monotonically decreases to 0. The inequality <ref type="bibr" target="#b26">(27)</ref> implies that A k -A † decreases to 0 monotonously .</p><p>Note that although A -AA k A monotonically decreases to 0, A k AA k -A k cannot be proved to monotonically decrease to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Non-linearized gaussian kernel attention</head><p>In our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated. More specifically, the relation between any two tokens is reconstructed via sampled bottleneck tokens. As the number m (e.g., 49), of bottleneck tokens is much smaller than the token sequence length, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now focuses the attentive learning on latent salient information captured by the m bottleneck tokens. (II)</p><p>The model becomes more robust against the underlying token noise due to the auto-encoder style reconstruction <ref type="bibr" target="#b12">[13]</ref>.</p><p>This explains why the model with an approximated gram matrix performs better than the one with a directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training difficulties. We first hypothesized that this might be due to lacking normalization (as normalization often helps with training stability and convergence), and tested a variant with softmax on top of an exact Gaussian kernel attention matrix. However, it turns out to suffer from a similar failure. We cannot find a solid hypothesis so far and will keep investigate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Attention visualization</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows more visualization of the attention masks by various Transformers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40]</ref> and our SOFT. For each model, we show the output from the first two attention heads (up and down row). It is noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other methods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to more precisely capture all these objects. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing the attention heatmaps of a query patch (marked by the cross "+") against all the patches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nystromformer [40] and (d) Our SOFT. See Appendix A.4 for more examples.</figDesc><graphic coords="9,108.00,72.00,395.96,107.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Figure 4 :</head><label>24</label><figDesc>Figure 4: Convergence analysis for the approximation of the Moore-Penrose inverse on SOFT-Tiny.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing the attention heatmaps of a query patch (marked by the cross "+") against all the patches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nyströmformer [40] and (d) Our SOFT.</figDesc><graphic coords="16,119.88,144.60,372.24,463.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,108.00,72.00,396.00,148.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different linear/efficient transformer variants on ImageNet<ref type="bibr" target="#b8">[9]</ref>, based on our multi-stage Tiny configuration (see Table2). The memory usage is measured with the batch size of 1024 which is our standard training setting. Transformer is tested at a batch size of 256, which is the maximal number possible with the GPU resource at our disposal. Throughput is in format as Train throughput / inference throughput.ComplexityWe summarize the complexity of SOFT in space and time. For time complexity, it involves: (1) Sampling: O(nd e ). (2) Calculating three decomposed matrices: O(nmd e + mnd e + m 2 d e ) = O(2mnd e + m 2 d e ); (3) Moore-Penrose inverse: O(T × m 3 ) = O(T m 3 ), where T is the iteration steps. (4) All matrix multiplication: O(nm 2 + mnd e + mnd e ) = O(nm 2 + 2mnd e ). The total time complexity is O((d e + 4md e + m 2 )n + T m 3 + d e m 2 ). The space complexity is decided by four decomposed matrices with O</figDesc><table><row><cell>Methods</cell><cell cols="6">Complexity Memory Params FLOPs Throughput (img/s) Top-1 %</cell></row><row><cell>Transformer [34]</cell><cell>O(n 2 )</cell><cell>19.0GB †</cell><cell>13M</cell><cell>3.9G</cell><cell>1073 / 3240</cell><cell>79.1</cell></row><row><cell>Linformer [35]</cell><cell>O(n)</cell><cell>11.7GB</cell><cell>13M</cell><cell>1.9G</cell><cell>2767 / 3779</cell><cell>78.2</cell></row><row><cell>Performer [5]</cell><cell>O(n)</cell><cell>15.0GB</cell><cell>13M</cell><cell>2.2G</cell><cell>2037 / 3657</cell><cell>76.1</cell></row><row><cell>Nyströmformer [40]</cell><cell>O(n)</cell><cell>17.2GB</cell><cell>13M</cell><cell>2.0G</cell><cell>1891 / 3518</cell><cell>78.6</cell></row><row><cell>SOFT</cell><cell>O(n)</cell><cell>15.8GB</cell><cell>13M</cell><cell>1.9G</cell><cell>1730 / 3436</cell><cell>79.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Architecture specifications of SOFT variants. sp.: sampling ratio. -d: the hidden dimension.-h: the number of heads in the self-attention block. C33-BN-ReLU: three 3x3 Conv-BN-ReLU, with the stride of 2, 1, 2 respectively. C31-BN-ReLU: one 3x3 Conv-BN-ReLU, with a stride of 2.</figDesc><table><row><cell></cell><cell>Tiny</cell><cell></cell><cell>Small</cell><cell></cell><cell>Medium</cell><cell></cell><cell>Large</cell><cell></cell><cell>Huge</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">C33-BN-ReLU, 64-d</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage 1</cell><cell>sp. 8x8, 64-d, 2-h</cell><cell>x 1</cell><cell>sp. 8x8, 64-d, 2-h</cell><cell>x 1</cell><cell>sp. 8x8, 64-d, 2-h</cell><cell>x 1</cell><cell>sp. 8x8, 64-d, 2-h</cell><cell>x 1</cell><cell>sp. 8x8, 64-d, 2-h</cell><cell>x 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">C31-BN-ReLU, 128-d</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage 2</cell><cell>sp. 4x4, 128-d, 4-h</cell><cell>x 2</cell><cell>sp. 4x4, 128-d, 4-h</cell><cell>x 3</cell><cell>sp. 4x4, 128-d, 4-h</cell><cell>x 3</cell><cell>sp. 4x4, 128-d, 4-h</cell><cell>x 3</cell><cell>sp. 4x4, 128-d, 4-h</cell><cell>x 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">C31-BN-ReLU, 320-d or 288-d</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage 3</cell><cell>sp. 2x2, 320-d, 10-h</cell><cell>x 3</cell><cell>sp. 2x2, 320-d, 10-h</cell><cell>x 7</cell><cell>sp. 2x2, 288-d, 9-h</cell><cell>x 29</cell><cell>sp. 2x2, 320-d, 10-h</cell><cell>x 40</cell><cell>sp. 2x2, 352-d, 11-h</cell><cell>x 49</cell></row><row><cell>Stage 4 w. cls token</cell><cell>sp. 1x1, 512-d, 16-h</cell><cell>x 2</cell><cell>sp. 1x1, 512-d, 16-h</cell><cell cols="4">C31-BN-ReLU, 512-d x 4 sp. 1x1, 512-d, 16-h x 5 512-d, 16-h sp. 1x1,</cell><cell>x 5</cell><cell>sp. 1x1, 512-d, 16-h</cell><cell>x 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>(a) Ablations on bottleneck token sequence length. (b) Ablations on sampling methods.Overlapped convolution: We ablate SOFT with overlapped convolution (our default choice, same as many recent works) and SOFT with non-overlapped convolution in our Tiny configuration. Table5bshows that SOFT with overlapped convolution performs better than SOFT without overlapped convolution. Our non-overlapped convolution variant still outperforms the PVT<ref type="bibr" target="#b35">[36]</ref> which also has the same non-overlapped convolution by a clear margin.</figDesc><table><row><cell></cell><cell></cell><cell>Top-1 %</cell><cell cols="3">Sampling methods Params FLOPs Top-1 %</cell></row><row><cell>36</cell><cell>15.1GB 1.9G</cell><cell>79.0</cell><cell>Convolution</cell><cell>13.07M 2.0G</cell><cell>79.3</cell></row><row><cell>49</cell><cell>15.8GB 1.9G</cell><cell>79.3</cell><cell cols="2">Random sampling 12.96M 1.9G</cell><cell>79.3</cell></row><row><cell>64</cell><cell>16.9GB 2.0G</cell><cell>79.3</cell><cell>Biased sampling</cell><cell>12.96M 1.9G</cell><cell>79.0</cell></row><row><cell>81</cell><cell>18.5GB 2.1G</cell><cell>78.9</cell><cell>Average pooling</cell><cell>12.96M 1.9G</cell><cell>79.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>(a) Ablations on pyramidal architecture. (b) Ablations on overlapped convolution.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different linear/efficient Transformer variants on Long Range Arena</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment This work was funded in part by <rs type="funder">Shanghai Municipal Science and Technology Major Projects</rs> (No.<rs type="grantNumber">2018SHZDZX01</rs> and No.<rs type="grantNumber">2021SHZDZX0103</rs>), <rs type="funder">Mindspore</rs>, <rs type="funder">National Science Foundation of China</rs> under Grant No.<rs type="grantNumber">11690013</rs>, <rs type="grantNumber">71991471</rs> and the <rs type="programName">scientific-technological innovation plan program</rs> of <rs type="institution">Universities</rs> guided by the <rs type="funder">Ministry of Education, China</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GXf8ZYA">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_arDJAmE">
					<idno type="grant-number">2021SHZDZX0103</idno>
				</org>
				<org type="funding" xml:id="_gz8yuk7">
					<idno type="grant-number">11690013</idno>
				</org>
				<org type="funding" xml:id="_GR9xmK7">
					<idno type="grant-number">71991471</idno>
					<orgName type="program" subtype="full">scientific-technological innovation plan program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><surname>Lambdanetworks</surname></persName>
		</author>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On iterative computation of generalized inverses and associated projections</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021. 2, 3, 6, 7, 9, 10, 15</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Stéphane D'ascoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Positive definite kernels: past, present and future</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><surname>Fasshauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dolomites Research Notes on Approximation</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Is attention better than matrix decomposition?</title>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<idno>ICLR, 2021. 15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2016. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><surname>Perceiver</surname></persName>
		</author>
		<title level="m">General perception with iterative attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13076</idno>
		<title level="m">Finetuning pretrained transformers into rnns</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In ICLR, 2020. 2, 3, 7</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://www.mindspore.cn/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<title level="m">Listops: A diagnostic dataset for latent tree learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02143</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
		<title level="m">Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2020. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2020. 2, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 1, 2, 3, 4, 6, 9, 10, 15</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2, 3, 6, 7, 10</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2, 6, 7, 8, 10</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<title level="m">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using the nyström method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nyströmformer: A nyström-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021. 2, 3, 4, 6, 7, 9, 10, 15</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing networks</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
