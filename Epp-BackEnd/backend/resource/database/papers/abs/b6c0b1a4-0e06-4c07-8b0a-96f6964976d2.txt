Contrastive Learning for Low-light Raw Denoising
  Image/video denoising in low-light scenes is an extremely challenging problemdue to limited photon count and high noise. In this paper, we propose a novelapproach with contrastive learning to address this issue. Inspired by thesuccess of contrastive learning used in some high-level computer vision tasks,we bring in this idea to the low-level denoising task. In order to achieve thisgoal, we introduce a new denoising contrastive regularization (DCR) to exploitthe information of noisy images and clean images. In the feature space, DCRmakes the denoised image closer to the clean image and far away from the noisyimage. In addition, we build a new feature embedding network called Wnet, whichis more effective to extract high-frequency information. We conduct theexperiments on a real low-light dataset that captures still images taken on amoonless clear night in 0.6 millilux and videos under starlight (no moonpresent, <0.001 lux). The results show that our method can achieve a higherPSNR and better visual quality compared with existing methods
RecipeSnap -- a lightweight image-to-recipe model
  In this paper we want to address the problem of automation for recognition ofphotographed cooking dishes and generating the corresponding food recipes.Current image-to-recipe models are computation expensive and require powerfulGPUs for model training and implementation. High computational cost preventsthose existing models from being deployed on portable devices, like smartphones. To solve this issue we introduce a lightweight image-to-recipeprediction model, RecipeSnap, that reduces memory cost and computational costby more than 90% while still achieving 2.0 MedR, which is in line with thestate-of-the-art model. A pre-trained recipe encoder was used to compute recipeembeddings. Recipes from Recipe1M dataset and corresponding recipe embeddingsare collected as a recipe library, which are used for image encoder trainingand image query later. We use MobileNet-V2 as image encoder backbone, whichmakes our model suitable to portable devices. This model can be furtherdeveloped into an application for smart phones with a few effort. A comparisonof the performance between this lightweight model to other heavy models arepresented in this paper. Code, data and models are publicly accessible ongithub.
Pruned Lightweight Encoders for Computer Vision
  Latency-critical computer vision systems, such as autonomous driving or dronecontrol, require fast image or video compression when offloading neural networkinference to a remote computer. To ensure low latency on a near-sensor edgedevice, we propose the use of lightweight encoders with constant bitrate andpruned encoding configurations, namely, ASTC and JPEG XS. Pruning introducessignificant distortion which we show can be recovered by retraining the neuralnetwork with compressed data after decompression. Such an approach does notmodify the network architecture or require coding format modifications. Byretraining with compressed datasets, we reduced the classification accuracy andsegmentation mean intersection over union (mIoU) degradation due to ASTCcompression to 4.9-5.0 percentage points (pp) and 4.4-4.0 pp, respectively.With the same method, the mIoU lost due to JPEG XS compression at the mainprofile was restored to 2.7-2.3 pp. In terms of encoding speed, our ASTCencoder implementation is 2.3x faster than JPEG. Even though the JPEG XSreference encoder requires optimizations to reach low latency, we showed thatdisabling significance flag coding saves 22-23% of encoding time at the cost of0.4-0.3 mIoU after retraining.
RarePlanes: Synthetic Data Takes Flight
  RarePlanes is a unique open-source machine learning dataset that incorporatesboth real and synthetically generated satellite imagery. The RarePlanes datasetspecifically focuses on the value of synthetic data to aid computer visionalgorithms in their ability to automatically detect aircraft and theirattributes in satellite imagery. Although other synthetic/real combinationdatasets exist, RarePlanes is the largest openly-available very-high resolutiondataset built to test the value of synthetic data from an overhead perspective.Previous research has shown that synthetic data can reduce the amount of realtraining data needed and potentially improve performance for many tasks in thecomputer vision domain. The real portion of the dataset consists of 253 MaxarWorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700hand-annotated aircraft. The accompanying synthetic dataset is generated viaAI.Reverie's simulation platform and features 50,000 synthetic satellite imagessimulating a total area of 9331.2 km^2 with ~630,000 aircraft annotations. Boththe real and synthetically generated aircraft feature 10 fine grain attributesincluding: aircraft length, wingspan, wing-shape, wing-position, wingspanclass, propulsion, number of engines, number of vertical-stabilizers, presenceof canards, and aircraft role. Finally, we conduct extensive experiments toevaluate the real and synthetic datasets and compare performances. By doing so,we show the value of synthetic data for the task of detecting and classifyingaircraft from an overhead perspective.
Pedestrain detection for low-light vision proposal
  The demand for pedestrian detection has created a challenging problem forvarious visual tasks such as image fusion. As infrared images can capturethermal radiation information, image fusion between infrared and visible imagescould significantly improve target detection under environmental limitations.In our project, we would approach by preprocessing our dataset with imagefusion technique, then using Vision Transformer model to detect pedestriansfrom the fused images. During the evaluation procedure, a comparison would bemade between YOLOv5 and the revised ViT model performance on our fused images
