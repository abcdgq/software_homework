A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge
  The Visual Question Answering (VQA) task aspires to provide a meaningfultestbed for the development of AI models that can jointly reason over visualand natural language inputs. Despite a proliferation of VQA datasets, this goalis hindered by a set of common limitations. These include a reliance onrelatively simplistic questions that are repetitive in both concepts andlinguistic structure, little world knowledge needed outside of the pairedimage, and limited reasoning required to arrive at the correct answer. Weintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about25K questions requiring a broad base of commonsense and world knowledge toanswer. In contrast to the existing knowledge-based VQA datasets, the questionsgenerally cannot be answered by simply querying a knowledge base, and insteadrequire some form of commonsense reasoning about the scene depicted in theimage. We demonstrate the potential of this new dataset through a detailedanalysis of its contents and baseline performance measurements over a varietyof state-of-the-art vision-language models. Project page:http://a-okvqa.allenai.org/
COIN: Counterfactual Image Generation for VQA Interpretation
  Due to the significant advancement of Natural Language Processing andComputer Vision-based models, Visual Question Answering (VQA) systems arebecoming more intelligent and advanced. However, they are still error-pronewhen dealing with relatively complex questions. Therefore, it is important tounderstand the behaviour of the VQA models before adopting their results. Inthis paper, we introduce an interpretability approach for VQA models bygenerating counterfactual images. Specifically, the generated image is supposedto have the minimal possible change to the original image and leads the VQAmodel to give a different answer. In addition, our approach ensures that thegenerated image is realistic. Since quantitative metrics cannot be employed toevaluate the interpretability of the model, we carried out a user study toassess different aspects of our approach. In addition to interpreting theresult of VQA models on single images, the obtained results and the discussionprovides an extensive explanation of VQA models' behaviour.
An experimental study of the vision-bottleneck in VQA
  As in many tasks combining vision and language, both modalities play acrucial role in Visual Question Answering (VQA). To properly solve the task, agiven model should both understand the content of the proposed image and thenature of the question. While the fusion between modalities, which is anotherobviously important part of the problem, has been highly studied, the visionpart has received less attention in recent work. Current state-of-the-artmethods for VQA mainly rely on off-the-shelf object detectors delivering a setof object bounding boxes and embeddings, which are then combined with questionword embeddings through a reasoning module. In this paper, we propose anin-depth study of the vision-bottleneck in VQA, experimenting with both thequantity and quality of visual objects extracted from images. We also study theimpact of two methods to incorporate the information about objects necessaryfor answering a question, in the reasoning module directly, and earlier in theobject selection stage. This work highlights the importance of vision in thecontext of VQA, and the interest of tailoring vision methods used in VQA to thetask at hand.
StarVQA: Space-Time Attention for Video Quality Assessment
  The attention mechanism is blooming in computer vision nowadays. However, itsapplication to video quality assessment (VQA) has not been reported. Evaluatingthe quality of in-the-wild videos is challenging due to the unknown of pristinereference and shooting distortion. This paper presents a novel\underline{s}pace-\underline{t}ime \underline{a}ttention networkfo\underline{r} the \underline{VQA} problem, named StarVQA. StarVQA builds aTransformer by alternately concatenating the divided space-time attention. Toadapt the Transformer architecture for training, StarVQA designs a vectorizedregression loss by encoding the mean opinion score (MOS) to the probabilityvector and embedding a special vectorized label token as the learnablevariable. To capture the long-range spatiotemporal dependencies of a videosequence, StarVQA encodes the space-time position information of each patch tothe input of the Transformer. Various experiments are conducted on the de-factoin-the-wild video datasets, including LIVE-VQC, KoNViD-1k, LSVQ, andLSVQ-1080p. Experimental results demonstrate the superiority of the proposedStarVQA over the state-of-the-art. Code and model will be available at:https://github.com/DVL/StarVQA.
EKTVQA: Generalized use of External Knowledge to empower Scene Text in  Text-VQA
  The open-ended question answering task of Text-VQA often requires reading andreasoning about rarely seen or completely unseen scene-text content of animage. We address this zero-shot nature of the problem by proposing thegeneralized use of external knowledge to augment our understanding of the scenetext. We design a framework to extract, validate, and reason with knowledgeusing a standard multimodal transformer for vision language understandingtasks. Through empirical evidence and qualitative results, we demonstrate howexternal knowledge can highlight instance-only cues and thus help deal withtraining data bias, improve answer entity type correctness, and detectmultiword named entities. We generate results comparable to thestate-of-the-art on three publicly available datasets, under the constraints ofsimilar upstream OCR systems and training data.
