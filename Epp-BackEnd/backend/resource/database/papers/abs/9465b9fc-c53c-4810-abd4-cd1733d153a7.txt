Dual Vision Transformer
  Prior works have proposed several strategies to reduce the computational costof self-attention mechanism. Many of these works consider decomposing theself-attention procedure into regional and local feature extraction proceduresthat each incurs a much smaller computational complexity. However, regionalinformation is typically only achieved at the expense of undesirableinformation lost owing to down-sampling. In this paper, we propose a novelTransformer architecture that aims to mitigate the cost issue, named DualVision Transformer (Dual-ViT). The new architecture incorporates a criticalsemantic pathway that can more efficiently compress token vectors into globalsemantics with reduced order of complexity. Such compressed global semanticsthen serve as useful prior information in learning finer pixel level details,through another constructed pixel pathway. The semantic pathway and pixelpathway are then integrated together and are jointly trained, spreading theenhanced self-attention information in parallel through both of the pathways.Dual-ViT is henceforth able to reduce the computational complexity withoutcompromising much accuracy. We empirically demonstrate that Dual-ViT providessuperior accuracy than SOTA Transformer architectures with reduced trainingcomplexity. Source code is available at\url{https://github.com/YehLi/ImageNetModel}.
ViR:the Vision Reservoir
  The most recent year has witnessed the success of applying the VisionTransformer (ViT) for image classification. However, there are still evidencesindicating that ViT often suffers following two aspects, i) the highcomputation and the memory burden from applying the multiple Transformer layersfor pre-training on a large-scale dataset, ii) the over-fitting when trainingon small datasets from scratch. To address these problems, a novel method,namely, Vision Reservoir computing (ViR), is proposed here for imageclassification, as a parallel to ViT. By splitting each image into a sequenceof tokens with fixed length, the ViR constructs a pure reservoir with a nearlyfully connected topology to replace the Transformer module in ViT. Two kinds ofdeep ViR models are subsequently proposed to enhance the network performance.Comparative experiments between the ViR and the ViT are carried out on severalimage classification benchmarks. Without any pre-training process, the ViRoutperforms the ViT in terms of both model and computational complexity.Specifically, the number of parameters of the ViR is about 15% even 5% of theViT, and the memory footprint is about 20% to 40% of the ViT. The superiorityof the ViR performance is explained by Small-World characteristics, Lyapunovexponents, and memory capacity.
Super Vision Transformer
  We attempt to reduce the computational costs in vision transformers (ViTs),which increase quadratically in the token number. We present a novel trainingparadigm that trains only one ViT model at a time, but is capable of providingimproved image recognition performance with various computational costs. Here,the trained ViT model, termed super vision transformer (SuperViT), is empoweredwith the versatile ability to solve incoming patches of multiple sizes as wellas preserve informative tokens with multiple keeping rates (the ratio ofkeeping tokens) to achieve good hardware efficiency for inference, given thatthe available hardware resources often change from time to time. Experimentalresults on ImageNet demonstrate that our SuperViT can considerably reduce thecomputational costs of ViT models with even performance increase. For example,we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existingstudies on efficient vision transformers. For example, when consuming the sameamount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SOTA) EViTby 1.1% when using DeiT-S as their backbones. The project of this work is madepublicly available at https://github.com/lmbxmu/SuperViT.
Quantum Vision Clustering
  Unsupervised visual clustering has garnered significant attention in recenttimes, aiming to characterize distributions of unlabeled visual images throughclustering based on a parameterized appearance approach. Alternatively,clustering algorithms can be viewed as assignment problems, often characterizedas NP-hard, yet precisely solvable for small instances on contemporaryhardware. Adiabatic quantum computing (AQC) emerges as a promising solution,poised to deliver substantial speedups for a range of NP-hard optimizationproblems. However, existing clustering formulations face challenges in quantumcomputing adoption due to scalability issues. In this study, we present thefirst clustering formulation tailored for resolution using Adiabatic quantumcomputing. An Ising model is introduced to represent the quantum mechanicalsystem implemented on AQC. The proposed approach demonstrates highcompetitiveness compared to state-of-the-art optimization-based methods, evenwhen utilizing off-the-shelf integer programming solvers. Lastly, this workshowcases the solvability of the proposed clustering problem oncurrent-generation real quantum computers for small examples and analyzes theproperties of the obtained solutions
Vision Transformer Pruning
  Vision transformer has achieved competitive performance on a variety ofcomputer vision applications. However, their storage, run-time memory, andcomputational demands are hindering the deployment to mobile devices. Here wepresent a vision transformer pruning approach, which identifies the impacts ofdimensions in each layer of transformer and then executes pruning accordingly.By encouraging dimension-wise sparsity in the transformer, important dimensionsautomatically emerge. A great number of dimensions with small importance scorescan be discarded to achieve a high pruning ratio without significantlycompromising accuracy. The pipeline for vision transformer pruning is asfollows: 1) training with sparsity regularization; 2) pruning dimensions oflinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios ofthe proposed algorithm are well evaluated and analyzed on ImageNet dataset todemonstrate the effectiveness of our proposed method.
