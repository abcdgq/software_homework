Global Context Vision Transformers
  We propose global context vision transformer (GC ViT), a novel architecturethat enhances parameter and compute utilization for computer vision. Our methodleverages global context self-attention modules, joint with standard localself-attention, to effectively and efficiently model both long and short-rangespatial interactions, without the need for expensive operations such ascomputing attention masks or shifting local windows. In addition, we addressthe lack of the inductive bias in ViTs, and propose to leverage a modifiedfused inverted residual blocks in our architecture. Our proposed GC ViTachieves state-of-the-art results across image classification, object detectionand semantic segmentation tasks. On ImageNet-1K dataset for classification, thevariants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and85.7% Top-1 accuracy, respectively, at 224 image resolution and without anypre-training, hence surpassing comparably-sized prior art such as CNN-basedConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin.Pre-trained GC ViT backbones in downstream tasks of object detection, instancesegmentation, and semantic segmentation using MS COCO and ADE20K datasetsoutperform prior work consistently. Specifically, GC ViT with a 4-scale DINOdetection head achieves a box AP of 58.3 on MS COCO dataset.
Container: Context Aggregation Network
  Convolutional neural networks (CNNs) are ubiquitous in computer vision, witha myriad of effective and efficient variations. Recently, Transformers --originally introduced in natural language processing -- have been increasinglyadopted in computer vision. While early adopters continue to employ CNNbackbones, the latest networks are end-to-end CNN-free Transformer solutions. Arecent surprising finding shows that a simple MLP based solution without anytraditional convolutional or Transformer components can produce effectivevisual representations. While CNNs, Transformers and MLP-Mixers may beconsidered as completely disparate architectures, we provide a unified viewshowing that they are in fact special cases of a more general method toaggregate spatial context in a neural network stack. We present the \model(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-headcontext aggregation that can exploit long-range interactions \emph{a la}Transformers while still exploiting the inductive bias of the local convolutionoperation leading to faster convergence speeds, often seen in CNNs. In contrastto Transformer-based methods that do not scale well to downstream tasks thatrely on larger input image resolutions, our efficient network, named\modellight, can be employed in object detection and instance segmentationnetworks such as DETR, RetinaNet and Mask-RCNN to obtain an impressivedetection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing largeimprovements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50backbone with a comparable compute and parameter size. Our method also achievespromising results on self-supervised learning compared to DeiT on the DINOframework. Code is released at \url{https://github.com/allenai/container}.
Scene Graph Generation with Geometric Context
  Scene Graph Generation has gained much attention in computer vision researchwith the growing demand in image understanding projects like visual questionanswering, image captioning, self-driving cars, crowd behavior analysis,activity recognition, and more. Scene graph, a visually grounded graphicalstructure of an image, immensely helps to simplify the image understandingtasks. In this work, we introduced a post-processing algorithm called GeometricContext to understand the visual scenes better geometrically. We use thispost-processing algorithm to add and refine the geometric relationships betweenobject pairs to a prior model. We exploit this context by calculating thedirection and distance between object pairs. We use Knowledge Embedded RoutingNetwork (KERN) as our baseline model, extend the work with our algorithm, andshow comparable results on the recent state-of-the-art algorithms.
CANet: A Context-Aware Network for Shadow Removal
  In this paper, we propose a novel two-stage context-aware network named CANetfor shadow removal, in which the contextual information from non-shadow regionsis transferred to shadow regions at the embedded feature spaces. At Stage-I, wepropose a contextual patch matching (CPM) module to generate a set of potentialmatching pairs of shadow and non-shadow patches. Combined with the potentialcontextual relationships between shadow and non-shadow regions, ourwell-designed contextual feature transfer (CFT) mechanism can transfercontextual information from non-shadow to shadow regions at different scales.With the reconstructed feature maps, we remove shadows at L and A/B channelsseparately. At Stage-II, we use an encoder-decoder to refine current resultsand generate the final shadow removal results. We evaluate our proposed CANeton two benchmark datasets and some real-world shadow images with complexscenes. Extensive experimental results strongly demonstrate the efficacy of ourproposed CANet and exhibit superior performance to state-of-the-arts.
Contextual Gradient Scaling for Few-Shot Learning
  Model-agnostic meta-learning (MAML) is a well-known optimization-basedmeta-learning algorithm that works well in various computer vision tasks, e.g.,few-shot classification. MAML is to learn an initialization so that a model canadapt to a new task in a few steps. However, since the gradient norm of aclassifier (head) is much bigger than those of backbone layers, the modelfocuses on learning the decision boundary of the classifier with similarrepresentations. Furthermore, gradient norms of high-level layers are smallthan those of the other layers. So, the backbone of MAML usually learnstask-generic features, which results in deteriorated adaptation performance inthe inner-loop. To resolve or mitigate this problem, we propose contextualgradient scaling (CxGrad), which scales gradient norms of the backbone tofacilitate learning task-specific knowledge in the inner-loop. Since thescaling factors are generated from task-conditioned parameters, gradient normsof the backbone can be scaled in a task-wise fashion. Experimental results showthat CxGrad effectively encourages the backbone to learn task-specificknowledge in the inner-loop and improves the performance of MAML up to asignificant margin in both same- and cross-domain few-shot classification.
