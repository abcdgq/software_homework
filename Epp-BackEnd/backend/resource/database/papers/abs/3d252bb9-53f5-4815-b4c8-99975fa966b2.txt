Container: Context Aggregation Network
  Convolutional neural networks (CNNs) are ubiquitous in computer vision, witha myriad of effective and efficient variations. Recently, Transformers --originally introduced in natural language processing -- have been increasinglyadopted in computer vision. While early adopters continue to employ CNNbackbones, the latest networks are end-to-end CNN-free Transformer solutions. Arecent surprising finding shows that a simple MLP based solution without anytraditional convolutional or Transformer components can produce effectivevisual representations. While CNNs, Transformers and MLP-Mixers may beconsidered as completely disparate architectures, we provide a unified viewshowing that they are in fact special cases of a more general method toaggregate spatial context in a neural network stack. We present the \model(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-headcontext aggregation that can exploit long-range interactions \emph{a la}Transformers while still exploiting the inductive bias of the local convolutionoperation leading to faster convergence speeds, often seen in CNNs. In contrastto Transformer-based methods that do not scale well to downstream tasks thatrely on larger input image resolutions, our efficient network, named\modellight, can be employed in object detection and instance segmentationnetworks such as DETR, RetinaNet and Mask-RCNN to obtain an impressivedetection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing largeimprovements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50backbone with a comparable compute and parameter size. Our method also achievespromising results on self-supervised learning compared to DeiT on the DINOframework. Code is released at \url{https://github.com/allenai/container}.
Scene Graph Generation with Geometric Context
  Scene Graph Generation has gained much attention in computer vision researchwith the growing demand in image understanding projects like visual questionanswering, image captioning, self-driving cars, crowd behavior analysis,activity recognition, and more. Scene graph, a visually grounded graphicalstructure of an image, immensely helps to simplify the image understandingtasks. In this work, we introduced a post-processing algorithm called GeometricContext to understand the visual scenes better geometrically. We use thispost-processing algorithm to add and refine the geometric relationships betweenobject pairs to a prior model. We exploit this context by calculating thedirection and distance between object pairs. We use Knowledge Embedded RoutingNetwork (KERN) as our baseline model, extend the work with our algorithm, andshow comparable results on the recent state-of-the-art algorithms.
CANet: A Context-Aware Network for Shadow Removal
  In this paper, we propose a novel two-stage context-aware network named CANetfor shadow removal, in which the contextual information from non-shadow regionsis transferred to shadow regions at the embedded feature spaces. At Stage-I, wepropose a contextual patch matching (CPM) module to generate a set of potentialmatching pairs of shadow and non-shadow patches. Combined with the potentialcontextual relationships between shadow and non-shadow regions, ourwell-designed contextual feature transfer (CFT) mechanism can transfercontextual information from non-shadow to shadow regions at different scales.With the reconstructed feature maps, we remove shadows at L and A/B channelsseparately. At Stage-II, we use an encoder-decoder to refine current resultsand generate the final shadow removal results. We evaluate our proposed CANeton two benchmark datasets and some real-world shadow images with complexscenes. Extensive experimental results strongly demonstrate the efficacy of ourproposed CANet and exhibit superior performance to state-of-the-arts.
Augmenting Depth Estimation with Geospatial Context
  Modern cameras are equipped with a wide array of sensors that enablerecording the geospatial context of an image. Taking advantage of this, weexplore depth estimation under the assumption that the camera is geocalibrated,a problem we refer to as geo-enabled depth estimation. Our key insight is thatif capture location is known, the corresponding overhead viewpoint offers avaluable resource for understanding the scale of the scene. We propose anend-to-end architecture for depth estimation that uses geospatial context toinfer a synthetic ground-level depth map from a co-located overhead image, thenfuses it inside of an encoder/decoder style segmentation network. To supportevaluation of our methods, we extend a recently released dataset with overheadimagery and corresponding height maps. Results demonstrate that integratinggeospatial context significantly reduces error compared to baselines, both atclose ranges and when evaluating at much larger distances than existingbenchmarks consider.
Efficient Pipelines for Vision-Based Context Sensing
  Context awareness is an essential part of mobile and ubiquitous computing.Its goal is to unveil situational information about mobile users like locationsand activities. The sensed context can enable many services like navigation,AR, and smarting shopping. Such context can be sensed in different waysincluding visual sensors. There is an emergence of vision sources deployedworldwide. The cameras could be installed on roadside, in-house, and on mobileplatforms. This trend provides huge amount of vision data that could be usedfor context sensing. However, the vision data collection and analytics arestill highly manual today. It is hard to deploy cameras at large scale for datacollection. Organizing and labeling context from the data are also laborintensive. In recent years, advanced vision algorithms and deep neural networksare used to help analyze vision data. But this approach is limited by dataquality, labeling effort, and dependency on hardware resources. In summary,there are three major challenges for today's vision-based context sensingsystems: data collection and labeling at large scale, process large datavolumes efficiently with limited hardware resources, and extract accuratecontext out of vision data. The thesis explores the design space that consistsof three dimensions: sensing task, sensor types, and task locations. Our priorwork explores several points in this design space. We make contributions by (1)developing efficient and scalable solutions for different points in the designspace of vision-based sensing tasks; (2) achieving state-of-the-art accuracy inthose applications; (3) and developing guidelines for designing such sensingsystems.
