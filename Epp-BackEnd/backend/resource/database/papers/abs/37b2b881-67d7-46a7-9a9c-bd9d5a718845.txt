PSViT: Better Vision Transformer via Token Pooling and Attention Sharing
  In this paper, we observe two levels of redundancies when applying visiontransformers (ViT) for image recognition. First, fixing the number of tokensthrough the whole network produces redundant features at the spatial level.Second, the attention maps among different transformer layers are redundant.Based on the observations above, we propose a PSViT: a ViT with token Poolingand attention Sharing to reduce the redundancy, effectively enhancing thefeature representation ability, and achieving a better speed-accuracytrade-off. Specifically, in our PSViT, token pooling can be defined as theoperation that decreases the number of tokens at the spatial level. Besides,attention sharing will be built between the neighboring transformer layers forreusing the attention maps having a strong correlation among adjacent layers.Then, a compact set of the possible combinations for different token poolingand attention sharing mechanisms are constructed. Based on the proposed compactset, the number of tokens in each layer and the choices of layers sharingattention can be treated as hyper-parameters that are learned from dataautomatically. Experimental results show that the proposed scheme can achieveup to 6.6% accuracy improvement in ImageNet classification compared with theDeiT.
